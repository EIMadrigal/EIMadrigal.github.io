<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Open Sans:300,300italic,400,400italic,700,700italic|Monaco:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"eimadrigal.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":true,"comments":{"style":"tabs","active":"gitalk","storage":true,"lazyload":false,"nav":null,"activeClass":"gitalk"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="深度学习中常用的优化算法">
<meta property="og:type" content="article">
<meta property="og:title" content="Optimization Methods in Deep Learning">
<meta property="og:url" content="https://eimadrigal.github.io/posts/optimization-methods-in-dl/index.html">
<meta property="og:site_name" content="EI Madrigal&#39;s Space">
<meta property="og:description" content="深度学习中常用的优化算法">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210710192205509.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210710192712932.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210710193206336.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210710193712619.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210710193823396.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210710194231292.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20210710194309531.png">
<meta property="article:published_time" content="2021-07-10T11:47:00.000Z">
<meta property="article:modified_time" content="2023-03-02T08:18:22.599Z">
<meta property="article:author" content="EIMadrigal">
<meta property="article:tag" content="Machine Learning">
<meta property="article:tag" content="Paper">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://img-blog.csdnimg.cn/20210710192205509.png">

<link rel="canonical" href="https://eimadrigal.github.io/posts/optimization-methods-in-dl/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Optimization Methods in Deep Learning | EI Madrigal's Space</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">EI Madrigal's Space</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://eimadrigal.github.io/posts/optimization-methods-in-dl/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/favicon.png">
      <meta itemprop="name" content="EIMadrigal">
      <meta itemprop="description" content="Hello World">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EI Madrigal's Space">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Optimization Methods in Deep Learning
        </h1>

        <div class="post-meta">

		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-07-10 19:47:00" itemprop="dateCreated datePublished" datetime="2021-07-10T19:47:00+08:00">2021-07-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2023-03-02 16:18:22" itemprop="dateModified" datetime="2023-03-02T16:18:22+08:00">2023-03-02</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Computer-Science/" itemprop="url" rel="index"><span itemprop="name">Computer Science</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
            <div class="post-description">深度学习中常用的优化算法</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="background">Background</h2>
<p>深度神经网络的训练过程主要通过求解一个特定的优化问题来实现，然而由于该问题是一个复杂的高维非线性优化问题，并且不同的网络结构差异很大，不能将传统的优化方法直接使用。即使数据集和网络结构完全相同，不同的优化算法也可能导致完全不同的收敛效果。实际应用的一些简单方法虽然行之有效，但现有理论无法充分解释其有效性，超参数的不断增加也给优化增加了不少难度。如何确保算法收敛、如何尽快收敛以及能否收敛到全局最优一直是困扰学术界和工业界的问题。如果能够用优化理论去解释神经网络的训练行为，对于深度学习的推广应用将会起到巨大的推动作用。</p>
<p>对于有监督学习，给定包含n个样本的训练集<span
class="math inline">\(\{(\mathbf{x}_1, y_1), \ldots, (\mathbf{x}_n,
y_n)\}\)</span>，<span
class="math inline">\(\mathbf{x}\)</span>表示样本的特征向量，<span
class="math inline">\(y\)</span>表示该样本对应的标签。我们的任务是利用样本信息来预测相应的标签，使预测值尽可能接近真实标签。如果用深度学习来完成这个任务，就需要通过调整神经网络的参数（权重W和偏差b）来近似数据背后的函数映射关系，这个关系往往是高度非线性的，网络越深表达能力也就越强，逼近效果的精度也就更高，因此网络结构很可能是极其复杂的。</p>
<p>为了衡量预测值和真实值之间的接近程度，通常需要采用某种距离度量方式<span
class="math inline">\(l\)</span>，<span
class="math inline">\(l\)</span>一般设计为<strong>可微</strong>的，接着用一些优化算法去最小化该目标函数。因此优化问题变为寻找最佳的参数使得<span
class="math inline">\(l\)</span>最小，在不考虑正则项的情况下有： <span
class="math display">\[
\mathop{\mathrm{min}}_f \frac{1}{n} \sum_{i=1}^n l(f(\mathbf{x}_i), y_i)
\]</span> <span
class="math inline">\(f\)</span>就是我们从输入到输出的映射函数，<span
class="math inline">\(l\)</span>通常也叫损失函数，衡量预测值<span
class="math inline">\(f(x_i)\)</span>和真实标签<span
class="math inline">\(y_i\)</span>的差距，比如回归问题中经常使用的平方损失函数<span
class="math inline">\(l=||f(x_i)-y_i||^2\)</span>。</p>
<p>需要注意的是：深度学习中的优化问题与传统意义上的优化问题有所差别。传统的优化问题需要尽可能找到目标函数的最值，而深度学习的最终目的是为了<strong>预测未知</strong>的数据，而不是将训练数据上的损失降到最低。我们定义的损失函数<span
class="math inline">\(J(\Theta)\)</span>衡量的是当前模型参数<span
class="math inline">\(\Theta\)</span>在<strong>训练集</strong>上的优劣，然而，最小化训练误差并不意味着模型的泛化误差也会最小，为了降低泛化误差我们还需要关注过拟合问题，因此损失函数往往要加上<strong>正则项</strong>。统计学上称为经验风险最小化，即由于无法获得全部数据，所以只能用经验风险作为实际风险的近似。非常有意思的是：尽管大多数神经网络都是严重过参数化的，但是反而有着比较不错的泛化能力，这与传统的机器学习观点是矛盾的，泛化理论也需要更加深入的研究。</p>
<p>深度学习中的<span
class="math inline">\(f\)</span>通常是多层的复合函数，由于太复杂而无法求出解析解，所以要用数值优化算法去求解。实际中主流的深度学习优化算法都利用梯度下降来求解，梯度下降是深度学习优化算法的基础，尽管目前已经很少直接使用，但它却是其他高级优化算法的基石：
假设网络的参数为<span
class="math inline">\(x=(x_1,x_2,...,x_d)^T\)</span>，优化的目标函数为<span
class="math inline">\(f\)</span>，那么<span
class="math inline">\(f\)</span>的梯度为： <span class="math display">\[
\nabla f(\mathbf{x}) = \bigg[\frac{\partial f(\mathbf{x})}{\partial
x_1}, \frac{\partial f(\mathbf{x})}{\partial x_2}, \ldots,
\frac{\partial f(\mathbf{x})}{\partial x_d}\bigg]^\top
\]</span>
每个元素对应着目标函数在该方向上的变化率，因此只要沿着梯度的反方向就可以使目标函数减小得最快：<span
class="math inline">\(\mathbf{x} \leftarrow \mathbf{x} - \eta \nabla
f(\mathbf{x})\)</span>，<span
class="math inline">\(\eta\)</span>是一个被称为学习率的超参数，用来控制每一步的大小。<span
class="math inline">\(\eta\)</span>过小，收敛过程极度缓慢；<span
class="math inline">\(\eta\)</span>过大，可能造成损失函数在最小点附近波动甚至发散。学习率的调整是神经网络训练过程中一个重要的调整参数，常常使人头痛不已，因此也出现了很多学习率自适应调整的算法，将在下面深入分析这些算法的优劣。</p>
<p>有了优化模型及最基础的求解方法后，我们需要对其性质和优缺点进行分析，以便于后续的改进。深度学习的优化问题大多是非凸的，因此存在很多挑战：</p>
<ol type="1">
<li>局部最优：对于凸优化问题，局部最优即是全局最优。然而对于非凸问题，当损失函数到达局部最优点时，<span
class="math inline">\(J(\Theta)\)</span>的梯度为0，<span
class="math inline">\(\Theta\)</span>无法继续更新，损失函数无法继续下降；</li>
<li>鞍点：该点既不是局部最小也不是全局最小，但是该点的梯度消失，无法继续更新；</li>
<li>梯度消失/爆炸：由于初始值和激活函数选择不当
(如sigmoid)，当梯度反向回传时，可能在某一层求导后梯度值很小/很大，导致训练速度极其缓慢。因此初始值的选择通常采用很小的随机数，避免收敛到比较差的区域，激活函数通常也会选择ReLU，避免梯度消失问题。</li>
</ol>
<p>局部最小和鞍点示意图如下： <img
src="https://img-blog.csdnimg.cn/20210710191530881.png"
alt="在这里插入图片描述" />
尤其在高维空间中，鞍点的问题变得更加严重：假设<span
class="math inline">\(\Theta\)</span>是一个k维向量，<span
class="math inline">\(J(\Theta)\)</span>的海森矩阵就有k个特征值，其梯度为0的点有可能是局部最小（特征值均为正）、局部最大（特征值均为负）或者是鞍点（特征值有正有负）。高维空间中特征值有正有负的概率很大，因此鞍点出现的可能性远大于局部最优点出现的可能性，并且鞍点周围的平坦区域可能很大，需要增加噪声扰动来逃离鞍点。</p>
<p>由于上述问题的存在，通常很难找到<span
class="math inline">\(J(\Theta)\)</span>的全局最优解，但实际上为了减少过拟合的风险我们并不需要训练集上的全局最优，经典的梯度下降就可以带来足够的局部最优。</p>
<p>分析完优化模型本身的问题，再来看看最基础的GD的问题：目标函数通常是训练集中所有样本的损失的平均值，故目标函数的梯度为：
<span class="math display">\[
\nabla f(\mathbf{x}) = \frac{1}{n} \sum_{i = 1}^n \nabla f_i(\mathbf{x})
\]</span> 如果用Full-batch
GD，那么每次迭代每个参数的梯度计算的时间复杂度为<span
class="math inline">\(O(n)\)</span>，对于大规模数据，这样的更新速度显然无法令人忍受。</p>
<p>学习率的选择是一项重要的调参工作，因此学习率的自适应变化就成为了研究热点之一，一些二阶方法应运而生，我们首先来看看牛顿法该如何解决这个问题。</p>
<p>对于损失函数<span
class="math inline">\(f\)</span>，利用泰勒展开式有： <span
class="math display">\[
f(\mathbf{x} + \boldsymbol{\epsilon}) = f(\mathbf{x}) +
\boldsymbol{\epsilon}^\top \nabla f(\mathbf{x}) + \frac{1}{2}
\boldsymbol{\epsilon}^\top \nabla^2 f(\mathbf{x}) \boldsymbol{\epsilon}
+ \mathcal{O}(\|\boldsymbol{\epsilon}\|^3)
\]</span> 式中的<span class="math inline">\(\nabla^2
f(\mathbf{x})\)</span>即<span
class="math inline">\(d*d\)</span>海森矩阵，存储了函数的二阶偏导数。为了求得<span
class="math inline">\(f\)</span>的最小值，令上式对<span
class="math inline">\(\epsilon\)</span>求导得0，有：<span
class="math inline">\(\boldsymbol{\epsilon}=-\nabla
f(\mathbf{x})H^{-1}\)</span>，即每次的参数更新为<span
class="math inline">\(\mathbf{x} \leftarrow \mathbf{x} - \nabla
f(\mathbf{x})H^{-1}\)</span>。二阶近似利用了损失函数的曲率信息，即如果曲率比较小，那么这步更新就会比较大，反之则更新较小。这里没有了学习率，而是通过“梯度的梯度”自动调整步幅，看起来比一阶的梯度下降要好一些。</p>
<p>然而深度学习的参数空间往往十分巨大，因此存储和计算海森矩阵的逆是不现实的，这也是牛顿法无法在DNN中使用的重要原因。为了缓解这个问题，学术界提出了一些拟牛顿法如L-BFGS等试图去降低存储消耗，但是计算代价仍然很高。</p>
<p>从以上的分析可以看到：无论是Full-batch
GD还是牛顿法，都存在计算消耗大等问题，不适用于深度学习任务的大规模数据集训练，因此已经很少被直接用在深度学习模型中。为了处理这些问题，学术界提出了很多替代的优化算法，因此接下来我将调研分析当前常用的深度学习优化算法
(SGD/Adam...)的优缺点，并结合实例及前沿研究进行相关讨论。 ## Popular
Algorithms
优化算法在神经网络的训练中有着举足轻重的作用，选择合适的优化算法可以使得损失函数收敛地更快，同时收敛到更好的区域。目前比较流行的算法有下面几种：
### 1 SGD
尝试用mini-batch的梯度平均值作为整体梯度的无偏估计，参数的更新非常简单，沿着梯度的反方向即是loss下降最快的方向：
<span class="math display">\[
x_{t+1}=x_t-\alpha\nabla f(x_t)
\]</span> 如果是Batch
GD并且学习率足够小时可以保证损失函数单调不增。实际使用时一般会采用学习率递减策略保证模型收敛。</p>
<p>实现也非常简单：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x -= lr * grads</span><br></pre></td></tr></table></figure>
<p>SGD存在几个问题：</p>
<p>首先，如果loss对于不同参数的敏感程度不同，那么收敛过程会在敏感参数方向上抖动：
<img src="https://img-blog.csdnimg.cn/20210710192205509.png" />
对于非常大的参数空间，可能会收敛到不同的区域。
其次，如果loss函数有局部最优或者鞍点，这些点上梯度为0，无法收敛到全局最优；
最后，如果采用mini-batch，那么计算出的梯度值是有噪声的，意味着收敛过程可能会是非常曲折的，也即需要更多时间。
### 2 SGD+Momentum
为了解决SGD的问题，有学者提出了带有动量的SGD，其思想也很简单：更新参数时不仅考虑当前的梯度方向，还要考虑历史累积梯度方向，如果两者方向一致，那么这一步更新幅度就会增大；如果不一致，就会减弱沿当前梯度的下降幅度。
<span class="math display">\[
v_{t+1}=\rho v_t+\nabla f(x_t) \\
x_{t+1}=x_t-\alpha v_{t+1}
\]</span> <span
class="math inline">\(\rho\)</span>可以看作是对历史梯度的衰减，一般取0.9。</p>
<p>带有动量的SGD实现： <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">v = rho * v + grads</span><br><span class="line">x -= lr * v</span><br></pre></td></tr></table></figure> 这样就解决了SGD的三个问题：
首先，由于历史梯度的存在，朝敏感方向步进的数量就会减少，会更加平滑的向最优点前进，减小了震荡，加速收敛；
其次，对于局部最优点，虽然当前梯度为0，但是依靠历史梯度可以越过该点继续下降；
最后，梯度噪声引起的震荡可以通过历史梯度互相抵消。 ### 3 Nesterov
Momentum Nesterov
Momentum由SGD+Momentum衍生而来，SGD+Momentum是将当前点的梯度和速度结合起来，而Nesterov
Momentum则是将当前点的速度和下一个近似点的梯度结合起来，意味着我们不是在当前位置去看未来，而是多看了一步，在稍远一些的下一步看未来，可以提前调整步进大小：
<img src="https://img-blog.csdnimg.cn/20210710192712932.png"
alt="在这里插入图片描述" /> 所以Nesterov Momentum的更新规则为： <span
class="math display">\[
v_{t+1}=\rho v_t-\alpha\nabla f(x_t+\rho v_t) \\
x_{t+1}=x_t+v_{t+1}
\]</span> 通常我们希望针对<span
class="math inline">\(x_t\)</span>计算梯度，通过简单的变量替换，得到新的更新规则：
<span class="math display">\[
v_{t+1}=\rho v_t-\alpha\nabla f(x_t) \\
x_{t+1}=x_t+v_{t+1}+\rho(v_{t+1}-v_t)
\]</span> Nesterov的实现： <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">v_prev = v</span><br><span class="line">v = rho * v - lr * grads</span><br><span class="line">x += (1 + rho) * v - rho * v_prev</span><br></pre></td></tr></table></figure> ### 4 AdaGrad
前面的几种方法都是设置了一个全局的学习率，AdaGrad则通过引入二阶动量使得学习率可以针对<strong>每个参数</strong>自适应地取值：对于更新频繁的参数，已经有了很多认知，不希望因为单个样本影响太大，所以学习率可以小一些；对于更新稀疏的参数，希望从偶尔出现的能更新该参数的样本中多获得一些信息，所以学习率可以设置地大一些。为了了解参数更新的频繁程度，引入二阶动量——每个维度上历史梯度值的平方和：
<span class="math display">\[
grad\_squared +=\nabla^2 f(x_t) \\
x_{t+1}=x_t-\cfrac{\alpha\nabla f(x_t)}{\sqrt{grad\_squared+10^{-7}}}
\]</span> 此时的学习率实质上是<span
class="math inline">\(\cfrac{\alpha}{\sqrt{grad\_squared}}\)</span>，为了避免除0，一般分母加上一个很小的平滑项。如果某个参数更新频繁，那么grad_squared就会增大，学习率也就越小。</p>
<p>AdaGrad的实现： <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">grad_sq += grads**2</span><br><span class="line">x -= lr * grads / (numpy.sqrt(grad_sq) + eps)</span><br></pre></td></tr></table></figure>
AdaGrad的问题在于随着grad_squared单调递增，学习率最终会单调衰减到0，意味着很可能会提早终止训练过程。
### 5 RMSProp/AdaDelta
为了缓解AdaGrad的学习率变化过于激进的问题，二阶动量的计算不累积全部的历史梯度，只关注过去某段时间内的梯度变化，用指数移动平均值来表示过去某时间段的二阶动量的均值：
<span class="math display">\[
grad\_squared=decay\_rate*grad\_squared+(1-decay\_rate)\nabla^2 f(x_t)
\\
x_{t+1}=x_t-\cfrac{\alpha\nabla f(x_t)}{\sqrt{grad\_squared+10^{-7}}}
\]</span> decay_rate是一个超参数，一般取值0.9。</p>
<p>RMSProp的实现： <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">grad_sq = decay * grad_sq + (1 - decay) * grads**2</span><br><span class="line">x -= lr * grads / (numpy.sqrt(grad_sq) + eps)</span><br></pre></td></tr></table></figure>
因此，RMSProp仍然是通过梯度的大小来调整每个参数的学习率，不过现在学习率不会单调递减。</p>
<h3 id="adam">6 Adam</h3>
<p>Adam的出现是集成了一阶动量思想和AdaGrad等的二阶动量思想，即Adaptive
Momentum： <span class="math display">\[
m_{t+1}=\beta_1m_t+(1-\beta_1)\nabla f(x_t)\\
V_{t+1}=\beta_2V_t+(1-\beta_2)\nabla^2 f(x_t)\\
x_{t+1}=x_t-\cfrac{\alpha m_{t+1}}{\sqrt{V_{t+1}+10^{-7}}}
\]</span>
由于m和V初始化为0，所以开始的几次迭代会偏向取值0，为了弥补这一缺点，又引入了偏差纠正项，完整的Adam算法如下：
<span class="math display">\[
m=\beta_1m+(1-\beta_1)\nabla f(x_t)\\
m_t=\cfrac{m}{1-\beta_1^t}\\
V=\beta_2V+(1-\beta_2)\nabla^2 f(x_t)\\
V_t=\cfrac{V}{1-\beta_2^t}\\
x_{t}=x_{t-1}-\cfrac{\alpha m_{t}}{\sqrt{V_{t}+10^{-7}}}
\]</span> Adam的实现： <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">m = beta_1 * m + (1 - beta_1) * grads</span><br><span class="line">m_t = m / (1 - beta_1**t)</span><br><span class="line">v = beta_2 * v + (1 - beta_2) * grads**2</span><br><span class="line">v_t = v / (1 - beta_2**t)</span><br><span class="line">x -= lr * m_t / (numpy.sqrt(v_t) + eps)</span><br></pre></td></tr></table></figure>
如果Adam再加上Nesterov的向后看一步的思想，就是Nadam算法。 ## Experiment
为了对上述算法有更加直观的认识，同时在部分程度上比较不同算法的性能，构造一维函数<span
class="math inline">\(f(x)\)</span>作为损失函数，其表达式如下： <span
class="math display">\[
f(x)=0.01x^2+sin(x)+\frac{1}{3}cos(3x)+\frac{1}{5}sin(5x)+\frac{1}{7}cos(7x)
\]</span> 这个损失函数含有大量的局部最小点以及悬崖，如图所示： <img
src="https://img-blog.csdnimg.cn/2021071019313296.png"
alt="在这里插入图片描述" />
为了公平起见，比较时将x的初始值设为-29，每种算法的迭代次数均设置为300次，学习率均设置为0.1，迭代过程如下图所示：
<img src="https://img-blog.csdnimg.cn/20210710193206336.png"
alt="在这里插入图片描述" /> 最终的收敛结果如下表所示： | 算法 | 最终x |
最终损失 | | -------- | ------ | -------- | | SGD | -27.98 | 7.19 | |
Momentum | -24.00 | 6.21 | | Nesterov | -24.00 | 6.21 | | AdaGrad |
-27.98 | 7.19 | | RMSProp | -28.95 | 9.10 | | Adam | -26.46 | 5.59 |</p>
<p>从上图和上表可以看到：Adam算法在前期收敛很快，并且最终效果最好，是综合性能最佳的算法；带动量的SGD能够越过一些局部极小值，在没有精细调参的情况下一度达到了和Adam类似的效果；AdaGrad开始时的梯度很大，但是由于学习率过早地减小，最终效果并不出众；这些结果进一步佐证了之前对各种算法的分析。</p>
<p>如果将学习率设置为0.01，对比如下： <img
src="https://img-blog.csdnimg.cn/20210710193256448.png"
alt="在这里插入图片描述" />
可以看到：精调后的Momentum、Nesterov和Adam的效果几乎不相上下，这只是初步调整了学习率参数，如果通过验证集更加精细地调整超参数的值，那么SGD+Momentum完全可以达到甚至超越Adam的表现，当然这也需要人为付出更多的努力，Adam这个烦恼则小得多。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Created on Sun Apr 11 18:31:58 2021</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">@author: Jingtao Ren</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">a = tf.constant(<span class="number">3.0</span>)</span><br><span class="line">b = tf.constant(<span class="number">5.0</span>)</span><br><span class="line">c = tf.constant(<span class="number">7.0</span>)</span><br><span class="line">d = tf.constant(<span class="number">0.1</span>)</span><br><span class="line">x = tf.Variable(initial_value=-<span class="number">29.0</span>, name=<span class="string">&quot;x&quot;</span>, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_f</span>():</span></span><br><span class="line">    x = np.linspace(-<span class="number">30</span>, <span class="number">30</span>, <span class="number">1000</span>)</span><br><span class="line">    <span class="comment"># y = -20.0 * np.exp(b * np.abs(x)) - np.exp(np.cos(c * x)) + 20.0 + np.exp(1)</span></span><br><span class="line">    y = (<span class="number">0.1</span> * x) ** <span class="number">2</span> + np.sin(x) + np.cos(a * x) / a + np.sin(b * x) / b + np.cos(c * x) / c</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;y&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">&#x27;Loss Function&#x27;</span>)</span><br><span class="line">    plt.plot(x, y)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_train</span>(<span class="params">y</span>):</span></span><br><span class="line">    x = np.arange(<span class="number">300</span>)</span><br><span class="line">    labels = [<span class="string">&#x27;SGD&#x27;</span>, <span class="string">&#x27;Momentum&#x27;</span>, <span class="string">&#x27;Nesterov&#x27;</span>, <span class="string">&#x27;AdaGrad&#x27;</span>, <span class="string">&#x27;RMSProp&#x27;</span>, <span class="string">&#x27;Adam&#x27;</span>]</span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Iteration&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">&#x27;Algorithm Comparison&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">6</span>):    </span><br><span class="line">        plt.plot(x, y[i], label=labels[i])</span><br><span class="line">        plt.legend()</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span>():</span></span><br><span class="line">    <span class="comment"># y = a * tf.exp(b * tf.abs(x)) - tf.exp(tf.cos(c * x)) - a + tf.exp(tf.constant(1.0))</span></span><br><span class="line">    y = tf.<span class="built_in">pow</span>(d * x, <span class="number">2</span>) + tf.sin(x) + tf.cos(a * x) / a + tf.sin(b * x) / b + tf.cos(c * x) / c</span><br><span class="line">    <span class="keyword">return</span> (y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">minimize</span>(<span class="params">optimizer, iters = <span class="number">300</span></span>):</span></span><br><span class="line">    y = []</span><br><span class="line">    <span class="comment"># optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> tf.<span class="built_in">range</span>(iters):</span><br><span class="line">        optimizer.minimize(loss, [x])</span><br><span class="line">        y.append(loss())</span><br><span class="line">    tf.<span class="built_in">print</span>(<span class="string">&quot;Final x = &quot;</span>, x)</span><br><span class="line">    tf.<span class="built_in">print</span>(<span class="string">&quot;Final Loss = &quot;</span>, loss())</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># plot_f()</span></span><br><span class="line">    ops = [tf.keras.optimizers.SGD(learning_rate=<span class="number">0.1</span>), tf.keras.optimizers.SGD(learning_rate=<span class="number">0.1</span>, momentum=<span class="number">0.9</span>),</span><br><span class="line">           tf.keras.optimizers.SGD(learning_rate=<span class="number">0.1</span>, momentum=<span class="number">0.9</span>, nesterov=<span class="literal">True</span>), tf.keras.optimizers.Adagrad(learning_rate=<span class="number">0.1</span>),</span><br><span class="line">           tf.keras.optimizers.Adadelta(learning_rate=<span class="number">0.1</span>, rho=<span class="number">0.9</span>), tf.keras.optimizers.Adam(learning_rate=<span class="number">0.1</span>)]</span><br><span class="line">    y = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">6</span>):</span><br><span class="line">        x.assign(-<span class="number">29.0</span>)</span><br><span class="line">        y.append(minimize(ops[i]))</span><br><span class="line">    plot_train(y)</span><br></pre></td></tr></table></figure>
<p>当然，这个实验非常简单，损失函数形式是一维的，实际中的网络模型参数的数量可能达到百万级别，超高维情况下算法的效率、鲁棒性以及模型最终的泛化能力才是我们真正关心的。</p>
<p>最后贴2张神图总结下： <img
src="https://img-blog.csdnimg.cn/20210306203509952.gif#pic_center"
alt="在这里插入图片描述" /><img
src="https://img-blog.csdnimg.cn/2021030620351970.gif#pic_center"
alt="在这里插入图片描述" /> ## Research
Adam虽然是集大成者，而且也被推荐为起始的默认优化算法，但是一些Paper揭示了Adam的一些问题。
### 1 过拟合 Berkeley在NIPS
2017的一篇文章指出：如果一个问题有多个全局极优，即使从相同的初始值出发，不同的优化算法也会得到完全不同的结果。文章构造了一个简单的线性可分的二分类问题，证明了SGD在这种情况下测试误差为0，而AdaGrad等自适应方法会把所有的测试样例分为正类，泛化能力极差，也就是根本不能工作。</p>
<p>随后作者又用VGG+BN+Dropout的网络结构在CIFAR-10数据集上进行了实验：
<img src="https://img-blog.csdnimg.cn/20210710193712619.png"
alt="在这里插入图片描述" />
可以看到：前期训练中Adam有优势，但SGD的泛化能力确实比Adam要好。</p>
<p>最后，为了彻底黑化Adam，文章又用了文本数据集和一些NLP模型做了实验：
<img src="https://img-blog.csdnimg.cn/20210710193823396.png"
alt="在这里插入图片描述" />
即便有时候自适应方法的训练loss会更低，但SGD的泛化能力都无一例外地胜过了自适应的方法。自适应方法在训练初期速度很快，但是后期表现平平。</p>
<p>泛化能力差的原因在于：自适应方法倾向于关注稀疏的特征，因为这些特征对于训练样例的鉴别是很有效的，尤其在训练样例数少而特征较多的数据集中，但是这些特征其实并非关键特征，这样自适应学习率算法出现过拟合的风险就会增大，导致泛化能力不佳，最终的收敛效果不如传统的SGD。
### 2 二阶动量波动
Google的一篇文章从数学上证明了在某些特定情况下Adam可能不收敛，因为二阶动量取的是某个时间窗口的变化，所以<span
class="math inline">\(V_t\)</span>的变化可能会剧烈震荡，尤其在高维情况下，梯度的方差可能随时间波动很大，导致学习率震荡，模型无法收敛。这也是为什么一般<span
class="math inline">\(\beta_2\)</span>要取0.999这么大的值，避免二阶动量有太大波动。</p>
<p>一般认为Adam默认的<span class="math inline">\(\beta_1\)</span>和<span
class="math inline">\(\beta_2\)</span>不需要调整，采用默认的0.9和0.999即可。但是这两个超参如果不按这样设置，Adam可能永远不会收敛到最优值。文章从数学上证明了对任意的<span
class="math inline">\(\beta_1,\beta_2\in[0,1),\beta_1&lt;\sqrt{\beta_2}\)</span>，都存在一个随机的凸优化问题使得Adam不能收敛到最优解。</p>
<p>为了避免二阶动量的剧烈震荡，文章对其进行了控制，提出了一个新算法AMSGrad确保模型收敛，<span
class="math inline">\(V_t=max(V_{t-1},\beta_2V_{t-1}+(1-\beta_2)\nabla^2
f(x_t))\)</span>。</p>
<p>作者随后通过人造数据和真实数据进行了实验：</p>
<p>人造数据上的结果： <img
src="https://img-blog.csdnimg.cn/20210710193934596.png"
alt="在这里插入图片描述" />
很显然在Adam没有找到最优解的这些数据上，改进后的算法都表现良好。</p>
<p>在MNIST上的效果： <img
src="https://img-blog.csdnimg.cn/20210710193956571.png"
alt="在这里插入图片描述" />
这篇文章最终获得了2018年ICLR最佳论文，但是引起了很大争议。主要原因在于其构造的令Adam失效的数据在实际情况中出现的概率极低，即使出现也会在数据预处理时被筛掉，因此并没有特别广泛的实际用处。另外，文章过于强调训练集上的损失函数值，甚至有人通过复现表明文章提出的AMSGrad算法在测试数据上表现很差，与原文中的某些结论相互矛盾。
### 3 学习率下降
arXiv上的一篇文章通过在CIFAR-10上的实验证明Adam在一些情况下虽然速度快，但收敛效果没有SGD好：
<img src="https://img-blog.csdnimg.cn/20210710194231292.png"
alt="在这里插入图片描述" />
文章通过实验发现主要原因在于后期Adam的学习率过低，影响了最终效果。文章尝试通过控制学习率下界，提高了最终收敛效果。</p>
<p>既然Adam后期有问题，那么一个自然的改进就是前期训练使用Adam，用来快速减小loss；后期训练转换为SGD，用稍慢的速度寻找更佳的解甚至是最优解。但是这样也会引入新的问题：在什么时刻切换？切换为SGD后的学习率又该如何设置？</p>
<p>文章提出了SWATS(Switches from Adam to
SGD)策略来解决上面2个问题，在CIFAR-10和CIFAR-100数据集上实验效果看着还不错：
<img src="https://img-blog.csdnimg.cn/20210710194309531.png"
alt="在这里插入图片描述" /> <img
src="https://img-blog.csdnimg.cn/20210710194329636.png"
alt="在这里插入图片描述" />这些文章都采用了一些比较极端的数据去探索Adam的不适情况，然而实际中遇到这些极端情况的概率并不大，因此Adam并不失为首选尝试。通过上面的讨论可以看到：SGD和Adam各有优劣，精调后的SGD一般最终会收敛到更好的效果；Adam在训练前期收敛速度快，在稀疏数据上表现更好，对超参不敏感，不需要十分精细的调参。</p>
<p>如果对优化算法不熟悉，可以先尝试SGD+Nesterov
Momentum或者Adam；如果对某个优化算法很精通，那么调参就会相对容易些。如果资源足够，也可以尝试L-BFGS等二阶优化方法。另外，选择之前要充分了解数据的性质，对于比较稀疏的数据可以优先尝试学习率自适应调整的算法。</p>
<h2 id="reference">Reference</h2>
<p>[1] CS231n: Convolutional Neural Networks for Visual Recognition.
lecture 8, Stanford University.<br />
[2] The Marginal Value of Adaptive Gradient Methods in Machine Learning.
NIPS'17<br />
[3] On the Convergence of Adam and Beyond. ICLR'18<br />
[4] Improving Generalization Performance by Switching from Adam to SGD.
arXiv<br />
[5] Optimization methods for large-scale machine learning. SIAM Review,
2018.<br />
[6] Optimization for deep learning: theory and algorithms. arXiv,
2019.<br />
[7] Understanding Black-box Predictions via Influence Functions.
ICML'17.<br />
[8] Understanding Deep Learning Requires Rethinking Generalization.
ICLR'17.</p>

    </div>

    
    
    
        <div class="reward-container">
  <div></div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    Donate
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechatpay.png" alt="EIMadrigal WeChat Pay">
        <p>WeChat Pay</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/alipay.png" alt="EIMadrigal Alipay">
        <p>Alipay</p>
      </div>

  </div>
</div>

        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>Post author:  </strong>EIMadrigal
  </li>
  <li class="post-copyright-link">
    <strong>Post link: </strong>
    <a href="https://eimadrigal.github.io/posts/optimization-methods-in-dl/" title="Optimization Methods in Deep Learning">https://eimadrigal.github.io/posts/optimization-methods-in-dl/</a>
  </li>
  <li class="post-copyright-license">
    <strong>Copyright Notice:  </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> unless stating additionally.
  </li>
</ul>
</div>


      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/Machine-Learning/" rel="tag"><i class="fa fa-tag"></i> Machine Learning</a>
              <a href="/tags/Paper/" rel="tag"><i class="fa fa-tag"></i> Paper</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/posts/gradient-boosting/" rel="prev" title="Gradient Boosting">
      <i class="fa fa-chevron-left"></i> Gradient Boosting
    </a></div>
      <div class="post-nav-item">
    <a href="/posts/thoughts-on-career/" rel="next" title="关于职业发展的考虑">
      关于职业发展的考虑 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="gitalk-container"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#background"><span class="nav-text">Background</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#adam"><span class="nav-text">6 Adam</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#reference"><span class="nav-text">Reference</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="EIMadrigal"
      src="/images/favicon.png">
  <p class="site-author-name" itemprop="name">EIMadrigal</p>
  <div class="site-description" itemprop="description">Hello World</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">191</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">12</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">30</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/EIMadrigal" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;EIMadrigal" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:jingtao.ren@outlook.com" title="E-Mail → mailto:jingtao.ren@outlook.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.linkedin.com/in/jingtaoren/" title="LinkedIn → https:&#x2F;&#x2F;www.linkedin.com&#x2F;in&#x2F;jingtaoren&#x2F;" rel="noopener" target="_blank"><i class="fab fa-linkedin fa-fw"></i>LinkedIn</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/EIMadrigal" title="Weibo → https:&#x2F;&#x2F;weibo.com&#x2F;EIMadrigal" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i>Weibo</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2018 – 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">EIMadrigal</span>
</div>


    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <span id="busuanzi_container_site_pv">Total views: <span id="busuanzi_value_site_pv"></span></span>
    <span class="post-meta-divider">|</span>
    <span id="busuanzi_container_site_uv">Total visitors: <span id="busuanzi_value_site_uv"></span></span>
    <span class="post-meta-divider">|</span>

<script>
$(document).ready(function() {

    var int = setInterval(fixCount, 50);
    var countOffset = 20000;

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset);
            clearInterval(int);
        }  
    }
       	
});
</script> 

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/pangu@4/dist/browser/pangu.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  
  <script>
    (function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();
  </script>




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : '285afe6e2d34f997b740',
      clientSecret: 'f3e77970aacbc5a14421e48db34996ed8223f7e9',
      repo        : 'EIMadrigal.github.io',
      owner       : 'EIMadrigal',
      admin       : ['EIMadrigal'],
      id          : 'dc9d97dc10ce304d3bc2d0a02e2bba38',
        language: '',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script>

</body>
</html>
