<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"eimadrigal.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Hello World">
<meta property="og:type" content="website">
<meta property="og:title" content="EI Madrigal&#39;s Space">
<meta property="og:url" content="https://eimadrigal.github.io/page/3/index.html">
<meta property="og:site_name" content="EI Madrigal&#39;s Space">
<meta property="og:description" content="Hello World">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="EIMadrigal">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://eimadrigal.github.io/page/3/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>EI Madrigal's Space</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">EI Madrigal's Space</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://eimadrigal.github.io/2021/07/10/Optimization%20Methods%20in%20Deep%20Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/favicon.png">
      <meta itemprop="name" content="EIMadrigal">
      <meta itemprop="description" content="Hello World">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EI Madrigal's Space">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/07/10/Optimization%20Methods%20in%20Deep%20Learning/" class="post-title-link" itemprop="url">Optimization Methods in Deep Learning</a>
        </h2>

        <div class="post-meta">

		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-07-10 11:47:00" itemprop="dateCreated datePublished" datetime="2021-07-10T11:47:00+08:00">2021-07-10</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="background">Background</h2>
<p>深度神经网络的训练过程主要通过求解一个特定的优化问题来实现，然而由于该问题是一个复杂的高维非线性优化问题，并且不同的网络结构差异很大，不能将传统的优化方法直接使用。即使数据集和网络结构完全相同，不同的优化算法也可能导致完全不同的收敛效果。实际应用的一些简单方法虽然行之有效，但现有理论无法充分解释其有效性，超参数的不断增加也给优化增加了不少难度。如何确保算法收敛、如何尽快收敛以及能否收敛到全局最优一直是困扰学术界和工业界的问题。如果能够用优化理论去解释神经网络的训练行为，对于深度学习的推广应用将会起到巨大的推动作用。</p>
<p>对于有监督学习，给定包含n个样本的训练集<span class="math inline">\(\{(\mathbf{x}_1, y_1), \ldots, (\mathbf{x}_n, y_n)\}\)</span>，<span class="math inline">\(\mathbf{x}\)</span>表示样本的特征向量，<span class="math inline">\(y\)</span>表示该样本对应的标签。我们的任务是利用样本信息来预测相应的标签，使预测值尽可能接近真实标签。如果用深度学习来完成这个任务，就需要通过调整神经网络的参数（权重W和偏差b）来近似数据背后的函数映射关系，这个关系往往是高度非线性的，网络越深表达能力也就越强，逼近效果的精度也就更高，因此网络结构很可能是极其复杂的。</p>
<p>为了衡量预测值和真实值之间的接近程度，通常需要采用某种距离度量方式<span class="math inline">\(l\)</span>，<span class="math inline">\(l\)</span>一般设计为<strong>可微</strong>的，接着用一些优化算法去最小化该目标函数。因此优化问题变为寻找最佳的参数使得<span class="math inline">\(l\)</span>最小，在不考虑正则项的情况下有： <span class="math display">\[
\mathop{\mathrm{min}}_f \frac{1}{n} \sum_{i=1}^n l(f(\mathbf{x}_i), y_i)
\]</span> <span class="math inline">\(f\)</span>就是我们从输入到输出的映射函数，<span class="math inline">\(l\)</span>通常也叫损失函数，衡量预测值<span class="math inline">\(f(x_i)\)</span>和真实标签<span class="math inline">\(y_i\)</span>的差距，比如回归问题中经常使用的平方损失函数<span class="math inline">\(l=||f(x_i)-y_i||^2\)</span>。</p>
<p>需要注意的是：深度学习中的优化问题与传统意义上的优化问题有所差别。传统的优化问题需要尽可能找到目标函数的最值，而深度学习的最终目的是为了<strong>预测未知</strong>的数据，而不是将训练数据上的损失降到最低。我们定义的损失函数<span class="math inline">\(J(\Theta)\)</span>衡量的是当前模型参数<span class="math inline">\(\Theta\)</span>在<strong>训练集</strong>上的优劣，然而，最小化训练误差并不意味着模型的泛化误差也会最小，为了降低泛化误差我们还需要关注过拟合问题，因此损失函数往往要加上<strong>正则项</strong>。统计学上称为经验风险最小化，即由于无法获得全部数据，所以只能用经验风险作为实际风险的近似。非常有意思的是：尽管大多数神经网络都是严重过参数化的，但是反而有着比较不错的泛化能力，这与传统的机器学习观点是矛盾的，泛化理论也需要更加深入的研究。</p>
<p>深度学习中的<span class="math inline">\(f\)</span>通常是多层的复合函数，由于太复杂而无法求出解析解，所以要用数值优化算法去求解。实际中主流的深度学习优化算法都利用梯度下降来求解，梯度下降是深度学习优化算法的基础，尽管目前已经很少直接使用，但它却是其他高级优化算法的基石： 假设网络的参数为<span class="math inline">\(x=(x_1,x_2,...,x_d)^T\)</span>，优化的目标函数为<span class="math inline">\(f\)</span>，那么<span class="math inline">\(f\)</span>的梯度为： <span class="math display">\[
\nabla f(\mathbf{x}) = \bigg[\frac{\partial f(\mathbf{x})}{\partial x_1}, \frac{\partial f(\mathbf{x})}{\partial x_2}, \ldots, \frac{\partial f(\mathbf{x})}{\partial x_d}\bigg]^\top
\]</span> 每个元素对应着目标函数在该方向上的变化率，因此只要沿着梯度的反方向就可以使目标函数减小得最快：<span class="math inline">\(\mathbf{x} \leftarrow \mathbf{x} - \eta \nabla f(\mathbf{x})\)</span>，<span class="math inline">\(\eta\)</span>是一个被称为学习率的超参数，用来控制每一步的大小。<span class="math inline">\(\eta\)</span>过小，收敛过程极度缓慢；<span class="math inline">\(\eta\)</span>过大，可能造成损失函数在最小点附近波动甚至发散。学习率的调整是神经网络训练过程中一个重要的调整参数，常常使人头痛不已，因此也出现了很多学习率自适应调整的算法，将在下面深入分析这些算法的优劣。</p>
<p>有了优化模型及最基础的求解方法后，我们需要对其性质和优缺点进行分析，以便于后续的改进。深度学习的优化问题大多是非凸的，因此存在很多挑战：</p>
<ol type="1">
<li>局部最优：对于凸优化问题，局部最优即是全局最优。然而对于非凸问题，当损失函数到达局部最优点时，<span class="math inline">\(J(\Theta)\)</span>的梯度为0，<span class="math inline">\(\Theta\)</span>无法继续更新，损失函数无法继续下降；</li>
<li>鞍点：该点既不是局部最小也不是全局最小，但是该点的梯度消失，无法继续更新；</li>
<li>梯度消失/爆炸：由于初始值和激活函数选择不当 (如sigmoid)，当梯度反向回传时，可能在某一层求导后梯度值很小/很大，导致训练速度极其缓慢。因此初始值的选择通常采用很小的随机数，避免收敛到比较差的区域，激活函数通常也会选择ReLU，避免梯度消失问题。</li>
</ol>
<p>局部最小和鞍点示意图如下： <img src="https://img-blog.csdnimg.cn/20210710191530881.png" alt="在这里插入图片描述" /> 尤其在高维空间中，鞍点的问题变得更加严重：假设<span class="math inline">\(\Theta\)</span>是一个k维向量，<span class="math inline">\(J(\Theta)\)</span>的海森矩阵就有k个特征值，其梯度为0的点有可能是局部最小（特征值均为正）、局部最大（特征值均为负）或者是鞍点（特征值有正有负）。高维空间中特征值有正有负的概率很大，因此鞍点出现的可能性远大于局部最优点出现的可能性，并且鞍点周围的平坦区域可能很大，需要增加噪声扰动来逃离鞍点。</p>
<p>由于上述问题的存在，通常很难找到<span class="math inline">\(J(\Theta)\)</span>的全局最优解，但实际上为了减少过拟合的风险我们并不需要训练集上的全局最优，经典的梯度下降就可以带来足够的局部最优。</p>
<p>分析完优化模型本身的问题，再来看看最基础的GD的问题：目标函数通常是训练集中所有样本的损失的平均值，故目标函数的梯度为： <span class="math display">\[
\nabla f(\mathbf{x}) = \frac{1}{n} \sum_{i = 1}^n \nabla f_i(\mathbf{x})
\]</span> 如果用Full-batch GD，那么每次迭代每个参数的梯度计算的时间复杂度为<span class="math inline">\(O(n)\)</span>，对于大规模数据，这样的更新速度显然无法令人忍受。</p>
<p>学习率的选择是一项重要的调参工作，因此学习率的自适应变化就成为了研究热点之一，一些二阶方法应运而生，我们首先来看看牛顿法该如何解决这个问题。</p>
<p>对于损失函数<span class="math inline">\(f\)</span>，利用泰勒展开式有： <span class="math display">\[
f(\mathbf{x} + \boldsymbol{\epsilon}) = f(\mathbf{x}) + \boldsymbol{\epsilon}^\top \nabla f(\mathbf{x}) + \frac{1}{2} \boldsymbol{\epsilon}^\top \nabla^2 f(\mathbf{x}) \boldsymbol{\epsilon} + \mathcal{O}(\|\boldsymbol{\epsilon}\|^3)
\]</span> 式中的<span class="math inline">\(\nabla^2 f(\mathbf{x})\)</span>即<span class="math inline">\(d*d\)</span>海森矩阵，存储了函数的二阶偏导数。为了求得<span class="math inline">\(f\)</span>的最小值，令上式对<span class="math inline">\(\epsilon\)</span>求导得0，有：<span class="math inline">\(\boldsymbol{\epsilon}=-\nabla f(\mathbf{x})H^{-1}\)</span>，即每次的参数更新为<span class="math inline">\(\mathbf{x} \leftarrow \mathbf{x} - \nabla f(\mathbf{x})H^{-1}\)</span>。二阶近似利用了损失函数的曲率信息，即如果曲率比较小，那么这步更新就会比较大，反之则更新较小。这里没有了学习率，而是通过“梯度的梯度”自动调整步幅，看起来比一阶的梯度下降要好一些。</p>
<p>然而深度学习的参数空间往往十分巨大，因此存储和计算海森矩阵的逆是不现实的，这也是牛顿法无法在DNN中使用的重要原因。为了缓解这个问题，学术界提出了一些拟牛顿法如L-BFGS等试图去降低存储消耗，但是计算代价仍然很高。</p>
<p>从以上的分析可以看到：无论是Full-batch GD还是牛顿法，都存在计算消耗大等问题，不适用于深度学习任务的大规模数据集训练，因此已经很少被直接用在深度学习模型中。为了处理这些问题，学术界提出了很多替代的优化算法，因此接下来我将调研分析当前常用的深度学习优化算法 (SGD/Adam...)的优缺点，并结合实例及前沿研究进行相关讨论。 ## Popular Algorithms 优化算法在神经网络的训练中有着举足轻重的作用，选择合适的优化算法可以使得损失函数收敛地更快，同时收敛到更好的区域。目前比较流行的算法有下面几种： ### 1 SGD 尝试用mini-batch的梯度平均值作为整体梯度的无偏估计，参数的更新非常简单，沿着梯度的反方向即是loss下降最快的方向： <span class="math display">\[
x_{t+1}=x_t-\alpha\nabla f(x_t)
\]</span> 如果是Batch GD并且学习率足够小时可以保证损失函数单调不增。实际使用时一般会采用学习率递减策略保证模型收敛。</p>
<p>实现也非常简单：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x -= lr * grads</span><br></pre></td></tr></table></figure>
<p>SGD存在几个问题：</p>
<p>首先，如果loss对于不同参数的敏感程度不同，那么收敛过程会在敏感参数方向上抖动： <img src="https://img-blog.csdnimg.cn/20210710192205509.png" /> 对于非常大的参数空间，可能会收敛到不同的区域。 其次，如果loss函数有局部最优或者鞍点，这些点上梯度为0，无法收敛到全局最优； 最后，如果采用mini-batch，那么计算出的梯度值是有噪声的，意味着收敛过程可能会是非常曲折的，也即需要更多时间。 ### 2 SGD+Momentum 为了解决SGD的问题，有学者提出了带有动量的SGD，其思想也很简单：更新参数时不仅考虑当前的梯度方向，还要考虑历史累积梯度方向，如果两者方向一致，那么这一步更新幅度就会增大；如果不一致，就会减弱沿当前梯度的下降幅度。 <span class="math display">\[
v_{t+1}=\rho v_t+\nabla f(x_t) \\
x_{t+1}=x_t-\alpha v_{t+1}
\]</span> <span class="math inline">\(\rho\)</span>可以看作是对历史梯度的衰减，一般取0.9。</p>
<p>带有动量的SGD实现： <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">v = rho * v + grads</span><br><span class="line">x -= lr * v</span><br></pre></td></tr></table></figure> 这样就解决了SGD的三个问题： 首先，由于历史梯度的存在，朝敏感方向步进的数量就会减少，会更加平滑的向最优点前进，减小了震荡，加速收敛； 其次，对于局部最优点，虽然当前梯度为0，但是依靠历史梯度可以越过该点继续下降； 最后，梯度噪声引起的震荡可以通过历史梯度互相抵消。 ### 3 Nesterov Momentum Nesterov Momentum由SGD+Momentum衍生而来，SGD+Momentum是将当前点的梯度和速度结合起来，而Nesterov Momentum则是将当前点的速度和下一个近似点的梯度结合起来，意味着我们不是在当前位置去看未来，而是多看了一步，在稍远一些的下一步看未来，可以提前调整步进大小： <img src="https://img-blog.csdnimg.cn/20210710192712932.png" alt="在这里插入图片描述" /> 所以Nesterov Momentum的更新规则为： <span class="math display">\[
v_{t+1}=\rho v_t-\alpha\nabla f(x_t+\rho v_t) \\
x_{t+1}=x_t+v_{t+1}
\]</span> 通常我们希望针对<span class="math inline">\(x_t\)</span>计算梯度，通过简单的变量替换，得到新的更新规则： <span class="math display">\[
v_{t+1}=\rho v_t-\alpha\nabla f(x_t) \\
x_{t+1}=x_t+v_{t+1}+\rho(v_{t+1}-v_t)
\]</span> Nesterov的实现： <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">v_prev = v</span><br><span class="line">v = rho * v - lr * grads</span><br><span class="line">x += (1 + rho) * v - rho * v_prev</span><br></pre></td></tr></table></figure> ### 4 AdaGrad 前面的几种方法都是设置了一个全局的学习率，AdaGrad则通过引入二阶动量使得学习率可以针对<strong>每个参数</strong>自适应地取值：对于更新频繁的参数，已经有了很多认知，不希望因为单个样本影响太大，所以学习率可以小一些；对于更新稀疏的参数，希望从偶尔出现的能更新该参数的样本中多获得一些信息，所以学习率可以设置地大一些。为了了解参数更新的频繁程度，引入二阶动量——每个维度上历史梯度值的平方和： <span class="math display">\[
grad\_squared +=\nabla^2 f(x_t) \\
x_{t+1}=x_t-\cfrac{\alpha\nabla f(x_t)}{\sqrt{grad\_squared+10^{-7}}}
\]</span> 此时的学习率实质上是<span class="math inline">\(\cfrac{\alpha}{\sqrt{grad\_squared}}\)</span>，为了避免除0，一般分母加上一个很小的平滑项。如果某个参数更新频繁，那么grad_squared就会增大，学习率也就越小。</p>
<p>AdaGrad的实现： <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">grad_sq += grads**2</span><br><span class="line">x -= lr * grads / (numpy.sqrt(grad_sq) + eps)</span><br></pre></td></tr></table></figure> AdaGrad的问题在于随着grad_squared单调递增，学习率最终会单调衰减到0，意味着很可能会提早终止训练过程。 ### 5 RMSProp/AdaDelta 为了缓解AdaGrad的学习率变化过于激进的问题，二阶动量的计算不累积全部的历史梯度，只关注过去某段时间内的梯度变化，用指数移动平均值来表示过去某时间段的二阶动量的均值： <span class="math display">\[
grad\_squared=decay\_rate*grad\_squared+(1-decay\_rate)\nabla^2 f(x_t) \\
x_{t+1}=x_t-\cfrac{\alpha\nabla f(x_t)}{\sqrt{grad\_squared+10^{-7}}}
\]</span> decay_rate是一个超参数，一般取值0.9。</p>
<p>RMSProp的实现： <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">grad_sq = decay * grad_sq + (1 - decay) * grads**2</span><br><span class="line">x -= lr * grads / (numpy.sqrt(grad_sq) + eps)</span><br></pre></td></tr></table></figure> 因此，RMSProp仍然是通过梯度的大小来调整每个参数的学习率，不过现在学习率不会单调递减。</p>
<h3 id="adam">6 Adam</h3>
<p>Adam的出现是集成了一阶动量思想和AdaGrad等的二阶动量思想，即Adaptive Momentum： <span class="math display">\[
m_{t+1}=\beta_1m_t+(1-\beta_1)\nabla f(x_t)\\
V_{t+1}=\beta_2V_t+(1-\beta_2)\nabla^2 f(x_t)\\
x_{t+1}=x_t-\cfrac{\alpha m_{t+1}}{\sqrt{V_{t+1}+10^{-7}}}
\]</span> 由于m和V初始化为0，所以开始的几次迭代会偏向取值0，为了弥补这一缺点，又引入了偏差纠正项，完整的Adam算法如下： <span class="math display">\[
m=\beta_1m+(1-\beta_1)\nabla f(x_t)\\
m_t=\cfrac{m}{1-\beta_1^t}\\
V=\beta_2V+(1-\beta_2)\nabla^2 f(x_t)\\
V_t=\cfrac{V}{1-\beta_2^t}\\
x_{t}=x_{t-1}-\cfrac{\alpha m_{t}}{\sqrt{V_{t}+10^{-7}}}
\]</span> Adam的实现： <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">m = beta_1 * m + (1 - beta_1) * grads</span><br><span class="line">m_t = m / (1 - beta_1**t)</span><br><span class="line">v = beta_2 * v + (1 - beta_2) * grads**2</span><br><span class="line">v_t = v / (1 - beta_2**t)</span><br><span class="line">x -= lr * m_t / (numpy.sqrt(v_t) + eps)</span><br></pre></td></tr></table></figure> 如果Adam再加上Nesterov的向后看一步的思想，就是Nadam算法。 ## Experiment 为了对上述算法有更加直观的认识，同时在部分程度上比较不同算法的性能，构造一维函数<span class="math inline">\(f(x)\)</span>作为损失函数，其表达式如下： <span class="math display">\[
f(x)=0.01x^2+sin(x)+\frac{1}{3}cos(3x)+\frac{1}{5}sin(5x)+\frac{1}{7}cos(7x)
\]</span> 这个损失函数含有大量的局部最小点以及悬崖，如图所示： <img src="https://img-blog.csdnimg.cn/2021071019313296.png" alt="在这里插入图片描述" /> 为了公平起见，比较时将x的初始值设为-29，每种算法的迭代次数均设置为300次，学习率均设置为0.1，迭代过程如下图所示： <img src="https://img-blog.csdnimg.cn/20210710193206336.png" alt="在这里插入图片描述" /> 最终的收敛结果如下表所示： | 算法 | 最终x | 最终损失 | | -------- | ------ | -------- | | SGD | -27.98 | 7.19 | | Momentum | -24.00 | 6.21 | | Nesterov | -24.00 | 6.21 | | AdaGrad | -27.98 | 7.19 | | RMSProp | -28.95 | 9.10 | | Adam | -26.46 | 5.59 |</p>
<p>从上图和上表可以看到：Adam算法在前期收敛很快，并且最终效果最好，是综合性能最佳的算法；带动量的SGD能够越过一些局部极小值，在没有精细调参的情况下一度达到了和Adam类似的效果；AdaGrad开始时的梯度很大，但是由于学习率过早地减小，最终效果并不出众；这些结果进一步佐证了之前对各种算法的分析。</p>
<p>如果将学习率设置为0.01，对比如下： <img src="https://img-blog.csdnimg.cn/20210710193256448.png" alt="在这里插入图片描述" /> 可以看到：精调后的Momentum、Nesterov和Adam的效果几乎不相上下，这只是初步调整了学习率参数，如果通过验证集更加精细地调整超参数的值，那么SGD+Momentum完全可以达到甚至超越Adam的表现，当然这也需要人为付出更多的努力，Adam这个烦恼则小得多。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Created on Sun Apr 11 18:31:58 2021</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">@author: Jingtao Ren</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">a = tf.constant(<span class="number">3.0</span>)</span><br><span class="line">b = tf.constant(<span class="number">5.0</span>)</span><br><span class="line">c = tf.constant(<span class="number">7.0</span>)</span><br><span class="line">d = tf.constant(<span class="number">0.1</span>)</span><br><span class="line">x = tf.Variable(initial_value=-<span class="number">29.0</span>, name=<span class="string">&quot;x&quot;</span>, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_f</span>():</span></span><br><span class="line">    x = np.linspace(-<span class="number">30</span>, <span class="number">30</span>, <span class="number">1000</span>)</span><br><span class="line">    <span class="comment"># y = -20.0 * np.exp(b * np.abs(x)) - np.exp(np.cos(c * x)) + 20.0 + np.exp(1)</span></span><br><span class="line">    y = (<span class="number">0.1</span> * x) ** <span class="number">2</span> + np.sin(x) + np.cos(a * x) / a + np.sin(b * x) / b + np.cos(c * x) / c</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;y&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">&#x27;Loss Function&#x27;</span>)</span><br><span class="line">    plt.plot(x, y)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_train</span>(<span class="params">y</span>):</span></span><br><span class="line">    x = np.arange(<span class="number">300</span>)</span><br><span class="line">    labels = [<span class="string">&#x27;SGD&#x27;</span>, <span class="string">&#x27;Momentum&#x27;</span>, <span class="string">&#x27;Nesterov&#x27;</span>, <span class="string">&#x27;AdaGrad&#x27;</span>, <span class="string">&#x27;RMSProp&#x27;</span>, <span class="string">&#x27;Adam&#x27;</span>]</span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Iteration&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">&#x27;Algorithm Comparison&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">6</span>):    </span><br><span class="line">        plt.plot(x, y[i], label=labels[i])</span><br><span class="line">        plt.legend()</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span>():</span></span><br><span class="line">    <span class="comment"># y = a * tf.exp(b * tf.abs(x)) - tf.exp(tf.cos(c * x)) - a + tf.exp(tf.constant(1.0))</span></span><br><span class="line">    y = tf.<span class="built_in">pow</span>(d * x, <span class="number">2</span>) + tf.sin(x) + tf.cos(a * x) / a + tf.sin(b * x) / b + tf.cos(c * x) / c</span><br><span class="line">    <span class="keyword">return</span> (y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">minimize</span>(<span class="params">optimizer, iters = <span class="number">300</span></span>):</span></span><br><span class="line">    y = []</span><br><span class="line">    <span class="comment"># optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> tf.<span class="built_in">range</span>(iters):</span><br><span class="line">        optimizer.minimize(loss, [x])</span><br><span class="line">        y.append(loss())</span><br><span class="line">    tf.<span class="built_in">print</span>(<span class="string">&quot;Final x = &quot;</span>, x)</span><br><span class="line">    tf.<span class="built_in">print</span>(<span class="string">&quot;Final Loss = &quot;</span>, loss())</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># plot_f()</span></span><br><span class="line">    ops = [tf.keras.optimizers.SGD(learning_rate=<span class="number">0.1</span>), tf.keras.optimizers.SGD(learning_rate=<span class="number">0.1</span>, momentum=<span class="number">0.9</span>),</span><br><span class="line">           tf.keras.optimizers.SGD(learning_rate=<span class="number">0.1</span>, momentum=<span class="number">0.9</span>, nesterov=<span class="literal">True</span>), tf.keras.optimizers.Adagrad(learning_rate=<span class="number">0.1</span>),</span><br><span class="line">           tf.keras.optimizers.Adadelta(learning_rate=<span class="number">0.1</span>, rho=<span class="number">0.9</span>), tf.keras.optimizers.Adam(learning_rate=<span class="number">0.1</span>)]</span><br><span class="line">    y = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">6</span>):</span><br><span class="line">        x.assign(-<span class="number">29.0</span>)</span><br><span class="line">        y.append(minimize(ops[i]))</span><br><span class="line">    plot_train(y)</span><br></pre></td></tr></table></figure>
<p>当然，这个实验非常简单，损失函数形式是一维的，实际中的网络模型参数的数量可能达到百万级别，超高维情况下算法的效率、鲁棒性以及模型最终的泛化能力才是我们真正关心的。</p>
<p>最后贴2张神图总结下： <img src="https://img-blog.csdnimg.cn/20210306203509952.gif#pic_center" alt="在这里插入图片描述" /><img src="https://img-blog.csdnimg.cn/2021030620351970.gif#pic_center" alt="在这里插入图片描述" /> ## Research Adam虽然是集大成者，而且也被推荐为起始的默认优化算法，但是一些Paper揭示了Adam的一些问题。 ### 1 过拟合 Berkeley在NIPS 2017的一篇文章指出：如果一个问题有多个全局极优，即使从相同的初始值出发，不同的优化算法也会得到完全不同的结果。文章构造了一个简单的线性可分的二分类问题，证明了SGD在这种情况下测试误差为0，而AdaGrad等自适应方法会把所有的测试样例分为正类，泛化能力极差，也就是根本不能工作。</p>
<p>随后作者又用VGG+BN+Dropout的网络结构在CIFAR-10数据集上进行了实验： <img src="https://img-blog.csdnimg.cn/20210710193712619.png" alt="在这里插入图片描述" /> 可以看到：前期训练中Adam有优势，但SGD的泛化能力确实比Adam要好。</p>
<p>最后，为了彻底黑化Adam，文章又用了文本数据集和一些NLP模型做了实验： <img src="https://img-blog.csdnimg.cn/20210710193823396.png" alt="在这里插入图片描述" /> 即便有时候自适应方法的训练loss会更低，但SGD的泛化能力都无一例外地胜过了自适应的方法。自适应方法在训练初期速度很快，但是后期表现平平。</p>
<p>泛化能力差的原因在于：自适应方法倾向于关注稀疏的特征，因为这些特征对于训练样例的鉴别是很有效的，尤其在训练样例数少而特征较多的数据集中，但是这些特征其实并非关键特征，这样自适应学习率算法出现过拟合的风险就会增大，导致泛化能力不佳，最终的收敛效果不如传统的SGD。 ### 2 二阶动量波动 Google的一篇文章从数学上证明了在某些特定情况下Adam可能不收敛，因为二阶动量取的是某个时间窗口的变化，所以<span class="math inline">\(V_t\)</span>的变化可能会剧烈震荡，尤其在高维情况下，梯度的方差可能随时间波动很大，导致学习率震荡，模型无法收敛。这也是为什么一般<span class="math inline">\(\beta_2\)</span>要取0.999这么大的值，避免二阶动量有太大波动。</p>
<p>一般认为Adam默认的<span class="math inline">\(\beta_1\)</span>和<span class="math inline">\(\beta_2\)</span>不需要调整，采用默认的0.9和0.999即可。但是这两个超参如果不按这样设置，Adam可能永远不会收敛到最优值。文章从数学上证明了对任意的<span class="math inline">\(\beta_1,\beta_2\in[0,1),\beta_1&lt;\sqrt{\beta_2}\)</span>，都存在一个随机的凸优化问题使得Adam不能收敛到最优解。</p>
<p>为了避免二阶动量的剧烈震荡，文章对其进行了控制，提出了一个新算法AMSGrad确保模型收敛，<span class="math inline">\(V_t=max(V_{t-1},\beta_2V_{t-1}+(1-\beta_2)\nabla^2 f(x_t))\)</span>。</p>
<p>作者随后通过人造数据和真实数据进行了实验：</p>
<p>人造数据上的结果： <img src="https://img-blog.csdnimg.cn/20210710193934596.png" alt="在这里插入图片描述" /> 很显然在Adam没有找到最优解的这些数据上，改进后的算法都表现良好。</p>
<p>在MNIST上的效果： <img src="https://img-blog.csdnimg.cn/20210710193956571.png" alt="在这里插入图片描述" /> 这篇文章最终获得了2018年ICLR最佳论文，但是引起了很大争议。主要原因在于其构造的令Adam失效的数据在实际情况中出现的概率极低，即使出现也会在数据预处理时被筛掉，因此并没有特别广泛的实际用处。另外，文章过于强调训练集上的损失函数值，甚至有人通过复现表明文章提出的AMSGrad算法在测试数据上表现很差，与原文中的某些结论相互矛盾。 ### 3 学习率下降 arXiv上的一篇文章通过在CIFAR-10上的实验证明Adam在一些情况下虽然速度快，但收敛效果没有SGD好： <img src="https://img-blog.csdnimg.cn/20210710194231292.png" alt="在这里插入图片描述" /> 文章通过实验发现主要原因在于后期Adam的学习率过低，影响了最终效果。文章尝试通过控制学习率下界，提高了最终收敛效果。</p>
<p>既然Adam后期有问题，那么一个自然的改进就是前期训练使用Adam，用来快速减小loss；后期训练转换为SGD，用稍慢的速度寻找更佳的解甚至是最优解。但是这样也会引入新的问题：在什么时刻切换？切换为SGD后的学习率又该如何设置？</p>
<p>文章提出了SWATS(Switches from Adam to SGD)策略来解决上面2个问题，在CIFAR-10和CIFAR-100数据集上实验效果看着还不错： <img src="https://img-blog.csdnimg.cn/20210710194309531.png" alt="在这里插入图片描述" /> <img src="https://img-blog.csdnimg.cn/20210710194329636.png" alt="在这里插入图片描述" />这些文章都采用了一些比较极端的数据去探索Adam的不适情况，然而实际中遇到这些极端情况的概率并不大，因此Adam并不失为首选尝试。通过上面的讨论可以看到：SGD和Adam各有优劣，精调后的SGD一般最终会收敛到更好的效果；Adam在训练前期收敛速度快，在稀疏数据上表现更好，对超参不敏感，不需要十分精细的调参。</p>
<p>如果对优化算法不熟悉，可以先尝试SGD+Nesterov Momentum或者Adam；如果对某个优化算法很精通，那么调参就会相对容易些。如果资源足够，也可以尝试L-BFGS等二阶优化方法。另外，选择之前要充分了解数据的性质，对于比较稀疏的数据可以优先尝试学习率自适应调整的算法。 ## Reference [1] CS231n: Convolutional Neural Networks for Visual Recognition. lecture 8, Stanford University. [2] The Marginal Value of Adaptive Gradient Methods in Machine Learning. NIPS'17 [3] On the Convergence of Adam and Beyond. ICLR'18 [4] Improving Generalization Performance by Switching from Adam to SGD. arXiv [5] Optimization methods for large-scale machine learning. SIAM Review, 2018. [6] Optimization for deep learning: theory and algorithms. arXiv, 2019. [7] Understanding Black-box Predictions via Influence Functions. ICML'17. [8] Understanding Deep Learning Requires Rethinking Generalization. ICLR'17.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://eimadrigal.github.io/2021/06/17/Gradient%20Boosting/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/favicon.png">
      <meta itemprop="name" content="EIMadrigal">
      <meta itemprop="description" content="Hello World">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EI Madrigal's Space">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/06/17/Gradient%20Boosting/" class="post-title-link" itemprop="url">Gradient Boosting</a>
        </h2>

        <div class="post-meta">

		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-06-17 08:51:00" itemprop="dateCreated datePublished" datetime="2021-06-17T08:51:00+08:00">2021-06-17</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="gradient-boosting-regression">Gradient Boosting Regression</h2>
<h2 id="gradient-boosting-classification">Gradient Boosting Classification</h2>
<h2 id="xgboost">XGBoost</h2>
<h2 id="决策树集成">决策树集成</h2>
<p>集成学习可以组合多个基学习器，产生更加优异的性能。将决策树（如CART）作为基学习器，结合每个基学习器的预测结果作为最终输出，就像<a target="_blank" rel="noopener" href="https://xgboost.readthedocs.io/en/latest/tutorials/model.html">下图</a>这样： <img src="https://img-blog.csdnimg.cn/2021061621501811.png" alt="在这里插入图片描述" /> 正式一些的表示： <span class="math display">\[\hat{y}_i = \sum_{k=1}^K f_k(x_i), f_k \in \mathcal{F}\]</span> 其中，<span class="math inline">\(K\)</span>是决策树个数，<span class="math inline">\(f_k(x_i)\)</span>表示第<span class="math inline">\(k\)</span>个决策树的预测值。</p>
<p>为了定量描述模型参数与训练数据的匹配程度，我们还要定义待优化的目标函数： <span class="math display">\[\text{obj}(\theta) = \sum_i^n l(y_i, \hat{y}_i) + \sum_{k=1}^K \Omega(f_k)\]</span> ## Boosting Decision Tree 集成多棵树的方式可以是Bagging，也可以是Boosting。Boosting的motivation是用一棵新树不断拟合当前的集成模型与真实值的残差，拟合后将该树也加入模型中，即所谓的Additive Training： <span class="math display">\[\hat{y}_i^{(0)} = 0\\
\hat{y}_i^{(1)} = f_1(x_i) = \hat{y}_i^{(0)} + f_1(x_i)\\
\hat{y}_i^{(2)} = f_1(x_i) + f_2(x_i)= \hat{y}_i^{(1)} + f_2(x_i)\\
\dots\\
\hat{y}_i^{(t)} = \sum_{k=1}^t f_k(x_i)= \hat{y}_i^{(t-1)} + f_t(x_i)\]</span> 好了，接下来的问题就是每次迭代时的那棵新树<span class="math inline">\(f_t\)</span>要怎么训练呢？一个直观的想法就是选择那棵令目标函数最小的树： <span class="math display">\[\text{obj}^{(t)} = \sum_{i=1}^n l(y_i, \hat{y}_i^{(t)}) + \sum_{i=1}^t\Omega(f_i) \\
          = \sum_{i=1}^n l(y_i, \hat{y}_i^{(t-1)} + f_t(x_i)) + \Omega(f_t) + \mathrm{constant}\]</span> 我们先选择MSE作为损失函数，看看会发生什么： <span class="math display">\[{obj}^{(t)} = \sum_{i=1}^n (y_i - (\hat{y}_i^{(t-1)} + f_t(x_i)))^2 + \sum_{i=1}^t\Omega(f_i) \\
          = \sum_{i=1}^n [2(\hat{y}_i^{(t-1)} - y_i)f_t(x_i) + f_t(x_i)^2] + \Omega(f_t) + \mathrm{constant}\]</span> 虽然MSE的形式比较友好，但是如果选择其它损失函数就很难有上式那般人性了，吃得太饱的同学可以试试logistic loss： <span class="math display">\[L(\theta) = \sum_i[ y_i\ln (1+e^{-\hat{y}_i}) + (1-y_i)\ln (1+e^{\hat{y}_i})]\]</span> 为了增强可扩展性、便于计算，一般采用损失函数的二阶泰勒展开去做一个近似： <span class="math display">\[\text{obj}^{(t)} = \sum_{i=1}^n [l(y_i, \hat{y}_i^{(t-1)}) + g_i f_t(x_i) + \frac{1}{2} h_i f_t^2(x_i)] + \Omega(f_t) + \mathrm{constant}\]</span> 其中，<span class="math inline">\(g_i = \partial_{\hat{y}_i^{(t-1)}} l(y_i, \hat{y}_i^{(t-1)}),h_i = \partial_{\hat{y}_i^{(t-1)}}^2 l(y_i, \hat{y}_i^{(t-1)})\)</span>。 扔掉所有常数项，就得到了第<span class="math inline">\(t\)</span>步的目标函数： <span class="math display">\[\sum_{i=1}^n [g_i f_t(x_i) + \frac{1}{2} h_i f_t^2(x_i)] + \Omega(f_t)\]</span></p>
<p>弄完了training loss，接着还得研究下正则项<span class="math inline">\(\Omega(f_t)\)</span>，首先得给<span class="math inline">\(f(x)\)</span>来一个正式点的定义： <span class="math display">\[f_t(x) = w_{q(x)}, w \in R^T, q:R^d\rightarrow \{1,2,\cdots,T\} .\]</span> 其中，<span class="math inline">\(w\)</span>是叶子结点的得分向量，<span class="math inline">\(q\)</span>是将样本点映射到对应叶子的函数，<span class="math inline">\(T\)</span>是叶子数目。 如果有点抽象，就看看上图中的左子图吧：<span class="math inline">\(w=[2,-1],f(男孩)=w_{q(男孩)}=w_0=2\)</span>。</p>
<p>模型复杂度的具体定义随你了，XGBoost是这么定义的： <span class="math display">\[\Omega(f) = \gamma T + \frac{1}{2}\lambda \sum_{j=1}^T w_j^2\]</span> 就用上式重新写出我们第<span class="math inline">\(t\)</span>步的目标函数： <span class="math display">\[\text{obj}^{(t)} \approx \sum_{i=1}^n [g_i w_{q(x_i)} + \frac{1}{2} h_i w_{q(x_i)}^2] + \gamma T + \frac{1}{2}\lambda \sum_{j=1}^T w_j^2\\
= \sum^T_{j=1} [(\sum_{i\in I_j} g_i) w_j + \frac{1}{2} (\sum_{i\in I_j} h_i + \lambda) w_j^2 ] + \gamma T\]</span> 其中，<span class="math inline">\(I_j = \{i|q(x_i)=j\}\)</span>表示第<span class="math inline">\(j\)</span>个叶子中样本点的索引集合，由于任意一个叶子中样本点得分相同，因此上式写成了对<span class="math inline">\(T\)</span>个叶子的求和。</p>
<p>令<span class="math inline">\(G_j = \sum_{i\in I_j} g_i\)</span>及<span class="math inline">\(H_j = \sum_{i\in I_j} h_i\)</span>，就有了一个相对简洁的表示： <span class="math display">\[\text{obj}^{(t)} = \sum^T_{j=1} [G_jw_j + \frac{1}{2} (H_j+\lambda) w_j^2] +\gamma T\]</span> 因为叶子之间相互独立，所以令目标函数最优的得分向量<span class="math inline">\(w\)</span>为： <span class="math display">\[w_j^\ast = -\frac{G_j}{H_j+\lambda}\\
\text{obj}^\ast = -\frac{1}{2} \sum_{j=1}^T \frac{G_j^2}{H_j+\lambda} + \gamma T\]</span> 目标函数<span class="math inline">\(obj^*\)</span>的值衡量着本次迭代树结构<span class="math inline">\(q(x)\)</span>对训练数据的拟合程度。</p>
<p>云里雾里一大堆，我都烦了，来看个例子： <img src="https://img-blog.csdnimg.cn/2021061716123725.png" alt="在这里插入图片描述" /> 假设在第<span class="math inline">\(t\)</span>次迭代选了这么一棵树，按照if-then规则将训练样本分到相应的叶子，将梯度信息相加得到每个叶子对应的<span class="math inline">\(G,H\)</span>，接着用<span class="math inline">\(obj^*\)</span>计算这棵树最小的损失，不行就换一种树结构，以求减小<span class="math inline">\(obj^*\)</span>。</p>
<p>忙活了大半天，终于知道了怎么度量一棵树的好坏。那么只要枚举所有可能的树结构，选那个令<span class="math inline">\(obj^*\)</span>最小的就好了。傻子都知道这是不行滴，所以只能贪心地一层一层地剥开你的心...哦不对，一层一层地优化：将结点分类为左孩子和右孩子的得分增益为： <span class="math display">\[Gain = \frac{1}{2} \left[\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}-\frac{(G_L+G_R)^2}{H_L+H_R+\lambda}\right] - \gamma\]</span> 其中，第一/二项分别表示左/右孩子的分数，第三项表示原始节点的分数，最后一项表示增加叶子的惩罚。可以看到：如果分裂后的得分增益小于<span class="math inline">\(\gamma\)</span>，就不要继续分了，凑合过吧...</p>
<p>为了在每层获取到最佳的分裂点，通常先将训练数据排个序： <img src="https://img-blog.csdnimg.cn/20210617164041605.png" alt="在这里插入图片描述" /> 暴力枚举一遍分裂点找最优就可以啦！</p>
<ul>
<li><span class="math inline">\(f_0(x)=0\)</span></li>
<li>对于第m棵树的训练：
<ul>
<li>首先计算每条训练数据的残差：<span class="math inline">\(r_{mi}=y_i-f_{m-1}(x_i),i=1,2...,N\)</span></li>
<li>接着通过拟合上面得到的残差数据，训练出回归树<span class="math inline">\(T_m(x)\)</span></li>
<li>此时第m棵树的输出即为<span class="math inline">\(f_m(x)=f_{m-1}(x)+T_m(x)\)</span></li>
</ul></li>
<li>进行M次训练后得到最终的模型</li>
</ul>
<p>可以看到：Boosting Decision Tree每次迭代都将上一轮预测结果的残差作为当前的训练集，对于平方损失容易求得损失函数最小值的点，但是对于稍复杂的损失函数，残差的获得就只能通过负梯度<span class="math inline">\(\frac{\partial L(y_i,f(x_i))}{f(x_i)}\)</span>去逼近，这就是GBDT的核心思想。 GBDT的训练与Boosting Decision Tree很相似：</p>
<ul>
<li>初始化弱学习器<span class="math inline">\(f_0(x)=\underset{c}{arg\ min}\sum_{i=1}^{N}L(y_i,c)\)</span>，如果损失函数是MSE，那么<span class="math inline">\(f_0(x)=\frac{1}{N}\sum_{i=1}^{N}y_i\)</span></li>
<li>对于第m棵树的训练：
<ul>
<li>计算负梯度：<span class="math inline">\(r_{mi}=-\frac{\partial L(y_i,f(x_i))}{\partial f(x_i)},f(x)=f_{m-1}(x)\)</span></li>
<li>得到新的训练集<span class="math inline">\((x_i,r_{mi})\)</span>，训练产生一棵新的回归树，对应的叶子结点域为<span class="math inline">\(R_{mj},j=1,...,J\)</span>，<span class="math inline">\(J\)</span>为叶子结点个数</li>
<li>对第j个叶子结点，计算最佳拟合值：<span class="math inline">\(c_{mj}=\underset{c}{arg\ min}\sum_{x_i\in R_{mj}}L(y_i,f_{m-1}(x_i)+c)\)</span></li>
<li>更新强学习器：<span class="math inline">\(f_m(x)=f_{m-1}(x)+\sum_{i=1}^{J}c_{mj}I(x\in R_{mj})\)</span></li>
</ul></li>
<li>最终的学习器为：<span class="math inline">\(\hat f(x)=f_M(x)=f_0(x)+\sum_{m=1}^{M}\sum_{j=1}^{J}c_{mj}I(x\in R_{mj})\)</span></li>
</ul>
<h2 id="implementation">Implementation</h2>
<h2 id="properties">Properties</h2>
<ol type="1">
<li>extrapolate问题 众所周知随机森林回归是不具备推理能力的，那么XGBoost可以吗？ 答案是可以，因为梯度提升模型并不直接根据训练集的结果做预测，而是通过一系列树的加和得到，加和结果取决于每棵树的权重，权重则是由损失函数的一二阶梯度优化得来，并不依赖于训练集的上下限。</li>
<li>缺失值问题 GBDT/GBRT自身不支持缺失值的自动填充，例如使用sklearn中的GradientBoostingRegressor在训练数据包含缺失值时将无法训练，人工填充可能会引入偏差，但是XGBoost却可以自动地处理缺失值（但并不是填充）。 根据陈天奇大佬的说法： &gt; Internally, XGBoost will automatically learn what is the best direction to go when a value is missing. Equivalently, this can be viewed as automatically "learn" what is the best imputation value for missing values based on reduction on training loss.</li>
</ol>
<p>那么究竟是如何自动学习最佳的分裂方向呢？ 假设在结点A有50条训练样本，并且该结点只有一个可能的分割点：比如只有一个二元特征x，那么分割点就只有该特征取值为0或1，这样训练数据可以被分为3组： 1. x取值为B的20条样例 2. x取值为C的20条样例 3. x缺失的10条样例，叫做M组</p>
<p>那么M组的样例会被分别赋到B和C，接着计算<span class="math inline">\(\{(B,M),C\}\)</span>和<span class="math inline">\(\{B,(C,M)\}\)</span>的得分及损失函数衰减，两者中选择损失函数衰减大的。 如果使用MSE作为损失函数，并且B的标签均值为5，C的标签均值为10，M的标签均值为0。 如果使用<span class="math inline">\(\{(B,M),C\}\)</span>：<span class="math inline">\(\frac{|M|}{|B| + |M|}\text{mean}(M) + \frac{|B|}{|M|+|B|}\text{mean}(B) = \frac{10}{30}0 + \frac{20}{30}5 = 3.\overline{3}\)</span> 如果使用<span class="math inline">\(\{B,(C,M)\}\)</span>：<span class="math inline">\(\frac{|M|}{|C| + |M|}\text{mean}(M) + \frac{|C|}{|M|+|C|}\text{mean}(C) = \frac{10}{30}0 + \frac{20}{30}10 = 6.\overline{3}\)</span> 最后计算两者的MSE与划分前MSE的差，选择使得MSE下降更快的作为分裂方向（也就是得分gain更大的方向）。</p>
<p>在寻找最优特征分裂点（如年龄＜20还是年龄＜30）时，只访问该特征不含缺失值的训练样例，即如果年龄缺失，就不参与20和30的决策，这样计算复杂度也就降低了，尤其是对于稀疏数据。</p>
<p>预测时的缺失值有２种情况： 1.　训练阶段已经见识过该缺失值了：按照训练时选定的方向往下走就行 2.　训练阶段该特征没有缺失：默认走向右子树。</p>
<p>Ref里还有一个更加全面的例子，训练集有6个小孩，只有一个特征年龄（其中有2个样例年龄缺失），标签是身高，初始预测值为0.5，接下来每棵树都要拟合残差。 | Age | Height | Res | | ------------ | ------------ | ------------ | | 7 | 130 | －129.5 | | 9 | 148 | －147.5 | | 6 | 115 | －114.5 | | 15 | 164 | －163.5 | | ？ | 125 | －124.5 | | ？ | 140 | －139.5 |</p>
<p>接着要根据年龄特征寻找最优的分裂点，将年龄排序并选择中点（<strong>注意：这里就不考虑缺失值样例了</strong>），因此候选分裂点有6.5，8，12，对于每个候选点，分别计算将缺失样例划到左子树和右子树的Quality/Similarity Score：</p>
<p><span class="math display">\[
Quality\ Score=\frac{(\sum residuals)^2}{\#residuals + \lambda}
\]</span></p>
<p>比如，对于分裂点6.5： 如果划到左子树：<span class="math inline">\(Gain＝划分后的Quality\ Score－划分前的Quality\ Score＝\frac{(-114.5-124.5-139.5)^2}{3} + \frac{(-129.5-147.5-163.5)^2}{3} - \frac{(-129.5-147.5-114.5-163.5-124.5-139.5)^2}{6}=640.7\)</span> 如果划到右子树：<span class="math inline">\(Gain＝划分后的Quality\ Score－划分前的Quality\ Score＝580.8\)</span></p>
<p>接着对于8：1083；630.8 对于12：874.8；216 从中选择gain最大的（也就是使得损失函数最小的），分裂点选8，缺失值划到左子树。 ## Bug <a target="_blank" rel="noopener" href="https://www.lycecho.com/archives/2364">PYTHON XGBOOST 报错 KEYERROR: ‘BASE_SCORE’</a></p>
<h2 id="references">References</h2>
<p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=3CC4N4z3GJc"><strong>Gradient Boost</strong></a> <a target="_blank" rel="noopener" href="https://xgboost.readthedocs.io/en/latest/tutorials/model.html">Introduction to Boosted Trees</a> <a target="_blank" rel="noopener" href="https://datascience.stackexchange.com/questions/15305/how-does-xgboost-learn-what-are-the-inputs-for-missing-values">Missing values in XGBoost</a> <a target="_blank" rel="noopener" href="https://stats.stackexchange.com/questions/304962/is-is-possible-for-a-gradient-boosting-regression-to-predict-values-outside-of-t">Is is possible for a gradient boosting regression to predict values outside of the range seen in its training data?</a> <a target="_blank" rel="noopener" href="https://datascience.stackexchange.com/questions/77234/can-boosted-trees-predict-below-the-minimum-value-of-the-training-label">Can Boosted Trees predict below the minimum value of the training label?</a> <a target="_blank" rel="noopener" href="https://github.com/dmlc/xgboost/issues/1581#issuecomment-249853718">Why does XGBoost regression predict completely unseen values?</a> <a target="_blank" rel="noopener" href="https://medium.com/hypatai/how-xgboost-handles-sparsities-arising-from-of-missing-data-with-an-example-90ce8e4ba9ca">How XGBoost Handles Sparsities Arising From of Missing Data? (With an Example)</a> <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=OtD8wVaFm6E"><strong>XGBoost Regression</strong></a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://eimadrigal.github.io/2021/06/03/Learn%20to%20Learn/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/favicon.png">
      <meta itemprop="name" content="EIMadrigal">
      <meta itemprop="description" content="Hello World">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EI Madrigal's Space">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/06/03/Learn%20to%20Learn/" class="post-title-link" itemprop="url">Learn to Learn</a>
        </h2>

        <div class="post-meta">

		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-06-03 18:20:32" itemprop="dateCreated datePublished" datetime="2021-06-03T18:20:32+08:00">2021-06-03</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Hills/" itemprop="url" rel="index"><span itemprop="name">Hills</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>学会学习</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/06/03/Learn%20to%20Learn/">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://eimadrigal.github.io/2021/05/23/%E4%BA%BA%E7%94%9F%E6%B0%B8%E8%BF%9C%E8%89%B0%E9%9A%BE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/favicon.png">
      <meta itemprop="name" content="EIMadrigal">
      <meta itemprop="description" content="Hello World">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EI Madrigal's Space">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/05/23/%E4%BA%BA%E7%94%9F%E6%B0%B8%E8%BF%9C%E8%89%B0%E9%9A%BE/" class="post-title-link" itemprop="url">人生永远艰难</a>
        </h2>

        <div class="post-meta">

		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-05-23 16:16:00" itemprop="dateCreated datePublished" datetime="2021-05-23T16:16:00+08:00">2021-05-23</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Life/" itemprop="url" rel="index"><span itemprop="name">Life</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>碎碎念</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/05/23/%E4%BA%BA%E7%94%9F%E6%B0%B8%E8%BF%9C%E8%89%B0%E9%9A%BE/">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://eimadrigal.github.io/2021/05/07/Dealing%20with%20Imbalanced%20Datasets/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/favicon.png">
      <meta itemprop="name" content="EIMadrigal">
      <meta itemprop="description" content="Hello World">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EI Madrigal's Space">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/05/07/Dealing%20with%20Imbalanced%20Datasets/" class="post-title-link" itemprop="url">Dealing with Imbalanced Datasets</a>
        </h2>

        <div class="post-meta">

		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-05-07 05:54:00" itemprop="dateCreated datePublished" datetime="2021-05-07T05:54:00+08:00">2021-05-07</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="motivation">Motivation</h2>
<p>The Imbalanced Datasets are very common in our life such as illegal users or illness check. The machine learning model always performs bad on these datasets if there are no specific dealings, especially the prediction accuracy of minority class. For example, if the data is highly imbalanced such as 9995(negative):5(positive), then if your model just let every instance to be negative and you can get an acc of 99.95% but the result is meaningless. Another example is that misclassifying the minority is very severe. Assume that you misclassify the patient as normal. Oh my god!</p>
<p>So researchers proposed two kinds of methods for this problem:</p>
<ul>
<li>Cost Sensitive Learning When <strong>training</strong> your model, it will give different classes different weights in the <strong>loss function</strong> thus let the model focus more on the minority class. In sklearn, there are <code>class_weight</code> and <code>sample_weight</code> for you. For <code>class_weight</code>, you can specify the weights for different classes such as <code>&#123;0:0.1,1:0.9&#125;</code> or you can set it to <code>balanced</code> then weights will be computed by <span class="math inline">\(\frac{\#samples}{\#classes\ *\ np.bincount(y)}\)</span>. For <code>fit(sample_weight=)</code>, you give <strong>every instance</strong> different weights. When computing the loss for the instance, it will be <code>class_weight</code> * <code>sample_weight</code> * <code>loss</code>.</li>
<li>Sampling Sampling means that we will change the original dataset rather than giving them different weights.</li>
</ul>
<h2 id="sampling-methods">Sampling Methods</h2>
<p>Over-sampling means to increment the minority class.</p>
<ul>
<li>Random Over Sampling To sample from minority class with replacement to let the number of each class is 1:1. Overfitting on minority class.</li>
<li>Synthetic Minority Oversampling Technique (SMOTE) <span class="math display">\[
x_{new}=x_i+\lambda(x_{zi}-x_i)
\]</span> First you find the <code>k_neighbors</code> of <span class="math inline">\(x_i\)</span> in the minority class, then just select one <span class="math inline">\(x_{zi}\)</span> randomly and produce the new one. There are some variants such as borderline SMOTE, SVM SMOTE and KMeans SMOTE.</li>
<li>Adaptive Synthetic (ADASYN) The difference between SMOTE and ADASYN is that SMOTE will generate new samples for random minority data until 1:1. But ADASYN will automatically decide the number of new points generated for each <span class="math inline">\(x_i\)</span>. There will be more points generated if there are more majority data around <span class="math inline">\(x_i\)</span>.</li>
</ul>
<p>Under-sampling means to decrease the majority class.</p>
<ul>
<li>RUS Data waste.</li>
<li></li>
</ul>
<h2 id="example">Example</h2>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://eimadrigal.github.io/2021/05/02/%E5%BD%BB%E5%BA%95%E4%BD%9C%E5%88%AB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/favicon.png">
      <meta itemprop="name" content="EIMadrigal">
      <meta itemprop="description" content="Hello World">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EI Madrigal's Space">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/05/02/%E5%BD%BB%E5%BA%95%E4%BD%9C%E5%88%AB/" class="post-title-link" itemprop="url">彻底作别</a>
        </h2>

        <div class="post-meta">

		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-05-02 15:29:00" itemprop="dateCreated datePublished" datetime="2021-05-02T15:29:00+08:00">2021-05-02</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Life/" itemprop="url" rel="index"><span itemprop="name">Life</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Say Goodbye~</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/05/02/%E5%BD%BB%E5%BA%95%E4%BD%9C%E5%88%AB/">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://eimadrigal.github.io/2021/04/22/Kick%20Start%202013/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/favicon.png">
      <meta itemprop="name" content="EIMadrigal">
      <meta itemprop="description" content="Hello World">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EI Madrigal's Space">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/04/22/Kick%20Start%202013/" class="post-title-link" itemprop="url">Kick Start 2013</a>
        </h2>

        <div class="post-meta">

		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-04-22 01:33:00" itemprop="dateCreated datePublished" datetime="2021-04-22T01:33:00+08:00">2021-04-22</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="practice-round">Practice Round</h2>
<ol type="1">
<li>题目大意：给定一堆人名，从上向下扫描，一旦当前值比前一个的字典序小，就将当前值移动到正确的位置，不论移动多远，代价都是1，求代价总和。</li>
</ol>
<p>和插入排序类似，如果当前值<span class="math inline">\(j\)</span>比<span class="math inline">\(j-1\)</span>小，将<span class="math inline">\(j\)</span>移到前面合适的位置，此时前<span class="math inline">\(j\)</span>个数是局部有序的。这道题只要求出代价和即可，不需要输出排序后的结果，不需要真正去移动，只要记录前<span class="math inline">\(j\)</span>个的最大值，如果<span class="math inline">\(j+1\)</span>比最大值小，那么必然触发一次移动。举例： 2 1 5 3 0 j=1, max=2, cost++ 1 2 5 3 0 j=3, max=5, cost++ 1 2 3 5 0 j=0, max=5, cost++ 0 1 2 3 5 有2个地方要注意：<code>cin</code>读入<code>string</code>时，会把空格/回车作为分隔符，遇到即停止，所以要用<code>getline()</code>，默认以回车结束；<code>cin</code>读完<code>int</code>后，换行符<code>\n</code>仍然在输入流里，所以下一次的<code>getline</code>会先读<code>\n</code>，故用<code>cin.get()</code>先取走<code>\n</code>。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>** argv)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> T, N;</span><br><span class="line">    cin &gt;&gt; T;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; T; ++i) &#123;</span><br><span class="line">        cin &gt;&gt; N;</span><br><span class="line">        cin.<span class="built_in">get</span>();</span><br><span class="line">        <span class="function">vector&lt;string&gt; <span class="title">names</span><span class="params">(N)</span></span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; N; ++j) &#123;</span><br><span class="line">            <span class="built_in">getline</span>(cin, names[j]);</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        string curMax;</span><br><span class="line">        <span class="keyword">int</span> money = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; N; ++j) &#123;</span><br><span class="line">            <span class="keyword">if</span> (j == <span class="number">0</span>) &#123;</span><br><span class="line">                curMax = names[<span class="number">0</span>];</span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (names[j].<span class="built_in">compare</span>(curMax) &lt; <span class="number">0</span>) &#123;</span><br><span class="line">                ++money;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span> &#123;</span><br><span class="line">                curMax = names[j];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        cout &lt;&lt; <span class="string">&quot;Case #&quot;</span> &lt;&lt; i + <span class="number">1</span> &lt;&lt; <span class="string">&quot;: &quot;</span> &lt;&lt; money &lt;&lt; endl;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ol start="2" type="1">
<li>题目大意：</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://eimadrigal.github.io/2021/04/13/Monte-Carlo%20Tree%20Search/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/favicon.png">
      <meta itemprop="name" content="EIMadrigal">
      <meta itemprop="description" content="Hello World">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EI Madrigal's Space">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/04/13/Monte-Carlo%20Tree%20Search/" class="post-title-link" itemprop="url">Monte-Carlo Tree Search</a>
        </h2>

        <div class="post-meta">

		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-04-13 11:53:00" itemprop="dateCreated datePublished" datetime="2021-04-13T11:53:00+08:00">2021-04-13</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="motivation">Motivation</h2>
<p>We all know that Monte Carlo Simulation is used to estimate some unknown variables through random simulation. It is because that the process is too complicated so we cannot know the true rule behind it. Only god knows. But thankful to the randomness we can do lots of experiments to approach the truth.</p>
<p>MCTS has the same idea but it is based on a tree. Every path from root to leaf forms a solution and the whole tree defines the search space. It is a heuristic search strategy based on some loss functions. But it will follow not only the loss but also try to explore the unvisited nodes. So it's also trying to make a balance between exploration &amp; exploitation.</p>
<p>One iteration has 4 processes: Selection, Expansion, Simulation and Backpropagation. Let's start to build the tree. Initially the tree only has a root node. Every node holds 3 info: action list for the next decision; visit times to measure the exploration; quality values to measure the exploitation.</p>
<ol type="1">
<li>Selection Using some criterion to select a child node which is eager to expand. There are 3 possibilities for the current state: If all the actions have been expanded thus the node has finished a complete search, then we will find a child with max UCB value and go down the tree recursively; Else if there are still some actions which have not been expanded (e.g. the node has 20 possible actions but there are 19 child node in the tree), then it will select one action randomly from the unexpanded actions and do Step 2 Expansion; Else game over and do Step 4 Backpropagation.</li>
<li>Expansion We have found the most eager node N to expand and the action A after Selection. So we need to add a new node S to the tree as N's child node by doing A.</li>
<li>Simulation/Playout Start from S to let the game run randomly until game over. Then we get a performance to be S's initial quality value.</li>
<li>Backpropagation The nodes along the path from root to N will update their quality values after S's simulation.</li>
</ol>
<p>After some fixed number of iterations or time limit, we will get a large tree and select the best leaf node as the result. Below is a figure: <img src="https://img-blog.csdnimg.cn/20210404201349819.png" alt="在这里插入图片描述" /> ## Upper Confidence Bound (UCB) When we need to select a child node to go down the tree, we usually use UCB criterion: <span class="math display">\[\underset{child}{\operatorname{arg\ max}}(\hat\mu_{child}+C\sqrt\frac{log\ n(s)}{n(child)})\]</span> <span class="math inline">\(\hat\mu_{child}\)</span> is the average reward gathered over all tree-walks with prefix child, <span class="math inline">\(n(s)\)</span> the number of the parent's visits and <span class="math inline">\(C\)</span> is constant controlling exploration &amp; exploitation. UCB tends to select a node with high quality value (for exploitation) and relatively low visit times (for exploration). ## An example 1. Initial tree Actually we only have root node <span class="math inline">\(S_0\)</span>. Assume there are only two actions <span class="math inline">\(A_1\)</span> and <span class="math inline">\(A_2\)</span>. <span class="math inline">\((Q,N)\)</span> means the quality value and #visits of this node. <img src="https://img-blog.csdnimg.cn/20210519201721793.png" alt="在这里插入图片描述" /> 2. First Iteration Since <span class="math inline">\(S_0\)</span> is a leaf node now, we should expand. Since the 2 actions are both unexpanded so we randomly select one (assume we select <span class="math inline">\(A_1\)</span>). Then we add <span class="math inline">\(S_1\)</span> to the tree and playout from <span class="math inline">\(S_1\)</span>. <img src="https://img-blog.csdnimg.cn/20210519202838706.png" alt="在这里插入图片描述" /> Assume we got a performance of 20. Next we need to backpropagate the value to <span class="math inline">\(S_1\)</span> and <span class="math inline">\(S_0\)</span> and update Q and #visits. <img src="https://img-blog.csdnimg.cn/20210519203559429.png" alt="在这里插入图片描述" /> We finished the first iteration. 3. Second iteration Start from <span class="math inline">\(S_0\)</span>, since <span class="math inline">\(A_2\)</span> has not been expanded so we have to choose it. Then add <span class="math inline">\(S_2\)</span> to the tree and playout from here. <img src="https://img-blog.csdnimg.cn/2021051921175522.png" alt="在这里插入图片描述" /> Then backpropagate to <span class="math inline">\(S_2\)</span> and <span class="math inline">\(S_0\)</span>: <img src="https://img-blog.csdnimg.cn/20210519212028325.png" alt="在这里插入图片描述" /> 4. Third iteration From <span class="math inline">\(S_0\)</span> there are no unexpanded actions so we need to select one child using UCB (assume C=2). <span class="math inline">\(UCB(S_1)=21.67,UCB(S_2)=11.67\)</span>. Thus we select the leaf node <span class="math inline">\(S_1\)</span>. Assume <span class="math inline">\(S_1\)</span> has 2 unexpanded actions. Choose one randomly (assume <span class="math inline">\(S_3\)</span>) and playout from here and backpropagate, assume we get performance of 0: <img src="https://img-blog.csdnimg.cn/20210519212745377.png" alt="在这里插入图片描述" /> 5. Fourth iteration From root we should decide which one to select. Again using UCB: <span class="math inline">\(UCB(S_1)=11.48,UCB(S_2)=12.10\)</span>. So we choose <span class="math inline">\(S_2\)</span>. Assume there are two unexpanded actions so we randomly choose <span class="math inline">\(S_5\)</span> and playout and get a performance of 14. After backpropagate: <img src="https://img-blog.csdnimg.cn/20210519213431456.png" alt="在这里插入图片描述" /></p>
<p>Assume the max iteration number is 4 so we get the final tree above. Finally we can select the best solution from <span class="math inline">\(S_0\)</span> to leaf node according to UCB value.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://eimadrigal.github.io/2021/04/06/Bayesian%20Optimization/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/favicon.png">
      <meta itemprop="name" content="EIMadrigal">
      <meta itemprop="description" content="Hello World">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EI Madrigal's Space">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/04/06/Bayesian%20Optimization/" class="post-title-link" itemprop="url">Bayesian Optimization</a>
        </h2>

        <div class="post-meta">

		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-04-06 14:18:00" itemprop="dateCreated datePublished" datetime="2021-04-06T14:18:00+08:00">2021-04-06</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="motivation">Motivation</h2>
<p>Hyper-parameters tuning has become an important work during training neural networks. As the number of Hyper-parameter is becoming larger, researchers proposed Grid Search &amp; Random Search to wish to get better combinations of Hyper-parameters. However, Grid Search has a high time cost. Although some experiments showed that Random Search got a better result than Grid Search but the result is still not fulfilling.</p>
<p>Besides, there are some gradient-based methods to solve the problem. But the objective function is usually not differentiable or even not continuous. Thus these methods have a very finite usage.</p>
<p>BO is a gradient-free optimization method to get global solutions of a black-box function. The function usually has a high cost to compute such as training a deep neural network after tuning the Hyper-parameters. For this reason, we usually find a <strong>surrogate</strong> function to approximate the original function <span class="math inline">\(f\)</span>. In the field of AutoML, we often use Gaussian Process, Random Forest or deep network as the surrogate model. The simplest form of BO is as follows: <img src="https://img-blog.csdnimg.cn/20210401193953429.png" alt="在这里插入图片描述" /> <span class="math inline">\(f\)</span> represents the black-box function that we want to optimize (black-box means that the function transforms a configuration <span class="math inline">\(x\)</span> to an output but we don't know the exact function relationship). <span class="math inline">\(\chi\)</span> represents the search space of the combination of hyper-parameters. <span class="math inline">\(S\)</span> represents <strong>Acquisition Function</strong> which is used to select the promising <span class="math inline">\(x\)</span>. <span class="math inline">\(M\)</span> represents the surrogate model which takes a configuration <span class="math inline">\(x\)</span> and outputs the performance (much like <span class="math inline">\(f\)</span> does).</p>
<p>First we need to get some samples from <span class="math inline">\((f,\chi)\)</span>, thus we get <span class="math inline">\(D=(x_i,f(x_i)), i=1...n\)</span>.</p>
<p>Next we iterate <span class="math inline">\(T\)</span> times (often fixed) to select configuration <span class="math inline">\(x\)</span>. Use the dataset <span class="math inline">\(D\)</span> to train the surrogate model <span class="math inline">\(M\)</span> (much easier than train <span class="math inline">\(f\)</span>). <span class="math inline">\(M\)</span> has several choices such as Random Forest, Tree Parzen Estimators. Here we use GP so we get the probabilistic model <span class="math inline">\(p(y|x,D)\)</span>.</p>
<p>Then we need to find the most promising configuration <span class="math inline">\(x\)</span>. The most important thing for Acquisition Function is to make a balance between <strong>exploration &amp; exploitation</strong>. It means that when selecting the next <span class="math inline">\(x\)</span> we not only want to select those untried points (exploration) but also want to select those tried points which has a great <span class="math inline">\(f(x)\)</span> (exploitation).</p>
<p>Finally use the promising <span class="math inline">\(x_i\)</span> to get corresponding performance <span class="math inline">\(y_i\)</span> and join the pair into <span class="math inline">\(D\)</span>. ## Gaussian Process If we assume <span class="math inline">\(x_i\)</span> is independent with each other, the Multivariant Gaussian Distribution's probability density is as follows: <span class="math display">\[
p(x_1,...,x_n)=\frac{1}{(2\pi)^{\frac{n}{2}}\sigma_1...\sigma_n}exp(-\frac{1}{2}[\frac{(x_1-\mu_1)^2}{\sigma_1^2}+...+\frac{(x_n-\mu_n)^2}{\sigma_n^2}])
\]</span> We can rewrite the formula to the vectorized version: <span class="math display">\[
p(x)=(2\pi)^{-\frac{n}{2}}|K|^{-\frac{1}{2}}exp[-\frac{1}{2}(x-\mu)^TK^{-1}(x-\mu)]
\]</span> in which <span class="math display">\[
K=\left[
\begin{matrix}
  \sigma_1^2     &amp; 0      &amp; \cdots &amp; 0      \\
 0      &amp; \sigma_2^2      &amp; \cdots &amp; 0      \\
 \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
 0     &amp; 0      &amp; \cdots &amp; \sigma_n^2     \\
\end{matrix}
\right], x-\mu=[x_1-\mu_1,...,x_n-\mu_n]^T
\]</span> Thus <span class="math inline">\(x\sim N(\mu,K)\)</span>, <span class="math inline">\(\mu\)</span> is the mean vector and <span class="math inline">\(K\)</span> is the covariance matrix (a diagonal matrix since the independence).</p>
<p>But what should we do when <span class="math inline">\(x\)</span> has infinite dimensions? Such as in a continuous temporal T or spatial S. Actually GP means Gaussian Distribution and Stochastic Process (about time T). GP is defined by an infinite number of Random Variables on a continuous domain. In other words, it is an infinite dimension Gaussian Distribution. Formally, let's sample n moments from T: <span class="math inline">\(t_1,...,t_n\in T\)</span>, thus we get a n-dimensional vector <span class="math inline">\((\xi_1,...,\xi_n)\)</span>, if this vector is a n-dimensional Gaussian Distribution then <span class="math inline">\({\xi_t}\)</span> is a GP.</p>
<p>Let's take an easy example to illustrate: suppose during peoples' life time, at every moment <span class="math inline">\(t\)</span> the energy of the population forms a Gaussian Distribution but different moments have different <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>: <img src="https://img-blog.csdnimg.cn/20210404141525979.png" alt="在这里插入图片描述" /> If we take 5 moments during a population's life time, then <span class="math inline">\(\xi_1-\xi_5\)</span> all forms Gaussian Distribution but they have different <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>. If we sample an arbitrary moment <span class="math inline">\(t\)</span> then <span class="math inline">\(\xi(t)\sim N(\mu_t,\sigma_t^2)\)</span>. If we sample at some points and connect them together we get two samples of the GP, as the figure shows.</p>
<p>Now that we know what happens at <span class="math inline">\(t\)</span>, let's consider the whole <span class="math inline">\(T\)</span>. We know that for a finite Gaussian Distribution, it can be determined by a n-dimensional vector <span class="math inline">\(\mu_n\)</span> (reflects every Random Variable's expectation) and a <span class="math inline">\(n\times n\)</span> matrix <span class="math inline">\(\Sigma\)</span> (reflects every RV's variances and covariance between different dimensions). It is almost the same for GP except that we cannot use a vector to describe every <span class="math inline">\(t\)</span>'s mean since it is infinite. So we need a function <span class="math inline">\(m(t)\)</span> to describe the continuous <span class="math inline">\(T\)</span>. For <span class="math inline">\(\Sigma\)</span> we should use a kernel function <span class="math inline">\(k(s,t)\)</span> to describe the covariance between time <span class="math inline">\(t\)</span> and <span class="math inline">\(s\)</span>. Once <span class="math inline">\(m(t)\)</span> and <span class="math inline">\(k(s,t)\)</span> is defined the GP is determined <span class="math inline">\(\xi_t\sim GP(m(t),k(s,t))\)</span>.</p>
<p>The most popular kernel function is RBF which is defined as follows: <span class="math display">\[k(s,t)=\sigma^2exp(-\frac{||s-t||^2}{2l^2})\]</span> <span class="math inline">\(\sigma\)</span> and <span class="math inline">\(l\)</span> are two hyper-parameters. If <span class="math inline">\(s\)</span> and <span class="math inline">\(t\)</span> are close in <span class="math inline">\(T\)</span> then the output covariance will be larger and it means the correlation between the two points is bigger.</p>
<p>Once we have some knowledge about GP we can start to know Gaussian Process Regression, which is a kind of Probabilistic Model. It means that we can use Prior and Observations to calculate Posterior. First we define a GP by <span class="math inline">\(m(t)\)</span> and <span class="math inline">\(k(s,t)\)</span>, which is a Prior. Then we observe some data to revise the GP's <span class="math inline">\(m(t)\)</span> and <span class="math inline">\(k(s,t)\)</span> to get Posterior. But how?</p>
<p>Here we need to use some Gaussian Distribution's nice properties: Once Gaussian always Gaussian. It means that marginal distribution, summation and conditional distribution of a GD are still GD. Assume a n-dimensional RV <span class="math inline">\(x\sim N(\mu,\Sigma)\)</span> and we divide it into two parts <span class="math inline">\(x_A\)</span> and <span class="math inline">\(x_B\)</span> then we get: <span class="math display">\[x=\begin{bmatrix} x_A\\ x_B \end{bmatrix},\mu=\begin{bmatrix} \mu_A\\ \mu_B \end{bmatrix},\Sigma=\begin{bmatrix} \Sigma_{AA}, \Sigma_{AB} \\ \Sigma_{BA}, \Sigma_{BB} \end{bmatrix}\]</span> Then we can get: <span class="math display">\[x_A|x_B\sim \mathcal{N}(\mu_A+\Sigma_{AB}\Sigma_{BB}^{-1}(x_B-\mu_B),\Sigma_{AA}-\Sigma_{AB}\Sigma_{BB}^{-1}\Sigma_{BA})\]</span> Thus we could update the GD's Posterior parameters. It is much the same in GP. If we get some samples <span class="math inline">\((X,Y)\)</span> then the rest is <span class="math inline">\((X^*,f(X^*))\)</span>. The joint distribution forms an infinite GD: <span class="math display">\[\begin{bmatrix} Y\\ f(X^*) \end{bmatrix}\sim N(\begin{bmatrix} \mu(X)\\ \mu(X^*) \end{bmatrix},\begin{bmatrix} k(X,X), k(X,X^*) \\ k(X^*,X), k(X^*,X^*) \end{bmatrix})\]</span> So we want to know the rest of the points based on the observed points: <span class="math inline">\(f(X^*)|Y\sim N(\mu^*,k^*)\)</span>. <span class="math display">\[\mu^*=\mu(X^*)+k(X^*,X)k(X,X)^{-1}(Y-\mu(X))\\
k^*=k(X^*,X^*)-k(X^*,X)k(X,X)^{-1}k(X,X^*)\]</span> Here is an example: <img src="https://img-blog.csdnimg.cn/20210404155244142.png" alt="在这里插入图片描述" /> Finally let's return back to our BO's <span class="math inline">\(M\)</span>. We first assume our prior: <span class="math inline">\(\mu(X)=0,k(X,X^*)=RBF\)</span>. Plus the observed and evaluated <span class="math inline">\(D=\{x_i,y_i\}\)</span> we can get <span class="math inline">\(\hat \mu\)</span> and <span class="math inline">\(\hat{\sigma}^{2}\)</span>, then the posterior prediction is <span class="math inline">\(p(y|x,D)\)</span>, which is still a Gaussian Distribution. The calculation process is as follows: <span class="math display">\[
y=(y_1,...,y_i)^T \\
\hat \mu=k^T(x)(k+\sigma_{n}^{2}I)^{-1}y \\
\hat{\sigma}^{2}=k(x^*x)-k(x)^T(k+\sigma_{n}^{2}I)^{-1}k(x)
\]</span> Once we get the posterior prediction <span class="math inline">\(p(y|x,D)\)</span>, we can feed them to the Acquisition Function to get next <span class="math inline">\(x_t\)</span>. ## Acquisition Function There are some popular Acquisition Functions:</p>
<ol type="1">
<li>Upper Confidence Bound (UCB) <span class="math inline">\(x_{t+1}=\underset{x\in X}{\operatorname{arg\ max}}[\mu_{t}(x)+\beta_t^{1/2}\sigma_t(x)]\)</span> The weighted sum of posterior mean and posterior standard deviation. The two items correspond exploitation and exploration, respectively.</li>
<li>Expected Improvement (EI) <span class="math inline">\(x_{t+1}=\underset{x\in X}{\operatorname{arg\ max}}\ E_{f(x)\sim N(\mu_{t}(x),\sigma_t^2(x))}[max(0,f(x)-f_t^+)]\)</span>, <span class="math inline">\(f_t^+\)</span> is the max observation value during the first <span class="math inline">\(t\)</span> iterations.</li>
</ol>
<p>Except the above functions, there are Probability of Improvement, Entropy Search and so on. ## Reference <a target="_blank" rel="noopener" href="https://jgoertler.com/visual-exploration-gaussian-processes/">A Visual Exploration of Gaussian Processes</a> <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/46631426">如何通俗易懂地介绍Gaussian Process</a> <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/76269142">贝叶斯优化/Bayesian Optimization</a> <a target="_blank" rel="noopener" href="https://github.com/fmfn/BayesianOptimization/blob/master/examples/exploitation_vs_exploration.ipynb">Exploitation vs Exploration</a> <a target="_blank" rel="noopener" href="https://github.com/fmfn/BayesianOptimization">BayesianOptimization</a> <a target="_blank" rel="noopener" href="https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote15.html">Lecture 15: Gaussian Processes</a> <a target="_blank" rel="noopener" href="https://distill.pub/2020/bayesian-optimization/">Exploring Bayesian Optimization</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://eimadrigal.github.io/2021/03/29/Inductive%20Representation%20Learning%20on%20Large%20Graphs/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/favicon.png">
      <meta itemprop="name" content="EIMadrigal">
      <meta itemprop="description" content="Hello World">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EI Madrigal's Space">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/03/29/Inductive%20Representation%20Learning%20on%20Large%20Graphs/" class="post-title-link" itemprop="url">Inductive Representation Learning on Large Graphs</a>
        </h2>

        <div class="post-meta">

		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-03-29 08:56:00" itemprop="dateCreated datePublished" datetime="2021-03-29T08:56:00+08:00">2021-03-29</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Stanford的帅哥Jure发在NIPS 2017的一篇文章。</p>
<p>GCN是Transductive Learning，训练时的图要包含所有结点，是固定的，不能快速泛化到未知结点（图），本文提出了一种Inductive Learning的GraphSAGE。</p>
<p>GCN学习的是每个单独节点的低维embedding，由于输入的图是固定的，所以可以很好捕获全局信息。但如果要获得新节点的embedding，加入图以后需要调整其它结点，至少也是局部重新训练，计算开销太大，应用受限。</p>
<p>GraphSAGE学习的不是每个结点的固定的表示，因为图结构不断变化，所以学习一种节点表示的函数，这样就可以快速得到未知结点的表示。</p>
<p>简单来说：学习每个结点的特征如何由邻居的特征聚合而来，学到聚合函数后，只要已知新节点的特征和邻边关系，就能得到表示，并且这个表示会由于图结构的变化而变化，是动态的。</p>
<p>前向传播是为了生成结点的向量表示， <img src="https://img-blog.csdnimg.cn/20210207163418476.png" alt="在这里插入图片描述" /> 如果聚合K次，就需要K个聚合函数，初始时每个结点的表示是原本的特征向量，对第k层，对结点v，先得到v的第k层结点的聚合表示，加上v在上一层的特征，最后得到v的最终表示。</p>
<p>以作者的图为例， <img src="https://img-blog.csdnimg.cn/20210220111424640.png" alt="在这里插入图片描述" /> 我觉得知乎上这张更清楚： <img src="https://img-blog.csdnimg.cn/20210220111932548.png" alt="在这里插入图片描述" /> 每一层的表示都是由上一层生成，与当前层其他节点无关。</p>
<p>由于需要学习参数，所以要设计损失函数。无监督学习的损失函数应该是让临近节点有相似的表示，有监督学习用cross-entropy即可。</p>
<p>聚合函数作者给了3种选择：</p>
<ol type="1">
<li>Mean 取邻居的平均值</li>
<li>LSTM</li>
<li>Pooling</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/2/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/17/">17</a><a class="extend next" rel="next" href="/page/4/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="EIMadrigal"
      src="/images/favicon.png">
  <p class="site-author-name" itemprop="name">EIMadrigal</p>
  <div class="site-description" itemprop="description">Hello World</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">169</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">19</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/EIMadrigal" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;EIMadrigal" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:andrew.renj@gmail.com" title="E-Mail → mailto:andrew.renj@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.cnblogs.com/EIMadrigal" title="cnblogs → https:&#x2F;&#x2F;www.cnblogs.com&#x2F;EIMadrigal" rel="noopener" target="_blank"><i class="fab fa-codiepie fa-fw"></i>cnblogs</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/EIMadrigal" title="Weibo → https:&#x2F;&#x2F;weibo.com&#x2F;EIMadrigal" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i>Weibo</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2018 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">EIMadrigal</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>


    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <span id="busuanzi_container_site_pv">Total views: <span id="busuanzi_value_site_pv"></span></span>
    <span class="post-meta-divider">|</span>
    <span id="busuanzi_container_site_uv">Total visitors: <span id="busuanzi_value_site_uv"></span></span>
    <span class="post-meta-divider">|</span>

<script>
$(document).ready(function() {

    var int = setInterval(fixCount, 50);
    var countOffset = 20000;

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset);
            clearInterval(int);
        }  
    }
       	
});
</script> 

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


</body>
</html>
