<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"eimadrigal.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Hello World">
<meta property="og:type" content="website">
<meta property="og:title" content="EI Madrigal&#39;s Space">
<meta property="og:url" content="https://eimadrigal.github.io/page/12/index.html">
<meta property="og:site_name" content="EI Madrigal&#39;s Space">
<meta property="og:description" content="Hello World">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="EIMadrigal">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://eimadrigal.github.io/page/12/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>EI Madrigal's Space</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">EI Madrigal's Space</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://eimadrigal.github.io/2019/06/30/Counting%20Sort%20and%20Radix%20Sort/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/favicon.png">
      <meta itemprop="name" content="EIMadrigal">
      <meta itemprop="description" content="Hello World">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EI Madrigal's Space">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/06/30/Counting%20Sort%20and%20Radix%20Sort/" class="post-title-link" itemprop="url">Counting Sort and Radix Sort</a>
        </h2>

        <div class="post-meta">

		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-06-30 02:18:00" itemprop="dateCreated datePublished" datetime="2019-06-30T02:18:00+08:00">2019-06-30</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="counting-sort">Counting Sort</h2>
<p>计数排序适用于数据量很大，但是数据类别很少的情况，可以做到线性时间。 举例来看：如果有100万个字符串，但只有cat, dog, person三种类型，采用基于比较的排序方式，可以做到<span class="math inline">\(NlogN\)</span>，计数排序采用了一种完全不同的思想：</p>
<ul>
<li>新建一个<code>counts[3]</code>，记录每种类型数据的出现次数；</li>
<li>遍历待排序数组，完成<code>count[]</code>的统计，并创建一个结果数组<code>sorted[]</code>： <img src="https://img-blog.csdnimg.cn/20200624082843898.png" alt="在这里插入图片描述" /></li>
<li>基于<code>count[]</code>，我们完全可以知道第一个cat应该放置在0，第一个dog应该放置在<code>count[0]=4</code>处，第一个person应该放置在<code>count[0]+count[1]=6</code>处，为了更加清晰，创建一个<code>starts[3]</code>表示每类数据中的第一个的起始位置： <img src="https://img-blog.csdnimg.cn/20200624083353219.png" alt="在这里插入图片描述" /></li>
<li>接着第二次遍历待排序数组，遇到第一个cat，我们知道它应该放在<code>sorted[starts[0]]</code>；第一个dog应该放在<code>sorted[starts[1]]</code>，第二个dog应该放在<code>sorted[starts[1]+1]</code>。或者可以这样做：每当放置完一个dog，就<code>++starts[1]</code>，这样下一次的dog还是会放在<code>sorted[starts[1]]</code>，最终结果： <img src="https://img-blog.csdnimg.cn/20200624084335135.png" alt="在这里插入图片描述" /></li>
</ul>
<p>对于字符串排序，我们需要规定<code>counts[]</code>中每个下标对应哪种类型。如果对于非负整数，我们可以用<code>counts[i]</code>表示i的出现次数，接着遍历<code>counts[]</code>，将整数i放置<code>counts[i]</code>次；如果有负数，可以找到最小值min和最大值max，平移到0~max-min即可。</p>
<h2 id="radix-sort">Radix Sort</h2>
<p>计数排序的前提就是需要知道待排序数组的内容/范围，那么如果范围很大，空间上是无法忍受的，由此来看更加general的基数排序：如果给定某种基（二进制2/十进制10/小写字母26）下的待排序数据，基数排序会逐位处理。基数排序有两种方式：</p>
<ol type="1">
<li>LSD(Least Significant Digit) 首先按照最右边一位排序，依次处理左边的每一位： 356, 112, 904, 294, 209, 820, 394, 810； 820, 810, 112, 904, 294, 394, 356, 209； 904, 209, 810, 112, 820, 356, 294, 394； 112, 209, 294, 356, 394, 810, 820, 904。 由于对第二位排序不能改变第一位排序的结果，所以要求按位排序算法必须是<strong>稳定</strong>的。</li>
<li>MSD(Most Significant Digit) 从左到右处理，MSD需要用到桶： [112], [294, 209], [356, 394], [820, 810], [904]； 对于每个桶采用类似的方法直到最后一位，以[294, 209]为例，接着处理第二位：[209], [294]。 最后收集每个桶中的元素即可。 ## Reference <a target="_blank" rel="noopener" href="https://github.com/EIMadrigal/CS61B/tree/master/lab13">具体实现</a>不知道LSD哪里实现的有问题，提交AutoGrader总是超时。 <a target="_blank" rel="noopener" href="https://sp18.datastructur.es/materials/lab/lab13/lab13">lab 13 Radix Sorts</a></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://eimadrigal.github.io/2019/06/08/Na%C3%AFve%20Bayes/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/favicon.png">
      <meta itemprop="name" content="EIMadrigal">
      <meta itemprop="description" content="Hello World">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EI Madrigal's Space">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/06/08/Na%C3%AFve%20Bayes/" class="post-title-link" itemprop="url">Naïve Bayes</a>
        </h2>

        <div class="post-meta">

		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-06-08 15:11:00" itemprop="dateCreated datePublished" datetime="2019-06-08T15:11:00+08:00">2019-06-08</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="motivation">Motivation</h2>
<h2 id="details">Details</h2>
<h2 id="example">Example</h2>
<h2 id="implementation">Implementation</h2>
<h2 id="properties">Properties</h2>
<p>NBC有坚实的数学基础，稳定的分类效率，超参数少（先验），对缺失数据不敏感，算法简单。 如果属性较多且相关性比较大，决策树优于NBC，否则NBC性能很好。</p>
<p>垃圾邮件分类器 事先做一个vocabulary存放常用的单词，如果邮件中包含vocabulary的第j个单词，特征向量中<span class="math inline">\(x_j=1\)</span> 朴素贝叶斯假设给定标签前提下各个属性是独立的（条件独立）： <span class="math inline">\(P(x_1,...,x_d|y)=P(x_1|y)P(x_2|y,x_1)...P(x_d|y,x_1,...,x_{d-1})=P(x_1|y)P(x_2|y)...P(x_d|y)\)</span> <span class="math inline">\(P(x_2|y,x_1)\)</span>表示在<span class="math inline">\((y,x_1)\)</span>的条件下<span class="math inline">\(x_2\)</span>发生的概率 <span class="math inline">\(P(junk|D)=\frac{P(junk)P(D|junk)}{P(D)},P(normal|D)=\frac{P(normal)P(D|normal)}{P(D)}\)</span> P(junk)/P(normal)根据邮件库的比例即可 P(D|junk)=P(word1,word2,...,wordn|junk)，联合概率分布的数据是稀疏的，在垃圾邮件集合中出现与当前邮件相同的邮件概率。 P(word1|junk)P(word2|word1,junk)P(word3|word2,word1,junk)... 如果条件独立假设成立， P(word1|junk)P(word2|junk)P(word3|junk)...只要统计垃圾邮件中每个单词的频率</p>
<p>拼写纠正 max P(猜测用户希望输入的单词|实际输入单词) <span class="math inline">\(P(h_1|D),P(h_2|D)\)</span> <span class="math inline">\(P(h|D)\propto P(h)P(D|h)\)</span> 对给定的观测数据，一个猜测的好坏正比于先验和这个猜测生成观测数据的可能性大小（似然）</p>
<p>假设实际输入D=thew,h1=the,h2=thaw <span class="math inline">\(P(h_1|D)=\)</span>the本身在词典中的出现概率及输入the前提下输thew的可能</p>
<p>ID3（Iterative Dichotomiser 3）迭代二叉树3代，启发式算法 以信息增益做属性选择，选择分裂后信息增益最大的属性进行分裂</p>
<ol type="1">
<li>top-down贪心遍历可能的决策树空间</li>
<li>核心问题在于如何选择划分属性</li>
<li>按照信息增益选择分类能力最好的属性</li>
<li>属性的每个值产生一个分支，将训练数据放在合适的分支，不回溯考虑之前的选择</li>
</ol>
<p>都知道熵用来衡量随机变量的不确定性，在这里就是刻画数据集的不纯度 条件熵是指在某个条件下，随机变量的不确定性 信息增益即熵-条件熵，即某条件下信息不确定性减少的程度 举例来看：明天下雨的熵为2，阴天条件下下雨的熵是0.01（即阴天下雨的可能性很大，所以不确定性很小），信息增益=2-0.01=1.99，获知阴天后，下雨的不确定性减少了很多，信息增益很大，所以阴天对下雨这一推断很重要，意味着这个特征很关键。</p>
<p>IG衡量给定属性区分训练样例的能力 <span class="math inline">\(Entropy(S)=\sum_{i=1}^{c}-p_ilog_2p_i\)</span>，c表示该属性有c个取值，Pi表示子集中样例占总数的比例。介于[0,1]之间，所有样例属于1类，最纯，熵为0；平分熵最大为1</p>
<p>一个属性的IG意味着用该属性分割样例导致的熵减的期望， <span class="math inline">\(IG(S,A)=E(S)-\sum_{i=1}^{c}\frac{|S_i|}{|S|}E(S_i)\)</span></p>
<p>C4.5用信息增益率作为属性选择的依据，构造过程中会进行剪枝（不考虑只有几个元素的结点，避免过拟合） 率即用相对性衡量（10经过10s到20，1经过1s到2，虽然前者的增益大，但如果用速度增加率即加速度衡量是一样的）</p>
<p>可以处理非离散数据和不完整数据</p>
<p>当前数据集S，当前属性A有c个取值，S需要被分割为c个子集 <span class="math inline">\(GainRatio(S,A)=\frac{IG(S,A)}{SplitInfo(S,A)}\)</span> <span class="math inline">\(SplitInfo(S,A)=-\sum_{i=1}^{c}\frac{|S_i|}{|S|}log_2\frac{|S_i|}{|S|}\)</span> 如果A能完全分割，那么splitinfo=0；如果对半分，那么=1，即splitinfo阻碍选择值均匀分布的属性 如果splitinfo=0，可以先计算每个属性的信息增益，选择增益&gt;平均值的属性再去应用增益比率，因为如果splitinfo=0，即A能完全分割，意味着信息增益=0，不可能&gt;平均值</p>
<p>实际中决策树的过拟合是比较严重的，C4.5克服了ID3用IG选择属性时倾向选择取值多的属性的不足。</p>
<p>马尔可夫随机过程： s1：名词 s2：动词 s3：形容词 转移矩阵A=0.3 0.5 0.2;0.5 0.3 0.2;0.4 0.2 0.4即A11表示s1后面跟着s1的概率</p>
<p>若某段话第一个词为名词s1，那么该句子是“名动形名”的概率是多少？ <span class="math inline">\(P(s1,s2,s3,s1|model)=P(s1)P(s2|s1)P(s3|s2)P(s1|s3)=1*A12*A23*A31=0.004\)</span>？？？ 马尔科夫链：转移弧上有概率的非确定有限状态自动机，圈代表状态，每个结点的出度加起来是1 隐马尔可夫模型HMM：状态转换是不可观察的</p>
<p>EM（expectation maximization） 假设数据点是围绕k个核心点的k个正态分布源产生，目标是根据已知点推断正态分布的核心及参数，这也是一个贝叶斯问题</p>
<p>矛盾之处在于：蛋与鸡的问题 只有已知哪些点属于同一个圈，才能预测参数 只有参数靠谱，才能知道哪些点属于哪个圈</p>
<p>解这种问题，一般要先随机给蛋或鸡，随便猜一个参数，计算每个点属于哪个圈，接着重新评估参数，直至最后参数基本不变，有点kmeans那味。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://eimadrigal.github.io/2019/06/07/#Week7%20Neural%20Networks%20Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/favicon.png">
      <meta itemprop="name" content="EIMadrigal">
      <meta itemprop="description" content="Hello World">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EI Madrigal's Space">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/06/07/#Week7%20Neural%20Networks%20Learning/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta">

		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-06-07 23:09:00" itemprop="dateCreated datePublished" datetime="2019-06-07T23:09:00+08:00">2019-06-07</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="一cost-function-and-backpropagation">一、Cost Function and Backpropagation</h2>
<p>神经网络的损失函数： <span class="math display">\[J(\Theta) = - \frac{1}{m} \sum_{i=1}^m \sum_{k=1}^K \left[y^{(i)}_k \log ((h_\Theta (x^{(i)}))_k) + (1 - y^{(i)}_k)\log (1 - (h_\Theta(x^{(i)}))_k)\right] + \frac{\lambda}{2m}\sum_{l=1}^{L-1} \sum_{i=1}^{s_l} \sum_{j=1}^{s_{l+1}} ( \Theta_{j,i}^{(l)})^2\]</span> <img src="https://img-blog.csdnimg.cn/20190520213954195.png" alt="在这里插入图片描述" /> 这个cost function是在logistic regression基础上演变而来，只是神经网络有很多输出结点，而logistic regression只有一个输出结点，所以这个cost function只是把所有的K个输出结点的损失函数进行累加。</p>
<p>得到cost function后，为了寻找使得<span class="math inline">\(J(\theta)\)</span>最小的那组参数<span class="math inline">\(\theta\)</span>，我们需要知道<span class="math inline">\(J(\theta)\)</span>关于每个<span class="math inline">\(\theta\)</span>的偏导数，而后向传播算法可以帮助我们计算偏导数： <img src="https://img-blog.csdnimg.cn/20190520215248992.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VJTWFkcmlnYWw=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" /> 对于每个训练样本，先利用forward propagation计算每一层的<span class="math inline">\(a\)</span>： <img src="https://img-blog.csdnimg.cn/20190520215514406.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VJTWFkcmlnYWw=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" /> 接着利用样本真实标签<span class="math inline">\(y^{(t)}\)</span>计算最后一层的误差值；</p>
<p>之后从右向左计算每一层（输入层除外）的误差： <img src="https://img-blog.csdnimg.cn/20190520215848578.png" alt="在这里插入图片描述" /> 这样每个样本一次正向、一次反向来更新误差矩阵： <img src="https://img-blog.csdnimg.cn/20190520220141936.png" alt="在这里插入图片描述" /> 向量化表示： <img src="https://img-blog.csdnimg.cn/2019052022020666.png" alt="在这里插入图片描述" /> 最后，就可以得到偏导数： <img src="https://img-blog.csdnimg.cn/20190520220255255.png" alt="在这里插入图片描述" /> <img src="https://img-blog.csdnimg.cn/20190520220307411.png" alt="在这里插入图片描述" /> ## 二、Backpropagation in Pratice 为了使用<code>fminunc</code>等高级的优化方法来求得cost function的最小值，所以将<span class="math inline">\(\theta\)</span>这个矩阵展成向量传入<code>fminunc</code>，完成后可以通过<code>reshape</code>从向量中提取<span class="math inline">\(\theta^{(1)}、\theta^{(2)}\)</span>等： <img src="https://img-blog.csdnimg.cn/20190521193802355.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VJTWFkcmlnYWw=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" /></p>
<p>为了确保我们使用Backpropagation求得的偏导数的正确性，可以使用Gradient Checking（<strong>很慢</strong>）来检验： 根据偏导数定义： <span class="math display">\[\dfrac{\partial}{\partial\Theta_j}J(\Theta) \approx \dfrac{J(\Theta_1, \dots, \Theta_j + \epsilon, \dots, \Theta_n) - J(\Theta_1, \dots, \Theta_j - \epsilon, \dots, \Theta_n)}{2\epsilon}\]</span> <span class="math display">\[一般\epsilon=10^{-4}\]</span> 通过将这种方式计算的偏导数与之前Backpropagation求得的偏导数比较，即可得知Backpropagation的正确性。</p>
<p>之前在Linear Regression和Logistic Regression，我们可以用全0来初始化<span class="math inline">\(\theta\)</span>，但在神经网络中，这样做会有问题，所以采用<strong>随机初始化</strong>： <img src="https://img-blog.csdnimg.cn/20190521195045220.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VJTWFkcmlnYWw=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" /> 最后，从整体捋一遍流程： 1、选择网络结构： <img src="https://img-blog.csdnimg.cn/20190521195351245.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VJTWFkcmlnYWw=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" /> 2、训练神经网络：</p>
<p>对每一个训练样本： <img src="https://img-blog.csdnimg.cn/20190521195432613.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VJTWFkcmlnYWw=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" /></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://eimadrigal.github.io/2019/06/06/Multilayer%20Perceptron/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/favicon.png">
      <meta itemprop="name" content="EIMadrigal">
      <meta itemprop="description" content="Hello World">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EI Madrigal's Space">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/06/06/Multilayer%20Perceptron/" class="post-title-link" itemprop="url">Multilayer Perceptron</a>
        </h2>

        <div class="post-meta">

		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-06-06 14:58:00" itemprop="dateCreated datePublished" datetime="2019-06-06T14:58:00+08:00">2019-06-06</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="non-linear-hypotheses">Non-linear Hypotheses</h2>
<p>线性回归和逻辑回归在特征很多时，计算量会很大。 一个简单的三层神经网络模型： <span class="math display">\[a_i^{(j)} = \text{&quot;activation&quot; of unit $i$ in layer $j$}\]</span><span class="math display">\[\Theta^{(j)} = \text{matrix of weights controlling function mapping from layer $j$ to layer $j+1$}\]</span> <img src="https://img-blog.csdnimg.cn/20190519144739691.png" alt="在这里插入图片描述" /> 其中：<span class="math display">\[a_1^{(2)} = g(\Theta_{10}^{(1)}x_0 + \Theta_{11}^{(1)}x_1 + \Theta_{12}^{(1)}x_2 + \Theta_{13}^{(1)}x_3)\]</span><span class="math display">\[a_2^{(2)} = g(\Theta_{20}^{(1)}x_0 + \Theta_{21}^{(1)}x_1 + \Theta_{22}^{(1)}x_2 + \Theta_{23}^{(1)}x_3)\]</span><span class="math display">\[a_3^{(2)} = g(\Theta_{30}^{(1)}x_0 + \Theta_{31}^{(1)}x_1 + \Theta_{32}^{(1)}x_2 + \Theta_{33}^{(1)}x_3)\]</span><span class="math display">\[h_\Theta(x) = a_1^{(3)} = g(\Theta_{10}^{(2)}a_0^{(2)} + \Theta_{11}^{(2)}a_1^{(2)} + \Theta_{12}^{(2)}a_2^{(2)} + \Theta_{13}^{(2)}a_3^{(2)})\]</span> ## vectorized implementation 将上面公式中函数<span class="math inline">\(g\)</span>中的东西用<span class="math inline">\(z\)</span>代替： <span class="math display">\[a_1^{(2)} = g(z_1^{(2)})\]</span><span class="math display">\[a_2^{(2)} = g(z_2^{(2)})\]</span><span class="math display">\[a_3^{(2)} = g(z_3^{(2)})\]</span> 令<span class="math inline">\(x=a^{(1)}\)</span>： <span class="math display">\[z^{(j)} = \Theta^{(j-1)}a^{(j-1)}\]</span> 得到： <span class="math display">\[
\begin{aligned}z^{(j)} = \begin{bmatrix}z_1^{(j)} \\ z_2^{(j)} \\ \cdots \\z_n^{(j)}\end{bmatrix}\end{aligned}
\]</span></p>
<p>这块的记号比较多，用例子梳理下： 实现一个逻辑与的神经网络： <img src="https://img-blog.csdnimg.cn/20190321105245428.png" alt="在这里插入图片描述" /> 那么： <img src="https://img-blog.csdnimg.cn/20190321105438230.png" alt="在这里插入图片描述" /> <img src="https://img-blog.csdnimg.cn/20190321105453184.png" alt="在这里插入图片描述" /> 所以有： <img src="https://img-blog.csdnimg.cn/20190321105628787.png" alt="在这里插入图片描述" /> 再来一个多层的，实现XNOR功能（两输入都为0或都为1，输出才为1）： <img src="https://img-blog.csdnimg.cn/20190321110644318.png" alt="在这里插入图片描述" /> 基本的神经元：</p>
<ul>
<li>逻辑与 <img src="https://img-blog.csdnimg.cn/2019032111133091.png" alt="在这里插入图片描述" /></li>
<li>逻辑或 <img src="https://img-blog.csdnimg.cn/20190321111355954.png" alt="在这里插入图片描述" /></li>
<li>逻辑非 <img src="https://img-blog.csdnimg.cn/20190321111411429.png" alt="在这里插入图片描述" /> 先构造一个表示后半部分的神经元：<img src="https://img-blog.csdnimg.cn/20190321111519333.png" alt="在这里插入图片描述" /> 这样的： <img src="https://img-blog.csdnimg.cn/20190321111806888.png" alt="在这里插入图片描述" /> 接着将前半部分组合起来： <img src="https://img-blog.csdnimg.cn/20190321112023420.png" alt="在这里插入图片描述" /> ## Multiclass Classification <img src="https://img-blog.csdnimg.cn/20190321165848893.png" alt="在这里插入图片描述" /> ## Motivation ## Implementation</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> FashionMNIST</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span>():</span></span><br><span class="line">    <span class="comment"># My data is in ../data/FashionMNIST/raw &amp; ../data/FashionMNIST/processed</span></span><br><span class="line">    train_data = FashionMNIST(root=<span class="string">&#x27;../data&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">False</span>, transform=transforms.ToTensor())</span><br><span class="line">    test_data = FashionMNIST(root=<span class="string">&#x27;../data&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">False</span>, transform=transforms.ToTensor())</span><br><span class="line">    train_iter = DataLoader(train_data, batch_size=<span class="number">256</span>, shuffle=<span class="literal">True</span>, num_workers=<span class="number">0</span>)</span><br><span class="line">    test_iter = DataLoader(test_data, batch_size=<span class="number">256</span>, shuffle=<span class="literal">False</span>, num_workers=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> train_iter, test_iter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">num_inputs, num_outputs, num_hiddens = <span class="number">784</span>, <span class="number">10</span>, <span class="number">256</span></span><br><span class="line">W1 = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, (num_inputs, num_hiddens)), dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">b1 = torch.zeros(num_hiddens, dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">W2 = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, (num_hiddens, num_outputs)), dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">b2 = torch.zeros(num_outputs, dtype=torch.<span class="built_in">float</span>)</span><br><span class="line"><span class="comment"># W1 = nn.Parameter(torch.randn(num_inputs, num_hiddens, requires_grad=True) * 0.01)</span></span><br><span class="line"><span class="comment"># b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True))</span></span><br><span class="line"><span class="comment"># W2 = nn.Parameter(torch.randn(num_hiddens, num_outputs, requires_grad=True) * 0.01)</span></span><br><span class="line"><span class="comment"># b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True))</span></span><br><span class="line">params = [W1, b1, W2, b2]</span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">    param.requires_grad_(requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span>(<span class="params">X</span>):</span></span><br><span class="line">    <span class="keyword">return</span> torch.<span class="built_in">max</span>(<span class="built_in">input</span>=X, other=torch.zeros(X.shape))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span>(<span class="params">X</span>):</span></span><br><span class="line">    X_exp = torch.exp(X)</span><br><span class="line">    partition = X_exp.<span class="built_in">sum</span>(<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> X_exp / partition</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">net</span>(<span class="params">X</span>):</span></span><br><span class="line">    X = X.reshape((-<span class="number">1</span>, num_inputs))</span><br><span class="line">    H = relu(torch.matmul(X, W1) + b1)</span><br><span class="line">    output = torch.matmul(H, W2) + b2</span><br><span class="line">    <span class="keyword">return</span> softmax(output)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cross_entropy</span>(<span class="params">y_predict, y</span>):</span></span><br><span class="line">    <span class="keyword">return</span> -torch.log(y_predict[<span class="built_in">range</span>(<span class="built_in">len</span>(y_predict)), y])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd</span>(<span class="params">params, lr, batch_size</span>):</span></span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param.data -= lr * param.grad / batch_size</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_accuracy</span>(<span class="params">net, data_iter</span>):</span></span><br><span class="line">    acc_sum, n = <span class="number">0.0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        acc_sum += (net(X).argmax(dim=<span class="number">1</span>) == y).<span class="built_in">float</span>().<span class="built_in">sum</span>().item()</span><br><span class="line">        n += y.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> acc_sum / n</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">net, train_iter, test_iter, loss, num_epochs, batch_size=<span class="number">256</span>, lr=<span class="number">0.1</span>, params=<span class="literal">None</span>, optimizer=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        train_loss_sum, train_acc_sum, n = <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            y_predict = net(X)</span><br><span class="line">            l = cross_entropy(y_predict, y).<span class="built_in">sum</span>()</span><br><span class="line">            l.backward()</span><br><span class="line">            sgd(params, lr, batch_size)</span><br><span class="line"></span><br><span class="line">            W1.grad.data.zero_()</span><br><span class="line">            b1.grad.data.zero_()</span><br><span class="line">            W2.grad.data.zero_()</span><br><span class="line">            b2.grad.data.zero_()</span><br><span class="line"></span><br><span class="line">            train_loss_sum += l.item()</span><br><span class="line">            train_acc_sum += (y_predict.argmax(dim=<span class="number">1</span>) == y).<span class="built_in">sum</span>().item()</span><br><span class="line">            n += y.shape[<span class="number">0</span>]</span><br><span class="line">        test_acc = evaluate_accuracy(net, test_iter)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;epoch %d, loss %.4f, train_acc %.4f, test_acc %.4f&#x27;</span></span><br><span class="line">              % (epoch + <span class="number">1</span>, train_loss_sum / n, train_acc_sum / n, test_acc))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)</span></span><br><span class="line">    train_iter, test_iter = load_data()</span><br><span class="line">    train(net, train_iter, test_iter, cross_entropy, <span class="number">10</span>, lr=<span class="number">0.1</span>, params=params)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.init <span class="keyword">as</span> init</span><br><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> FashionMNIST</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span>():</span></span><br><span class="line">    <span class="comment"># My data is in ../data/FashionMNIST/raw &amp; ../data/FashionMNIST/processed</span></span><br><span class="line">    train_data = FashionMNIST(root=<span class="string">&#x27;../data&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">False</span>, transform=transforms.ToTensor())</span><br><span class="line">    test_data = FashionMNIST(root=<span class="string">&#x27;../data&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">False</span>, transform=transforms.ToTensor())</span><br><span class="line">    train_iter = DataLoader(train_data, batch_size=<span class="number">256</span>, shuffle=<span class="literal">True</span>, num_workers=<span class="number">0</span>)</span><br><span class="line">    test_iter = DataLoader(test_data, batch_size=<span class="number">256</span>, shuffle=<span class="literal">False</span>, num_workers=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> train_iter, test_iter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_inputs, num_outputs, num_hiddens</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.l1 = nn.Linear(num_inputs, num_hiddens)</span><br><span class="line">        self.relu1 = nn.ReLU()</span><br><span class="line">        self.l2 = nn.Linear(num_hiddens, num_outputs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, X</span>):</span></span><br><span class="line">        X = X.view(X.shape[<span class="number">0</span>], -<span class="number">1</span>)</span><br><span class="line">        o1 = self.relu1(self.l1(X))</span><br><span class="line">        o2 = self.l2(o1)</span><br><span class="line">        <span class="keyword">return</span> o2</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_params</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> self.parameters():</span><br><span class="line">            init.normal_(param, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_accuracy</span>(<span class="params">net, data_iter</span>):</span></span><br><span class="line">    acc_sum, n = <span class="number">0.0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter:</span><br><span class="line">        acc_sum += (net(X).argmax(dim=<span class="number">1</span>) == y).<span class="built_in">float</span>().<span class="built_in">sum</span>().item()</span><br><span class="line">        n += y.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> acc_sum / n</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">net, train_iter, test_iter, loss, num_epochs, batch_size=<span class="number">256</span>, lr=<span class="number">0.1</span>, params=<span class="literal">None</span>, optimizer=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        train_loss_sum, train_acc_sum, n = <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> X, y <span class="keyword">in</span> train_iter:</span><br><span class="line">            y_predict = net(X)</span><br><span class="line">            l = loss(y_predict, y).<span class="built_in">sum</span>()</span><br><span class="line">            l.backward()</span><br><span class="line"></span><br><span class="line">            optimizer.step()</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">            train_loss_sum += l.item()</span><br><span class="line">            train_acc_sum += (y_predict.argmax(dim=<span class="number">1</span>) == y).<span class="built_in">sum</span>().item()</span><br><span class="line">            n += y.shape[<span class="number">0</span>]</span><br><span class="line">        test_acc = evaluate_accuracy(net, test_iter)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;epoch %d, loss %.4f, train_acc %.4f, test_acc %.4f&#x27;</span></span><br><span class="line">              % (epoch + <span class="number">1</span>, train_loss_sum / n, train_acc_sum / n, test_acc))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">num_inputs, num_outputs, num_hiddens = <span class="number">784</span>, <span class="number">10</span>, <span class="number">256</span></span><br><span class="line">net = Net(num_inputs, num_outputs, num_hiddens)</span><br><span class="line">net.init_params()</span><br><span class="line"></span><br><span class="line">loss = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    train_iter, test_iter = load_data()</span><br><span class="line">    train(net, train_iter, test_iter, loss, <span class="number">10</span>, optimizer=optimizer)</span><br><span class="line">    </span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://eimadrigal.github.io/2019/06/05/Advice%20for%20applying%20ML%20&%20ML%20System%20Design/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/favicon.png">
      <meta itemprop="name" content="EIMadrigal">
      <meta itemprop="description" content="Hello World">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EI Madrigal's Space">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/06/05/Advice%20for%20applying%20ML%20&%20ML%20System%20Design/" class="post-title-link" itemprop="url">Advice for applying ML & ML System Design</a>
        </h2>

        <div class="post-meta">

		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-06-05 14:57:00" itemprop="dateCreated datePublished" datetime="2019-06-05T14:57:00+08:00">2019-06-05</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="一the-problem-of-overfitting">一、The Problem of Overfitting</h2>
<p><img src="https://img-blog.csdnimg.cn/20190320163616569.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VJTWFkcmlnYWw=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" /> <img src="https://img-blog.csdnimg.cn/20190320164216967.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VJTWFkcmlnYWw=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" /> 欠拟合（high bias）：模型不能很好地适应训练集； 过拟合（high variance）：模型过于强调拟合原始数据，测试时效果会比较差。 处理过拟合： 1、丢弃一些特征，包括人工丢弃和算法选择； 2、正则化：保留所有特征，但减小参数的值。 ## 二、Cost Function 过拟合一般是由高次项引起，那么我们可以通过增加某些项的cost，来降低它们的权重。 在梯度下降过程中，要使损失函数变小，那么<span class="math inline">\(\theta\)</span>就会变得很小，所以假设函数中的<span class="math inline">\(\theta\)</span>就会变小，该项的权重就会降低。</p>
<p>如果不知道要惩罚哪些特征，可以一起惩罚（除了<span class="math inline">\(\theta_0\)</span>）。 将代价函数改为： <img src="https://img-blog.csdnimg.cn/20190320165635129.png" alt="在这里插入图片描述" /> <span class="math inline">\(\lambda\)</span>是正则化参数。 如果<span class="math inline">\(\lambda\)</span>过大，那么所有的参数都会最小化，那么假设就会变为<span class="math inline">\(h_\theta(x)=\theta_0\)</span>，造成欠拟合。 ## 三、Regularized Linear Regression <span class="math inline">\(\theta_0\)</span>没有正则化处理，所以梯度下降要分情况： <img src="https://img-blog.csdnimg.cn/20190320170642494.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VJTWFkcmlnYWw=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" /> 化简下： <img src="https://img-blog.csdnimg.cn/20190320170825316.png" alt="在这里插入图片描述" /> 可以看到： 正则化后的参数更新比原来多减小了一个值。</p>
<p>再看线性回归的另外一个工具：常规方程。 <img src="https://img-blog.csdnimg.cn/20190320171502423.png" alt="在这里插入图片描述" /> 推导过程省略...... ## 四、Regularized Logistic Regression 对于逻辑回归的代价函数，同样增加一个正则化表达式： <img src="https://img-blog.csdnimg.cn/2019032019010815.png" alt="在这里插入图片描述" /> 梯度下降算法与线性回归相同，不过<span class="math inline">\(h_\theta(x)\)</span>不同。 <img src="https://img-blog.csdnimg.cn/20190320190430424.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VJTWFkcmlnYWw=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" /></p>
<h2 id="一evaluating-a-learning-algorithm">一、Evaluating a Learning Algorithm</h2>
<p>训练后测试时如果发现模型表现很差，可以有很多种方法去更改：</p>
<ol type="1">
<li>用更多的训练样本；</li>
<li>减少/增加特征数目；</li>
<li>尝试多项式特征；</li>
<li>增大/减小正则化参数<span class="math inline">\(\lambda\)</span>。 那么该怎么去选择采用哪种方式呢？ 一般将70%的数据作为训练集，30%的数据作为测试集。 先用训练集最小化<span class="math inline">\(J_{train}(\Theta)\)</span>，得到一组参数值<span class="math inline">\(\Theta\)</span>； 然后计算测试集误差<span class="math inline">\(J_{test}(\Theta)\)</span>： 对于<strong>线性回归</strong>： <img src="https://img-blog.csdnimg.cn/20190603210422113.png" alt="在这里插入图片描述" /> 对于<strong>逻辑回归</strong>： <img src="https://img-blog.csdnimg.cn/20190603210631921.png" alt="在这里插入图片描述" /> 测试集的平均误差（分类错误的比率）： <img src="https://img-blog.csdnimg.cn/20190603210807680.png" alt="在这里插入图片描述" /> 假设要选择用几次多项式<span class="math inline">\(d\)</span>去作为假设函数，那么做法就是不断尝试<span class="math inline">\(d\)</span>，选择一个在测试集上损失最小的<span class="math inline">\(d\)</span>，以此作为模型泛化能力的衡量。但是这样是有问题的，因为<span class="math inline">\(d\)</span>相当于是被测试集训练的，再用测试集去测试，很不公平。所以一般将数据集分为3部分：60%训练集、20%交叉验证集、20%测试集： <img src="https://img-blog.csdnimg.cn/20190605102227988.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VJTWFkcmlnYWw=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" /> ## 二、Bias vs. Variance 看图： <img src="https://img-blog.csdnimg.cn/20190719151251691.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VJTWFkcmlnYWw=,size_16,color_FFFFFF,t_70" alt="图来自知乎" /> <img src="https://img-blog.csdnimg.cn/20190605102435642.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VJTWFkcmlnYWw=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" /> 正则化和Bias/Variance的关系： <img src="https://img-blog.csdnimg.cn/20190605102608526.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VJTWFkcmlnYWw=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" /> 训练集大小与Bias/Variance的关系： <img src="https://img-blog.csdnimg.cn/20190605102730371.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VJTWFkcmlnYWw=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" /> <img src="https://img-blog.csdnimg.cn/20190605102745765.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VJTWFkcmlnYWw=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" /> ## 三、Error Analysis Andrew推荐的流程： <img src="https://img-blog.csdnimg.cn/20190605103603490.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VJTWFkcmlnYWw=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" /> ## 四、Handling Skewed Data 如果数据集中正负类的数据规模差距过大，只用误差衡量模型是不可靠的，此时需要查准率和召回率两个指标。 <img src="https://img-blog.csdnimg.cn/20190605104947575.png" alt="在这里插入图片描述" /> 如何权衡这两个指标，一般使用<span class="math inline">\(F1\)</span>得分： <span class="math display">\[F_1=2\frac{PR}{P+R}\]</span></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://eimadrigal.github.io/2019/06/04/Logistic%20Regression/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/favicon.png">
      <meta itemprop="name" content="EIMadrigal">
      <meta itemprop="description" content="Hello World">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EI Madrigal's Space">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/06/04/Logistic%20Regression/" class="post-title-link" itemprop="url">Logistic Regression</a>
        </h2>

        <div class="post-meta">

		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-06-04 14:55:00" itemprop="dateCreated datePublished" datetime="2019-06-04T14:55:00+08:00">2019-06-04</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="motivation">Motivation</h2>
<p>If y only takes a finite set of discrete values such as {0,1}, then using Linear Regression to predict a <span class="math inline">\(\hat y&gt;1/\hat y&lt;0\)</span> does not make sense at all. But fortunately we can fix Linear Regression to produce a value between [0,1]. ## Details We choose sigmoid/logistic function to map the value: <span class="math display">\[h_\theta(x)=g(\theta^Tx),g(z)=\frac{1}{1+e^{-z}}\]</span> <img src="https://img-blog.csdnimg.cn/20190516212200143.png" alt="在这里插入图片描述" /> We can assume that: <span class="math display">\[h_\theta(x)=P(y=1|x;\theta)\\
1-h_\theta(x)=P(y=0|x;\theta)\]</span> Or more compactly: <span class="math display">\[p(y|x;\theta)=[h_\theta(x)]^y[1-h_\theta(x)]^{1-y}\]</span> Now we will use maximum likelihood to fit parameters <span class="math inline">\(\theta\)</span>, assume n training examples are independent, then the likelihood of the parameters is: <span class="math display">\[L(\theta)=p(\vec y|X;\theta)=\prod_{i=1}^{n}p(y^{(i)}|x^{(i)};\theta)=\prod_{i=1}^{n}[h(x^{(i)})]^{y^{(i)}}[1-h(x^{(i)})]^{1-y^{(i)}}\]</span> To make life easier, we use the log likelihood: <span class="math display">\[l(\theta)=log\ L(\theta)=\sum_{i=1}^{n}y^{(i)}log\ h(x^{(i)})+(1-y^{(i)})log\ (1-h(x^{(i)}))\]</span> Let's first take out one example <span class="math inline">\((x,y)\)</span> to derive the stochastic gradient ascent rule: <span class="math display">\[\frac{\partial}{\partial\theta_j}l(\theta)=[y\frac{1}{g(\theta^Tx)}-(1-y)\frac{1}{1-g(\theta^Tx)}]\frac{\partial}{\partial\theta_j}g(\theta^Tx) \\=[y\frac{1}{g(\theta^Tx)}-(1-y)\frac{1}{1-g(\theta^Tx)}]g(\theta^Tx)(1-g(\theta^Tx))\frac{\partial}{\partial\theta_j}\theta^Tx \\=[y(1-g(\theta^Tx))-(1-y)g(\theta^Tx)]x_j=(y-h_\theta(x))x_j\]</span> Then we can update the parameters: <span class="math display">\[\theta_j=\theta_j+\alpha(y^{(i)}-h_{\theta}(x^{(i)}))x_j^{(i)}\]</span></p>
<p>Here we use maximum likelihood to get the update rule. Generally we would like to minimize the object function. So we can add a negative sign to the maximum likelihood's formula, it is called <strong>logistic loss</strong>. Thus there exists another way to understand it.</p>
<p>The loss on a single sample can be formulated as follows: <span class="math display">\[
cost(h_{\theta}(x),y)=\left\{
\begin{aligned}
-log(h_{\theta}(x))\ \ \ if\ y=1\\
-log(1-h_{\theta}(x))\ \ \ if\ y=0
\end{aligned}
\right.
\]</span> If y=1 and the prediction=1, then loss=0; else if y=1 and the prediction=0, then loss=<span class="math inline">\(+\infin\)</span> is a huge penalty for the totally wrong prediction. It is the same for y=0.</p>
<p>We can unify the two cases together and the loss for the whole training data is: <span class="math display">\[
cost((h_{\theta}(x),y))=-ylog(h_{\theta}(x))-(1-y)log(1-h_{\theta}(x))\\=-\frac{1}{m}\sum_{i=1}^{m}[y^{(i)}log(h_{\theta}(x^{(i)}))+(1-y^{(i)})log(1-h_{\theta}(x^{(i)}))]
\]</span> Here the reason why we don't use the MSE loss such as Linear Regression is that the <span class="math inline">\(J(\theta)\)</span> is non-convex and very hard to optimize for the global optimum.</p>
<p>To make life easier again, we can write the formula as the vectorized version: <span class="math display">\[h = g(X\theta),J(\theta) = \frac{1}{m} \cdot \left(-y^{T}\log(h)-(1-y)^{T}\log(1-h)\right)\]</span> Then our goal is to minimize <span class="math inline">\(J(\theta)\)</span> and get appropriate parameters <span class="math inline">\(\theta\)</span> and use <span class="math inline">\(h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}\)</span> to get our predictions.</p>
<p>Since it is a little complex to get answer analytically, so we still use Gradient Descent to minimize the loss numerically. The update rule is the same as the above one: <span class="math display">\[
\theta_j=\theta_j+\alpha\frac{1}{m}\sum_{i=1}^{m}(y^{(i)}-h_{\theta}(x^{(i)}))x_j^{(i)}
\]</span> Here you should notice that all <span class="math inline">\(\theta_j\)</span> should be updated simultaneously when you program. Again the vectorized version: <span class="math display">\[\theta=\theta-\frac{\alpha}{m}X^T[g(X\theta)-y]\]</span> It is the same formula as the Linear Regression except that <span class="math inline">\(h_\theta(x)\)</span> is different. ## 牛顿法 除了用梯度上升法去最大化<span class="math inline">\(l(\theta)\)</span>，牛顿迭代法也能干这件事。</p>
<p>普通同学都是在求方程的零点<span class="math inline">\(f(\theta)=0\)</span>时接触到牛顿法，其更新规则为： <span class="math display">\[\theta=\theta-\frac{f(\theta)}{f^{&#39;}(\theta)}\]</span> 这个规则可以理解为：我们一直在用一个线性函数去近似<span class="math inline">\(f\)</span>，因此希望下一次迭代的<span class="math inline">\(\theta\)</span>就是该线性函数的零点： <img src="https://img-blog.csdnimg.cn/20210616190823439.png" alt="在这里插入图片描述" /> 再结合一点高中数学，<span class="math inline">\(l(\theta)\)</span>极大值点处的一阶导数为0，因此只要令<span class="math inline">\(l^{&#39;}(\theta)=0\)</span>就能解出对应的<span class="math inline">\(\theta\)</span>： <span class="math display">\[\theta=\theta-\frac{l^{&#39;}(\theta)}{l^{&#39;&#39;}(\theta)}\]</span> 由于逻辑回归中<span class="math inline">\(\theta\)</span>是向量而非scalar，因此需要稍稍改变下更新规则： <span class="math display">\[\theta=\theta-H^{-1}\nabla_{\theta}l(\theta)\]</span> 其中，Hessian阵中的元素为<span class="math inline">\(H_{ij}=\frac{\partial^2l(\theta)}{\partial\theta_i\partial\theta_j}\)</span>。</p>
<p>牛顿法通常比梯度上升收敛快得多，因为利用了<span class="math inline">\(l(\theta)\)</span>的二阶信息，但是存储和求解<span class="math inline">\(H^{-1}\)</span>开销会比较大。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://eimadrigal.github.io/2019/06/03/Linear%20Regression/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/favicon.png">
      <meta itemprop="name" content="EIMadrigal">
      <meta itemprop="description" content="Hello World">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EI Madrigal's Space">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/06/03/Linear%20Regression/" class="post-title-link" itemprop="url">Linear Regression</a>
        </h2>

        <div class="post-meta">

		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-06-03 14:54:00" itemprop="dateCreated datePublished" datetime="2019-06-03T14:54:00+08:00">2019-06-03</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="线性回归模型">线性回归模型</h2>
<p><span class="math display">\[h_{\theta}(x)=\sum_{i=0}^{d}\theta_ix_i=\theta^Tx=x^T\theta\]</span> 其中，<span class="math inline">\(x_0=1\)</span>，<span class="math inline">\(d\)</span>表示<span class="math inline">\(x\)</span>的特征数量。</p>
<p>在给定训练集下，需要用学习算法确定参数<span class="math inline">\(\theta\)</span>，使得模型的预测值<span class="math inline">\(h(x)\)</span>与真实值<span class="math inline">\(y\)</span>尽可能接近。为了精确地描述这种接近程度，定义损失函数： <span class="math display">\[J(\theta)=\frac{1}{2}\sum_{i=1}^{n}(h_{\theta}(x^{(i)})-y^{(i)})^2\]</span> 问题转化为选择一组<span class="math inline">\(\theta\)</span>使得<span class="math inline">\(J(\theta)\)</span>最小，这里有2种求解方法：</p>
<ol type="1">
<li>梯度下降</li>
<li>Normal Equation ## 梯度下降 梯度下降的motivation非常直观，首先随机选择一组参数<span class="math inline">\(\theta\)</span>，接着沿<span class="math inline">\(J(\theta)\)</span>下降最快的方向更新<span class="math inline">\(\theta\)</span>，经过若干次迭代就有望找到令<span class="math inline">\(J(\theta)\)</span>收敛的参数<span class="math inline">\(\theta\)</span>： <span class="math display">\[\theta_j=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta)\]</span> 将<span class="math inline">\(J(\theta)\)</span>的偏导数代入即得到所谓的batch gradient descent更新规则： <span class="math display">\[\theta_j=\theta_j-\alpha\sum_{i=1}^{n}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}\]</span> 其向量化表示为： <span class="math display">\[\theta=\theta-\alpha\sum_{i=1}^{n}(h_\theta(x^{(i)})-y^{(i)})x^{(i)}\]</span> 由于损失函数<span class="math inline">\(J(\theta)\)</span>是凸二次函数，因此总能收敛到唯一的全局最小值。</li>
</ol>
<p>batch gradient descent一次更新需要计算所有训练样本，开销较大，因此有同学提出了stochastic gradient descent，每遇到一个训练样本就进行一次参数更新： <span class="math display">\[\theta=\theta-\alpha(h_\theta(x^{(i)})-y^{(i)})x^{(i)}\]</span> stochastic gradient descent一般比batch gradient descent收敛快，但是有可能在<span class="math inline">\(J(\theta)\)</span>的最优点附近振荡，永远无法收敛到精确最优。不过一般选择最优点附近的参数也可以接受，还可以通过递减学习率<span class="math inline">\(\alpha\)</span>确保其精确收敛。</p>
<p>值得一提的是：梯度下降算法存在“锯齿”效应，因此为了加速收敛，通常要进行归一化处理使得不同特征的尺度相近。 ## Normal Equation 除了用迭代的方式求解<span class="math inline">\(J(\theta)\)</span>的最小值，还可以用数学工具直接求得闭式解。</p>
<p>为了简洁地表示后续求导，使得人生不要太过凌乱，我们首先研究下<span class="math inline">\(J(\theta)\)</span>的向量表示： 假设训练集<span class="math inline">\(X\)</span>和对应的标签<span class="math inline">\(y\)</span>分别为： <span class="math display">\[X=\left[
\begin{matrix}
 (x^{(1)})^T \\
 (x^{(2)})^T \\
 \vdots \\
 (x^{(n)})^T \\
\end{matrix}
\right],y=\left[
\begin{matrix}
 y^{(1)} \\
 y^{(2)} \\
 \vdots \\
 y^{(n)} \\
\end{matrix}
\right]
\]</span> 由于<span class="math inline">\(h_{\theta}(x^{(i)})=(x^{(i)})^T\theta\)</span>，所以有： <span class="math display">\[X\theta-y=\left[
\begin{matrix}
 (x^{(1)})^T\theta \\
 (x^{(2)})^T\theta \\
 \vdots \\
 (x^{(n)})^T\theta \\
\end{matrix}
\right]-\left[
\begin{matrix}
 y^{(1)} \\
 y^{(2)} \\
 \vdots \\
 y^{(n)} \\
\end{matrix}
\right]=\left[
\begin{matrix}
 h_{\theta}(x^{(1)})-y^{(1)}  \\
 h_{\theta}(x^{(2)})-y^{(2)} \\
 \vdots \\
 h_{\theta}(x^{(n)})-y^{(n)} \\
\end{matrix}
\right]\]</span> 根据向量运算法则<span class="math inline">\(x^Tx=\sum_ix_i^2\)</span>，终于得到了<span class="math inline">\(J(\theta)\)</span>的简单点的表示： <span class="math display">\[\frac{1}{2}(X\theta-y)^T(X\theta-y)=\frac{1}{2}\sum_{i=1}^{n}(h_{\theta}(x^{(i)})-y^{(i)})^2=J(\theta)\]</span> 利用高中数学导数的知识，只要求得<span class="math inline">\(J(\theta)\)</span>关于参数<span class="math inline">\(\theta\)</span>的导数并令其为0，就大功告成了： <span class="math display">\[\nabla_{\theta}J(\theta)=\frac{1}{2}(X\theta-y)^T(X\theta-y)\\=\frac{1}{2}\nabla_{\theta}[(X\theta)^TX\theta-(X\theta)^Ty-y^T(X\theta)+y^Ty]=\frac{1}{2}\nabla_{\theta}[\theta^T(X^TX)\theta-y^T(X\theta)-y^T(X\theta)]\\=\frac{1}{2}\nabla_{\theta}[\theta^T(X^TX)\theta-2(X^Ty)^T\theta]=\frac{1}{2}(2X^TX\theta-2X^Ty)=X^TX\theta-X^Ty\]</span> 哦，高中数学好像不太够，还要知道<span class="math inline">\(a^Tb=b^Ta,\nabla_{x}Ax=A^T,\nabla_{x}x^TAx=(A+A^T)x\)</span>。</p>
<p>结束了无聊的数学推导，所谓的Normal Equation就来了： <span class="math display">\[X^TX\theta=X^Ty\]</span> 我们暂时先不考虑<span class="math inline">\(X^TX\)</span>不可逆的情况，最终的解析解就是<span class="math inline">\(\theta=(X^TX)^{-1}X^Ty\)</span>。这种方法不需要做Feature Scaling，但是只能用于容易求解的模型。 ## Probabilistic view 当观测数据满足一些假设条件时，就可以自然而然地推导出均方误差形式的损失函数。</p>
<p>假设观测数据满足： <span class="math display">\[y^{(i)}=\theta^Tx^{(i)}+\epsilon^{(i)}\]</span> 其中，<span class="math inline">\(\epsilon^{(i)}\)</span>表示偏差项，并且<span class="math inline">\(\epsilon^{(i)}\)</span>服从IID的高斯分布，即<span class="math inline">\(\epsilon^{(i)}\sim \mathcal{N}(0, \sigma^2)\)</span>。</p>
<p>在满足上述假设的条件下，给定<span class="math inline">\(x^{(i)}\)</span>，观测到的<span class="math inline">\(y^{(i)}\)</span>满足概率分布<span class="math inline">\(y^{(i)}|x^{(i)};\theta\sim \mathcal{N}(\theta^Tx^{(i)}, \sigma^2)\)</span>，即： <span class="math display">\[p(y^{(i)}|x^{(i)};\theta)=\frac{1}{\sqrt{2\pi }\sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2 \sigma^2})\]</span> 我们希望选择合适的参数<span class="math inline">\(\theta\)</span>，使得在整个训练集上最大化观测数据出现的概率，也就是所谓的极大似然估计： <span class="math display">\[\prod_{i=1}^{n}p(y^{(i)}|x^{(i)};\theta)=\prod_{i=1}^{n}\frac{1}{\sqrt{2\pi }\sigma}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2 \sigma^2})=L(\theta)\]</span> To make our life easier，采用对数似然函数的形式去求<span class="math inline">\(L(\theta)\)</span>的最大值： <span class="math display">\[l(\theta)=log\ L(\theta)=nlog\ \frac{1}{\sqrt{2\pi }\sigma}-\frac{1}{2\sigma^2}\sum_{i=1}^n(y^{(i)}-\theta^Tx^{(i)})^2\]</span> 因此，最大化<span class="math inline">\(L(\theta)\)</span>与最小化<span class="math inline">\(J(\theta)=\frac{1}{2}\sum_{i=1}^{n}(h_{\theta}(x^{(i)})-y^{(i)})^2\)</span>等价，也就证明了均方误差损失函数的合理性。</p>
<p>值得一提的是：上述假设并不唯一，存在其它合理的假设同样能够证明均方误差作为损失函数的合理性。 ## 局部加权线性回归 在朴素的线性回归中，训练模型得到的参数<span class="math inline">\(\theta\)</span>是固定的，对于每个要预测的点<span class="math inline">\(x\)</span>计算<span class="math inline">\(\theta^Tx\)</span>就完事了。这种参数化的学习算法在预测时不需要训练数据的支持，非常快捷。</p>
<p>局部加权线性回归的motivation在于：朴素线性模型强行拟合所有训练样本，因为模型简单往往欠拟合。对于任意一个样本<span class="math inline">\(x\)</span>，如果只根据其周围几个样本来建立局部的线性模型，且距离<span class="math inline">\(x\)</span>越近其在损失函数中的权值越大，就得到了所谓的Locally Weighted Linear Regression： <span class="math display">\[J(\theta)=\frac{1}{2}\sum_{i=1}^{n}w^{(i)}(h_{\theta}(x^{(i)})-y^{(i)})^2\]</span> 直观上看：如果一个点权值较大，其对损失函数的贡献就越大；如果权值较小，那么该点基本可以忽略不计。</p>
<p>权值一般会设计为指数函数： <span class="math display">\[w^{(i)}=exp(-\frac{(x^{(i)}-x)^T(x^{(i)}-x)}{2\tau^2})\]</span> 其中，<span class="math inline">\(x\)</span>表示待测试样本，<span class="math inline">\(\tau\)</span>负责控制随距离增加权值的衰减快慢。</p>
<p>另外，与kNN类似，LWR也是一种懒惰学习算法，即只有给出测试样例时才会训练并预测。因此，这种非参数算法在预测时需要存储训练集，并且参数数量会随训练集大小线性增长。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://eimadrigal.github.io/2019/06/02/Machine%20Learning%20Introduction/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/favicon.png">
      <meta itemprop="name" content="EIMadrigal">
      <meta itemprop="description" content="Hello World">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EI Madrigal's Space">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/06/02/Machine%20Learning%20Introduction/" class="post-title-link" itemprop="url">Machine Learning Introduction</a>
        </h2>

        <div class="post-meta">

		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-06-02 14:53:00" itemprop="dateCreated datePublished" datetime="2019-06-02T14:53:00+08:00">2019-06-02</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>对一个背后的pattern很复杂的问题，我们无法显式编程求解，就需要ML。如果能做到100%正确并且这个过程并不很复杂，就完全没有必要上ML。</p>
<p>ML我们并没有也不可能直接考虑所有可能出现的情况，然后用对应的方法解决。此时我们编写的程序其实是在操纵一个学习器，它可以根据模型参数的不同灵活应对输入的情况，并且每组参数都可以很好地处理输入的变化。</p>
<p>现实生活中有许多不同类型的问题，因此学习器也有很多不同的类型，不同类型的学习器针对不同的问题，当然也可以一对多或者多对一或者多对多。训练就是要根据数据获得一组表现良好的参数，下次给出相似类型的输入，我们的模型（参数固定的学习器）可以获得良好的预测表现。此外，还要有一个衡量当前这组参数表现好坏的措施，这就是objective function，训练时要用algorithm去有策略地调整参数，优化objective/loss function。</p>
<p>数据的难点在于：数量要够只是基本，数据应该合理反应所有可能的类别，对于特定任务的特征选择也应该慎重，比如简历筛选时如果把历史面试结果作为一个特征，那么就很可能在无意中引入historical injustices。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://eimadrigal.github.io/2019/06/01/Topological%20Sort/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/favicon.png">
      <meta itemprop="name" content="EIMadrigal">
      <meta itemprop="description" content="Hello World">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EI Madrigal's Space">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/06/01/Topological%20Sort/" class="post-title-link" itemprop="url">Topological Sort</a>
        </h2>

        <div class="post-meta">

		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-06-01 04:11:00" itemprop="dateCreated datePublished" datetime="2019-06-01T04:11:00+08:00">2019-06-01</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>拓扑排序将有向无环图的所有顶点排成一个线性序列，使得其中任意两个顶点<span class="math inline">\(u、v\)</span>，若存在有向边<span class="math inline">\(u-&gt;v\)</span>，那么在线性序列中<span class="math inline">\(u\)</span>必然在<span class="math inline">\(v\)</span>之前。</p>
<p>思想：</p>
<ol type="1">
<li>将所有入度为0的顶点入队；</li>
<li>取队首结点输出，删除所有从该结点出发的边，并将这些边到达的顶点的入度减1，若某顶点入度减为0，将其入队；</li>
<li>重复2，直到队列为空。若进过队的结点数为<span class="math inline">\(n\)</span>，排序成功，否则<strong>图中有环</strong>。</li>
</ol>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*复杂度O(V+E)*/</span></span><br><span class="line"><span class="keyword">int</span> vertexNum;</span><br><span class="line">vector&lt;<span class="keyword">int</span>&gt; adjList[MAXV];  <span class="comment">//邻接表 </span></span><br><span class="line"><span class="keyword">int</span> inDegree[MAXV];    <span class="comment">//顶点入度，读入时记录</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">topologicalSort</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="keyword">int</span> cnt = <span class="number">0</span>;</span><br><span class="line">	queue&lt;<span class="keyword">int</span>&gt; q;   <span class="comment">//若有多个入度为0的顶点要选择编号最小的，可使用priority_queue</span></span><br><span class="line">	</span><br><span class="line">	<span class="comment">//将所有入度为0的顶点入队 </span></span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i &lt; vertexNum;i++)</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="keyword">if</span>(inDegree[i] == <span class="number">0</span>)</span><br><span class="line">		q.<span class="built_in">push</span>(i);</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">while</span>(!q.<span class="built_in">empty</span>())</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="keyword">int</span> front = q.<span class="built_in">front</span>();</span><br><span class="line">		cout &lt;&lt; front &lt;&lt; endl;  <span class="comment">//输出拓扑序列</span></span><br><span class="line">		q.<span class="built_in">pop</span>();</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i &lt; adjList[front].<span class="built_in">size</span>();i++)</span><br><span class="line">		&#123;</span><br><span class="line">			<span class="keyword">int</span> v = adjList[front][i];  <span class="comment">//front的后继结点</span></span><br><span class="line">			inDegree[v]--;</span><br><span class="line">			<span class="keyword">if</span>(indegree[v] == <span class="number">0</span>)</span><br><span class="line">				q.<span class="built_in">push</span>(v); </span><br><span class="line">		&#125;</span><br><span class="line">		adjList[front].<span class="built_in">clear</span>();   <span class="comment">//删掉从该顶点出发的所有边</span></span><br><span class="line">		cnt++;</span><br><span class="line">	&#125;</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">if</span>(cnt == vertexNum)</span><br><span class="line">		<span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">	<span class="keyword">else</span></span><br><span class="line">		<span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure>
<p>``` // dfs version, reverse ans is the topological sequence enum states {UNKNOWN, VISITING, VISITED}; bool hasCycle(vector&lt;vector<int>&gt;&amp; graph, int cur, vector<int>&amp; state, vector<int>&amp; ans) { if (state[cur] == VISITING) return true; if (state[cur] == VISITED) return false;</p>
<pre><code>state[cur] = VISITING;
for (auto neighbors : graph[cur]) &#123;
    if (hasCycle(graph, neighbors, state, ans))
        return true;
&#125;
state[cur] = VISITED;
ans.push_back(cur);

return false;</code></pre>
<p>} ```</int></int>&lt;/vector<int></int></int></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://eimadrigal.github.io/2019/05/24/Mathematics%20for%20Machine%20Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/favicon.png">
      <meta itemprop="name" content="EIMadrigal">
      <meta itemprop="description" content="Hello World">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EI Madrigal's Space">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2019/05/24/Mathematics%20for%20Machine%20Learning/" class="post-title-link" itemprop="url">Mathematics for Machine Learning</a>
        </h2>

        <div class="post-meta">

		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-05-24 09:11:00" itemprop="dateCreated datePublished" datetime="2019-05-24T09:11:00+08:00">2019-05-24</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="motivation">Motivation</h2>
<p><span class="math inline">\(f(\mathbf{x}) = \boldsymbol{\beta}^\top\mathbf{x}\)</span> <span class="math display">\[\frac{df}{d\mathbf{x}} = \begin{bmatrix}
\frac{df}{dx_1} \\
\vdots \\
\frac{df}{dx_n}
\end{bmatrix} = \begin{bmatrix}
\beta_1 \\
\vdots \\
\beta_n
\end{bmatrix} = \boldsymbol{\beta}\]</span></p>
<p><span class="math inline">\(\frac{d}{d\mathbf{x}}(\mathbf{x}^\top A \mathbf{x}) = (\mathbf{A} + \mathbf{A}^\top)\mathbf{x}.\)</span></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/11/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/11/">11</a><span class="page-number current">12</span><a class="page-number" href="/page/13/">13</a><span class="space">&hellip;</span><a class="page-number" href="/page/17/">17</a><a class="extend next" rel="next" href="/page/13/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="EIMadrigal"
      src="/images/favicon.png">
  <p class="site-author-name" itemprop="name">EIMadrigal</p>
  <div class="site-description" itemprop="description">Hello World</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">167</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/EIMadrigal" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;EIMadrigal" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:andrew.renj@gmail.com" title="E-Mail → mailto:andrew.renj@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.cnblogs.com/EIMadrigal" title="cnblogs → https:&#x2F;&#x2F;www.cnblogs.com&#x2F;EIMadrigal" rel="noopener" target="_blank"><i class="fab fa-codiepie fa-fw"></i>cnblogs</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/EIMadrigal" title="Weibo → https:&#x2F;&#x2F;weibo.com&#x2F;EIMadrigal" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i>Weibo</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2018-02 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">EIMadrigal</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>


    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <span id="busuanzi_container_site_pv">Total views: <span id="busuanzi_value_site_pv"></span></span>
    <span class="post-meta-divider">|</span>
    <span id="busuanzi_container_site_uv">Total visitors: <span id="busuanzi_value_site_uv"></span></span>
    <span class="post-meta-divider">|</span>

<script>
$(document).ready(function() {

    var int = setInterval(fixCount, 50);
    var countOffset = 20000;

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset);
            clearInterval(int);
        }  
    }
       	
});
</script> 

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


</body>
</html>
