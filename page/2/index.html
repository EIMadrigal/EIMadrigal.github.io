<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"eimadrigal.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Hello World">
<meta property="og:type" content="website">
<meta property="og:title" content="EI Madrigal&#39;s Space">
<meta property="og:url" content="https://eimadrigal.github.io/page/2/index.html">
<meta property="og:site_name" content="EI Madrigal&#39;s Space">
<meta property="og:description" content="Hello World">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="EIMadrigal">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://eimadrigal.github.io/page/2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>EI Madrigal's Space</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">EI Madrigal's Space</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://eimadrigal.github.io/2021/09/24/%E6%97%A5%E5%B8%B8%E7%A2%8E%E7%A2%8E%E5%BF%B5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/favicon.png">
      <meta itemprop="name" content="EIMadrigal">
      <meta itemprop="description" content="Hello World">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EI Madrigal's Space">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/09/24/%E6%97%A5%E5%B8%B8%E7%A2%8E%E7%A2%8E%E5%BF%B5/" class="post-title-link" itemprop="url">日常碎碎念</a>
        </h2>

        <div class="post-meta">

		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-09-24 14:10:00" itemprop="dateCreated datePublished" datetime="2021-09-24T14:10:00+08:00">2021-09-24</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Life/" itemprop="url" rel="index"><span itemprop="name">Life</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>write thoughts down</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/09/24/%E6%97%A5%E5%B8%B8%E7%A2%8E%E7%A2%8E%E5%BF%B5/">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://eimadrigal.github.io/2021/09/11/%E4%BB%80%E4%B9%88%E6%98%AF%E6%B1%9F%E6%B9%96/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/favicon.png">
      <meta itemprop="name" content="EIMadrigal">
      <meta itemprop="description" content="Hello World">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EI Madrigal's Space">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/09/11/%E4%BB%80%E4%B9%88%E6%98%AF%E6%B1%9F%E6%B9%96/" class="post-title-link" itemprop="url">什么是江湖</a>
        </h2>

        <div class="post-meta">

		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-09-11 15:24:00" itemprop="dateCreated datePublished" datetime="2021-09-11T15:24:00+08:00">2021-09-11</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Life/" itemprop="url" rel="index"><span itemprop="name">Life</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>碎碎念</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/09/11/%E4%BB%80%E4%B9%88%E6%98%AF%E6%B1%9F%E6%B9%96/">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://eimadrigal.github.io/2021/08/30/How%20to%20Make%20Plans/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/favicon.png">
      <meta itemprop="name" content="EIMadrigal">
      <meta itemprop="description" content="Hello World">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EI Madrigal's Space">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/08/30/How%20to%20Make%20Plans/" class="post-title-link" itemprop="url">How to Make Plans</a>
        </h2>

        <div class="post-meta">

		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-08-30 15:09:00" itemprop="dateCreated datePublished" datetime="2021-08-30T15:09:00+08:00">2021-08-30</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>如何平衡好科研、自学、生活？这是一个非常重要的问题，关乎未来发展和生活质量，并且随着年龄增长这个问题会变得更加严重，因为我们将要面临的世界会更加纷繁复杂。</p>
<p>要处理好这个问题并不容易，甚至有些玄学、有些艺术。我个人觉得解决这个问题的关键在于计划：制定合理的、可行的、有侧重性的、不那么死板、也不那么松散随意的计划，这其中包括长期计划、中期计划和短期计划，并且还要学会巧妙利用时间。</p>
<p>我自己尝试过各种各样的计划，包括但不限于按照时间段严格限定任务、按照任务进展松散型、TODO List等，基本都是坚持不到2礼拜就废弃了，而且也没有贯彻长期和中期计划。 <img src="https://img2020.cnblogs.com/blog/1260581/202112/1260581-20211223092219887-1893756805.png" alt="image" /></p>
<p>如果纯粹按照follow your heart去排序，那么毫无疑问是生活&gt;自学&gt;科研，但是问题在于需要保证科研的最低限度（实习及毕业），所以计划应该怎么定呢？</p>
<ol type="1">
<li>长期计划（2021.7~2025.7）：毕业工作两年。</li>
<li>中期计划（2021.7~2023.7）：</li>
<li>短期计划（每月每周）：</li>
</ol>
<p>核心问题在于：计划的目的并不在于100%完成，这听起来可能有些荒谬，计划的真正目的在于使你有一个为之努力的东西，全身心地投入到这个过程中，自然而然最后的结果也不会太差劲。所以TODO List中的事项没有全部划掉并不可怕，完成70%~80%足矣。剩下未完成的部分视重要程度作为下一阶段的首当其冲的事项。</p>
<p>达成目标本身并不会提高幸福水平的baseline，只是让自己future-oriented</p>
<h2 id="from-2021.11.23-to-2022.12.31">From 2021.11.23 To 2022.12.31</h2>
<p><strong>这段时间非常非常关键，会决定第一份工作的去向！！！绝对不能每天每个小时浑浑噩噩</strong></p>
<p>本计划包括阶段性目标和短期计划两部分，从后端开发（系统编程/云计算）的岗位需求出发设计目标：</p>
<p>阶段性目标主要是在什么时间节点完成什么事情，短期计划会具体到每天甚至每个小时的安排。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://eimadrigal.github.io/2021/07/28/Inspiring%20Quotes/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/favicon.png">
      <meta itemprop="name" content="EIMadrigal">
      <meta itemprop="description" content="Hello World">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EI Madrigal's Space">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/07/28/Inspiring%20Quotes/" class="post-title-link" itemprop="url">Inspiring Quotes</a>
        </h2>

        <div class="post-meta">

		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-07-28 02:57:00" itemprop="dateCreated datePublished" datetime="2021-07-28T02:57:00+08:00">2021-07-28</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <ol type="1">
<li>还差得远呢！ -ddd 深夜看到ddd对2020年华为软件比赛的精准<a target="_blank" rel="noopener" href="https://github.com/justarandomstring/2020-Huawei-Code-Craft">分析</a>，深感自己平日实在太过轻松。</li>
<li>天赋游戏，遥不可及。 一位计算机同学的个性签名，深感自己早已不可能成为最顶尖的那批人，借此来宽慰遇到麻烦时的窘境。</li>
<li>所有的优越感都是因为没有见识。</li>
<li>深夜里碰杯，都是梦破碎的声音。</li>
<li>I'm tired of all those boring comparisons.</li>
<li>Missing your Vanilla Sky.</li>
<li>消失在人海。</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://eimadrigal.github.io/2021/07/21/CS231n%20Assignment/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/favicon.png">
      <meta itemprop="name" content="EIMadrigal">
      <meta itemprop="description" content="Hello World">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EI Madrigal's Space">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/07/21/CS231n%20Assignment/" class="post-title-link" itemprop="url">CS231n Assignment</a>
        </h2>

        <div class="post-meta">

		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-07-21 05:29:00" itemprop="dateCreated datePublished" datetime="2021-07-21T05:29:00+08:00">2021-07-21</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>先吹一波Google Colab，所有操作都可在云上进行，还能白嫖🐕家的GPU；再吹一下Stanford的骨架代码，真的是干净整洁优美，堪称典范。 <a target="_blank" rel="noopener" href="https://github.com/EIMadrigal/CS231n">My Code</a> ## kNN 最幼稚的机器学习算法。</p>
<ol type="1">
<li>计算测试集和训练集的距离 训练集<code>X_train</code>的shape为<span class="math inline">\((N, D)\)</span>，<code>y_train</code>的shape为<span class="math inline">\((N,)\)</span>，<code>y[i]</code>取值范围<span class="math inline">\([0,C-1]\)</span> 测试集<code>X_test</code>的shape为<span class="math inline">\((M, D)\)</span>，最终的distance matrix的shape为<span class="math inline">\((M,N)\)</span> 声明：不能使用类似于<code>np.linalg.norm()</code>这种东西作弊。 首先来看看2重循环：第一重遍历测试集，第二重遍历训练集，当然如果你愿意，还可以用第三重遍历dimension去累加距离； 再来看看只遍历测试集的单层循环：对于每个测试样例<code>X[i]</code>，减去<code>X_train</code>，通过广播机制得到一个<span class="math inline">\((N, D)\)</span>的差矩阵，做element-wise的平方，按列相加得到<span class="math inline">\((N,)\)</span>，表示测试样例<code>X[i]</code>与每个训练样例的距离，作为距离矩阵的第<span class="math inline">\(i\)</span>行； 最后来看看full-vectorized的版本，数学推导见<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/146076139">NumPy之计算两个矩阵的成对平方欧氏距离</a>，吃饱没事干的同学可以自己推推，我数学太差就溜了。</li>
<li>根据距离矩阵预测测试集的标签 对于每个测试样例<code>X[i]</code>，选k个距离最小的训练样例，将其label（从<code>y_train</code>获得）存入<code>cloest_y</code>中，投票决定最终的预测标签。 先用<code>idx=np.argsort(dists[i])[:k]</code>取出前k个训练样例的index，再用<code>y_train[idx]</code>得到对应的k个label，最后用<code>np.argmax(np.bincount(cloest_y))</code>得到最终的预测label。</li>
</ol>
<p>kNN效果当然比较拉垮了，在CIFAR-10的子集上分类正确率大概在27%左右。比较令我震惊的是三个计算距离函数耗费的时间，2重循环57s，单层循环41s，fully-vectorized只有0.57s，竟然<strong>降低了100倍</strong>，写出高效的代码对于程序性能有着至关重要的影响，反思下自己写出的junk code，不由得留下了伤心的泪水...</p>
<p>最后就是用cross-validation确定超参k的取值，就略过了哈。 ## Linear Multiclass SVM 首先要为多分类SVM写一个损失函数，老规矩还是先写一个naive版本<code>svm_loss_naive(W, X, y, reg)</code>： 权重矩阵W：<span class="math inline">\((D, C)\)</span> minibatch输入X：<span class="math inline">\((N, D)\)</span> 标签y：<span class="math inline">\((N,)\)</span>，<code>y[i]=c</code>表示<code>X[i]</code>的标签是c，<span class="math inline">\(0 \leq c&lt;c\)</span> <span class="math display">\[l=&quot;\frac{1}{N}&quot; \]</span>l_i="_{j" +="" -="" )="" ),s_j="f(x_i," ="" ="" _i="" <em>k<em>l="" </em>{j="" f(x_i;="" loss为：="" loss和正则项两部分，对于第<span class="math inline">\(i\)</span>个训练样本，data="" loss是这么定义的：="" machine="" multiclass="" s_j="" s</em>{y_i}="" support="" vector="" w)_j="" w)_j<span class="math display">\[=&quot;&quot; w)_{y_i}=&quot;&quot; w_{k,l}^2\]</span>="" y_i}="" 什么意思呢？<span class="math inline">\(s\)</span>是第<span class="math inline">\(i\)</span>个训练样本的得分向量<span class="math inline">\((c,)\)</span>，<span class="math inline">\(s_{y_i}\)</span>表示正确标签的得分，<span class="math inline">\(s_j\)</span>表示其他类的得分。不妨看看什么时候损失为0呢？稍作变形即有：当<span class="math inline">\(s_{y_i}-s_j=&quot;&quot; 看着有点复杂哦！主要有data=&quot;&quot; 返回浮点数`loss`和解析梯度`dw`=&quot;&quot;&gt;\Delta\)</span>时，第<span class="math inline">\(j\)</span>类损失为0，说人话就是只有当正确类的得分减去其他类的得分大于某个间隔<span class="math inline">\(\Delta\)</span>时才不会累积损失，否则就累加损失（必然为正数），这就是大名鼎鼎的<strong>Hinge Loss</strong>。</p>
<p>如果<span class="math inline">\(f\)</span>用的是linear score function，进一步有： <span class="math display">\[L_i = \sum_{j\neq y_i} \max(0, w_j^T x_i - w_{y_i}^T x_i + \Delta)\]</span> 其中，<span class="math inline">\(w_j\)</span>表示W的第<span class="math inline">\(j\)</span>列。 至此，naive版本的<code>loss</code>实现就不必废话了。接着来求<code>dW</code>，老规矩，还是先研究单个样本。</p>
<p>如果你的数学还行，下面的梯度推导可以略过： <span class="math display">\[L_i = max(0,w_1^T x_i - w_{y_i}^T x_i + \Delta)+max(0,w_2^T x_i - w_{y_i}^T x_i + \Delta)+...+max(0,w_C^T x_i - w_{y_i}^T x_i + \Delta)\]</span> 共有<span class="math inline">\(C-1\)</span>项，因为<span class="math inline">\(j=y_i\)</span>那项不算。另，只有在<span class="math inline">\(w_j^T x_i - w_{y_i}^T x_i + \Delta&gt;0\)</span>时第<span class="math inline">\(j\)</span>项的梯度不为0。</p>
<ol type="1">
<li>对<span class="math inline">\(w_{y_i}\)</span>的梯度 每项都有，并且都是0或<span class="math inline">\(-x_i\)</span>，因此只要看几项大于0，梯度就是几倍的<span class="math inline">\(-x_i\)</span>，正式点就是： <span class="math display">\[\nabla_{w_{y_i}} L_i = - \left( \sum_{j\neq y_i} \mathbb{1}(w_j^Tx_i - w_{y_i}^Tx_i + \Delta &gt; 0) \right) x_i\]</span></li>
<li>对<span class="math inline">\(w_j\)</span>的梯度 只有第<span class="math inline">\(j\)</span>项有，0或<span class="math inline">\(x_i\)</span>，正式点就是： <span class="math display">\[\nabla_{w_j} L_i = \mathbb{1}(w_j^Tx_i - w_{y_i}^Tx_i + \Delta &gt; 0) x_i\]</span></li>
</ol>
<p>naive版本的<code>dW[:,j]</code>和<code>dW[:,y[i]]</code>就2重循环按部就班更新即可，别忘了除以<span class="math inline">\(N\)</span>和正则项梯度。</p>
<p>接着来实现<code>svm_loss_vectorized(W, X, y, reg)</code>：</p>
<ol type="1">
<li>loss 首先求得整个训练集的得分矩阵<code>scores</code>，shape为<span class="math inline">\((N,C)\)</span>，每一行表示一个样例的得分。正确类得分向量<code>correct_class_score</code>可用<code>scores[np.arange(num_train), y]</code>得到，shape为<span class="math inline">\((N,)\)</span>，注意这里不能用<code>scores[:, y]</code>，简单试验下：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]])</span><br><span class="line">y = np.array([<span class="number">2</span>, <span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(X[np.arange(<span class="number">2</span>), y])  <span class="comment"># [3,4]</span></span><br><span class="line"><span class="built_in">print</span>(X[:, y])  <span class="comment"># [[3,2],[5,4]]</span></span><br></pre></td></tr></table></figure>
<p>下来到了最关键的<code>margins</code>矩阵，该矩阵和<code>scores</code>矩阵shape相同<span class="math inline">\((N,C)\)</span>，第<span class="math inline">\(i\)</span>行表示第<span class="math inline">\(i\)</span>个训练样本的margin即<span class="math inline">\(max(0,s_j - s_{y_i} + \Delta)\)</span>，在每一行第<span class="math inline">\(y_i\)</span>个位置上应当设置为0，其余位置按照公式即可：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">margins = np.maximum(<span class="number">0</span>, scores - correct_class_score[:, np.newaxis] + <span class="number">1</span>)</span><br><span class="line">margins[np.arange(num_train), y] = <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>需要注意：<code>correct_class_score</code>是一个<span class="math inline">\((N,)\)</span>的向量，如果直接<code>scores-correct_class_score</code>就会报错，广播机制从最后一个维度开始比对，只有相等或者其中某个为1才行，因此用<code>np.newaxis</code>将<code>correct_class_score</code>的shape变为<span class="math inline">\((N,1)\)</span>；还有就是<code>np.max()</code>和<code>np.maximum()</code>的区别，<code>np.max()</code>和<code>np.amax(a, axis=None, ...)</code>等价，返回数组的最大值，<code>np.maximum(x1, x2, out=None, ...)</code>返回element-wise的较大值。</p>
<ol start="2" type="1">
<li>梯度 这里也稍微有点tricky，根据naive版本对梯度的讨论：对<span class="math inline">\(w_j\)</span>的梯度需要知道margin的正负，对<span class="math inline">\(w_{y_i}\)</span>的梯度需要知道<strong>有几项大于0</strong>。怎么借助<code>margins</code>矩阵统计每一行大于0的项数呢？无聊的程序员先将矩阵中大于0的项都设为1，然后按列相加即可：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">margins[margins &gt; <span class="number">0</span>] = <span class="number">1.0</span></span><br><span class="line">num_to_loss = np.<span class="built_in">sum</span>(margins, axis=<span class="number">1</span>)  <span class="comment"># (N,)</span></span><br><span class="line">margins[np.arange(num_train, y)] = -num_to_loss</span><br></pre></td></tr></table></figure>
<p>对单个样本<span class="math inline">\(i\)</span>来说，其对<code>dW</code>的贡献要么是在第<span class="math inline">\(j\)</span>列（即第<span class="math inline">\(j\)</span>个类）加上<span class="math inline">\(x_i\)</span>，要么在第<span class="math inline">\(y_i\)</span>列加上<span class="math inline">\(-kx_i\)</span>，<span class="math inline">\(k\)</span>为<code>margins[i]</code>中大于0的元素个数，即<code>num_to_loss[i]</code>，整个训练集对<code>dW</code>的更新即是在累加单个样本对<code>dW</code>每一列（每个类）的影响。对第<span class="math inline">\(j\)</span>列，其更新即为每个训练样本对该类贡献的线性组合，组合系数取决于该样本的标签以及是否满足指示函数，即为<code>margins</code>的第<span class="math inline">\(j\)</span>列，取值范围<span class="math inline">\(\{0,1,-k\}\)</span>，0表示该样本对第<span class="math inline">\(j\)</span>个类的梯度没有贡献（该样本标签不是<span class="math inline">\(j\)</span>且不满足指示函数），1表示贡献了<span class="math inline">\(x_i\)</span>（该样本标签不是<span class="math inline">\(j\)</span>且满足指示函数），<span class="math inline">\(-k\)</span>表示贡献了<span class="math inline">\(-kx_i\)</span>（该样本的标签就是<span class="math inline">\(j\)</span>），因此：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dW = np.dot(X.T, margins) / num_train + <span class="number">2</span> * reg * W</span><br></pre></td></tr></table></figure>
<p>可以从矩阵维度相容的角度验证。 ## Softmax 先用循环实现一个<code>softmax_loss_naive(W, X, y, reg)</code>，输入的shape和SVM相同。 softmax分类器不再将<span class="math inline">\(f(x_i;W)\)</span>看做每个类的得分，而是希望输出normalized class probabilities，最终选一个概率最大的类作为预测，<strong>softmax函数</strong>就能将<span class="math inline">\(f(x_i;W)\)</span>映射到<span class="math inline">\([0,1]\)</span>且满足概率的性质： <span class="math display">\[P(y_i \mid x_i; W) = \frac{e^{f_{y_i}}}{\sum_j e^{f_j} }\]</span> 从预测函数可以看到：softmax是把<span class="math inline">\(f(x_i;W)\)</span>看作unnormalized log probabilities，因此对<span class="math inline">\(f(x_i;W)\)</span>先指数再归一化得到每个类的概率。</p>
<p>再来看softmax的损失函数： <span class="math display">\[L_i = -\log\left(\frac{e^{f_{y_i}}}{ \sum_j e^{f_j} }\right) \hspace{0.1in} \text{or equivalently} \hspace{0.1in} L_i = -f_{y_i} + \log\sum_j e^{f_j}\]</span> 从直觉上说：属于正确类<span class="math inline">\(y_i\)</span>的概率（括号里的分式）越高，损失应该越小，这就是大名鼎鼎的<strong>cross-entropy loss</strong>，衡量了真实分布<span class="math inline">\(p\)</span>和预测分布<span class="math inline">\(q\)</span>之间的差距： <span class="math display">\[H(p,q) = - \sum_x p(x) \log q(x)= H(p) + D_{KL}(p||q)\]</span> 具体到softmax： <span class="math display">\[q=\frac{e^{f_{y_i}}}{ \sum_j e^{f_j} },p = [0, \ldots 1, \ldots, 0]\]</span> 其中，<span class="math inline">\(p\)</span>在第<span class="math inline">\(y_i\)</span>个位置上为1。 由于<span class="math inline">\(H(p)=0\)</span>，因此其实是在最小化<span class="math inline">\(p\)</span>和<span class="math inline">\(q\)</span>的KL散度，即希望预测结果<span class="math inline">\(q\)</span>尽量向<span class="math inline">\(p\)</span>靠近。</p>
<p>从概率的角度出发看损失函数，我们是在最小化正确类<span class="math inline">\(y_i\)</span>的负对数似然，本质上就是在做一个极大似然估计。</p>
<p>看完理论，还要考虑一些现实问题。比如数值稳定性，由于指数的原因可能会导致overflow或者underflow，因此做一个简单的等价变换： <span class="math display">\[\frac{e^{f_{y_i}}}{\sum_j e^{f_j}}
= \frac{Ce^{f_{y_i}}}{C\sum_j e^{f_j}}
= \frac{e^{f_{y_i} + \log C}}{\sum_j e^{f_j + \log C}}\]</span> 一般选<span class="math inline">\(\log C = -\max_j f_j\)</span>，这个变换不会改变预测函数或者损失函数，只是将得分做了平移。</p>
<p>至此，naive版本的loss就基本有了，接着看看梯度咋求。先稍稍展开康康： <span class="math display">\[L_i=-f_{y_i} + \log\sum_j e^{f_j}=-w_{y_i}^Tx_i+log\sum_je^{w_j^Tx_i}\]</span> 其中，<span class="math inline">\(w_j\)</span>表示W的第<span class="math inline">\(j\)</span>列。 然后使用我们的小学数学知识去求偏导： <span class="math display">\[\nabla_{w_{y_i}} L_i =(\frac{e^{f_{y_i}}}{ \sum_j e^{f_j}}-1)x_i \\
\nabla_{w_j} L_i = \frac{e^{f_{j}}}{ \sum_j e^{f_j}}x_i\]</span> 记<code>p = np.exp(scores) / np.sum(np.exp(scores))</code>，shape为<span class="math inline">\((C,)\)</span>，表示样本<span class="math inline">\(i\)</span>属于每个类的概率。 所以<code>dW</code>的第<code>y[i]</code>列更新即为<code>(p[y[i]] - 1) * X[i]</code>，其他列更新为<code>p[j] * X[i]</code>。</p>
<p>接着看下vectorized版本，<code>scores</code>的shape变为了<span class="math inline">\((N,C)\)</span>，首先处理数值稳定性问题，每一行都减去该行的最大值（注意<code>keepdim=True</code>）；接着求出概率矩阵<code>p</code>，shape与<code>scores</code>相同，那么loss为： <code>loss = np.sum(-np.log(p[np.arange(X.shape[0]), y])) / X.shape[0]</code> 与SVM类似，<code>dW</code>的每一列（每个类）是由每个训练样本影响的线性组合决定的，组合系数取决于该训练样例的标签，比如对于<code>dW</code>的第<span class="math inline">\(j\)</span>个类来说，如果某个样例的标签恰好是<span class="math inline">\(j\)</span>，那么其对梯度的贡献就是<code>p[j]-1</code>，否则系数就是<code>p[j]</code>。因此只要将概率矩阵<code>p</code>中所有正确标签的值减1即得到系数矩阵，进而得到<code>dW</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">p[np.arange(X.shape[<span class="number">0</span>]), y] = p[np.arange(X.shape[<span class="number">0</span>]), y] - <span class="number">1</span></span><br><span class="line">dW = np.dot(X.T, p)</span><br></pre></td></tr></table></figure>
<p>同样可以用维度相容去check。 ## Neural Network 这是一个两层的全连接神经网络，架构如下： 输入<span class="math inline">\((N,D)\)</span>-&gt;全连接层1-&gt;ReLU-&gt;全连接层2（输出每个类的得分）-&gt;softmax 参数们的shape为：<span class="math inline">\(X(N,D),W1(D,H),b1(H,),W2(H,C),b2(C,)\)</span></p>
<p>第一步Forward Pass，根据输入X和权值W计算<span class="math inline">\(scores(N,C)\)</span>，然后计算softmax loss； 第二步Backward Pass，需要计算loss对于参数们的梯度，根据网络结构： <span class="math display">\[h=XW1+b1\\
o=ReLU(h)\\
s=oW2+b2\\
L=\sum_i(-s_{y_i}+log\sum_j e^{s_j})\]</span> 根据链式法则+维度相容： <span class="math display">\[\nabla_{w_2} L =o^T \nabla_{s} L\\
\nabla_{b_2} L =(\nabla_{s} L)^T(\nabla_{b_2} s)=(C,N)(N,1)=(C,N)(all\ 1\ col)\\
\nabla_{w_1} L =X^T \nabla_{s} L\nabla_{h} s\\
\nabla_{b_1} L =\nabla_{h} s (\nabla_{s} L)^T\nabla_{b_1} h=(H,C)(C,N)(N,1)=(H,C)(C,N)(all\ 1\ col)\]</span> 可以看出：关键在于求出<span class="math inline">\(\nabla_{s} L\)</span>，在对softmax的讨论中可知，对于第<span class="math inline">\(y_i\)</span>列导数为<code>p[y[i]]-1</code>，对其他列为<code>p[j]</code>，因此该偏导为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">d2 = p</span><br><span class="line">d2[np.arange(X.shape[<span class="number">0</span>]), y] -= <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>另外对于<span class="math inline">\(W_1,b_1\)</span>，还需要<span class="math inline">\(\nabla_{h} s\)</span>：这玩意在<span class="math inline">\(h&gt;0\)</span>就是<span class="math inline">\(W_2^T\)</span>，否则就是0。因此<span class="math inline">\(\nabla_{s} L\nabla_{h} s\)</span>可以写为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">d1 = d2.dot(W2.T) * (h &gt; <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h2 id="image-features">Image Features</h2>
<p>之前的样例都是直接用raw pixel，加上都是线性模型，效果拉跨太正常了。这里用的人工feature包括HOG(Histogram of Oriented Gradients)和color histogram，HOG捕捉texture（纹理变化？）信息，color histogram捕捉颜色信息，两者互相辅助。&lt;/c$&gt;</p>
<h2 id="fully-connected-neural-network">Fully-connected Neural Network</h2>
<p>我好菜啊！！很早就写完代码了，gradient check也过了，但是需要过拟合50张图片的时候一直不太对，调了几下学习率，我看train acc只有0.14左右，loss曲线波动也很大： <img src="https://img2020.cnblogs.com/blog/1260581/202109/1260581-20210912161403754-1113281186.png" alt="image" /> 因为最后要100%的train acc嘛，我看差的挺远的，就开始怀疑是网络哪里写错了，就没管超参数，检查代码检查了好几天tmd，深度学习debug还真是无从下手...后来跑去看了下别人的东西，发现原因竟然是不会调参(T^T)。</p>
<p>仔细看下，这里还是很明显的，20个epoch训练损失才下降了一点点，说明学习率太小了。 <img src="https://img2020.cnblogs.com/blog/1260581/202109/1260581-20210912162656860-43278840.png" alt="image" /></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://eimadrigal.github.io/2021/07/17/%E5%90%8C%E8%BE%88%E5%8E%8B%E5%8A%9B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/favicon.png">
      <meta itemprop="name" content="EIMadrigal">
      <meta itemprop="description" content="Hello World">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EI Madrigal's Space">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/07/17/%E5%90%8C%E8%BE%88%E5%8E%8B%E5%8A%9B/" class="post-title-link" itemprop="url">同辈压力</a>
        </h2>

        <div class="post-meta">

		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-07-17 14:50:00" itemprop="dateCreated datePublished" datetime="2021-07-17T14:50:00+08:00">2021-07-17</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Life/" itemprop="url" rel="index"><span itemprop="name">Life</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Peer Pressure</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/07/17/%E5%90%8C%E8%BE%88%E5%8E%8B%E5%8A%9B/">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://eimadrigal.github.io/2021/07/16/LeetCode%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/favicon.png">
      <meta itemprop="name" content="EIMadrigal">
      <meta itemprop="description" content="Hello World">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EI Madrigal's Space">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/07/16/LeetCode%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A/" class="post-title-link" itemprop="url">LeetCode解题报告</a>
        </h2>

        <div class="post-meta">

		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-07-16 15:21:00" itemprop="dateCreated datePublished" datetime="2021-07-16T15:21:00+08:00">2021-07-16</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Computer-Science/" itemprop="url" rel="index"><span itemprop="name">Computer Science</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>部分题目分析</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/07/16/LeetCode%E8%A7%A3%E9%A2%98%E6%8A%A5%E5%91%8A/">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://eimadrigal.github.io/2021/07/15/Research%20Proposal/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/favicon.png">
      <meta itemprop="name" content="EIMadrigal">
      <meta itemprop="description" content="Hello World">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EI Madrigal's Space">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/07/15/Research%20Proposal/" class="post-title-link" itemprop="url">Research Proposal</a>
        </h2>

        <div class="post-meta">

		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-07-15 02:35:00" itemprop="dateCreated datePublished" datetime="2021-07-15T02:35:00+08:00">2021-07-15</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Computer-Science/" itemprop="url" rel="index"><span itemprop="name">Computer Science</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>科研垃圾</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/07/15/Research%20Proposal/">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://eimadrigal.github.io/2021/07/14/%E5%85%B3%E4%BA%8E%E8%81%8C%E4%B8%9A%E5%8F%91%E5%B1%95%E7%9A%84%E8%80%83%E8%99%91/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/favicon.png">
      <meta itemprop="name" content="EIMadrigal">
      <meta itemprop="description" content="Hello World">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EI Madrigal's Space">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/07/14/%E5%85%B3%E4%BA%8E%E8%81%8C%E4%B8%9A%E5%8F%91%E5%B1%95%E7%9A%84%E8%80%83%E8%99%91/" class="post-title-link" itemprop="url">关于职业发展的考虑</a>
        </h2>

        <div class="post-meta">

		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-07-14 11:22:00" itemprop="dateCreated datePublished" datetime="2021-07-14T11:22:00+08:00">2021-07-14</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Life/" itemprop="url" rel="index"><span itemprop="name">Life</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>磨刀不误砍柴工</p>
          <!--noindex-->
            <div class="post-button">
              <a class="btn" href="/2021/07/14/%E5%85%B3%E4%BA%8E%E8%81%8C%E4%B8%9A%E5%8F%91%E5%B1%95%E7%9A%84%E8%80%83%E8%99%91/">
                Read more &raquo;
              </a>
            </div>
          <!--/noindex-->
        
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://eimadrigal.github.io/2021/07/10/Optimization%20Methods%20in%20Deep%20Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/favicon.png">
      <meta itemprop="name" content="EIMadrigal">
      <meta itemprop="description" content="Hello World">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="EI Madrigal's Space">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/07/10/Optimization%20Methods%20in%20Deep%20Learning/" class="post-title-link" itemprop="url">Optimization Methods in Deep Learning</a>
        </h2>

        <div class="post-meta">

		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-07-10 11:47:00" itemprop="dateCreated datePublished" datetime="2021-07-10T11:47:00+08:00">2021-07-10</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="background">Background</h2>
<p>深度神经网络的训练过程主要通过求解一个特定的优化问题来实现，然而由于该问题是一个复杂的高维非线性优化问题，并且不同的网络结构差异很大，不能将传统的优化方法直接使用。即使数据集和网络结构完全相同，不同的优化算法也可能导致完全不同的收敛效果。实际应用的一些简单方法虽然行之有效，但现有理论无法充分解释其有效性，超参数的不断增加也给优化增加了不少难度。如何确保算法收敛、如何尽快收敛以及能否收敛到全局最优一直是困扰学术界和工业界的问题。如果能够用优化理论去解释神经网络的训练行为，对于深度学习的推广应用将会起到巨大的推动作用。</p>
<p>对于有监督学习，给定包含n个样本的训练集<span class="math inline">\(\{(\mathbf{x}_1, y_1), \ldots, (\mathbf{x}_n, y_n)\}\)</span>，<span class="math inline">\(\mathbf{x}\)</span>表示样本的特征向量，<span class="math inline">\(y\)</span>表示该样本对应的标签。我们的任务是利用样本信息来预测相应的标签，使预测值尽可能接近真实标签。如果用深度学习来完成这个任务，就需要通过调整神经网络的参数（权重W和偏差b）来近似数据背后的函数映射关系，这个关系往往是高度非线性的，网络越深表达能力也就越强，逼近效果的精度也就更高，因此网络结构很可能是极其复杂的。</p>
<p>为了衡量预测值和真实值之间的接近程度，通常需要采用某种距离度量方式<span class="math inline">\(l\)</span>，<span class="math inline">\(l\)</span>一般设计为<strong>可微</strong>的，接着用一些优化算法去最小化该目标函数。因此优化问题变为寻找最佳的参数使得<span class="math inline">\(l\)</span>最小，在不考虑正则项的情况下有： <span class="math display">\[
\mathop{\mathrm{min}}_f \frac{1}{n} \sum_{i=1}^n l(f(\mathbf{x}_i), y_i)
\]</span> <span class="math inline">\(f\)</span>就是我们从输入到输出的映射函数，<span class="math inline">\(l\)</span>通常也叫损失函数，衡量预测值<span class="math inline">\(f(x_i)\)</span>和真实标签<span class="math inline">\(y_i\)</span>的差距，比如回归问题中经常使用的平方损失函数<span class="math inline">\(l=||f(x_i)-y_i||^2\)</span>。</p>
<p>需要注意的是：深度学习中的优化问题与传统意义上的优化问题有所差别。传统的优化问题需要尽可能找到目标函数的最值，而深度学习的最终目的是为了<strong>预测未知</strong>的数据，而不是将训练数据上的损失降到最低。我们定义的损失函数<span class="math inline">\(J(\Theta)\)</span>衡量的是当前模型参数<span class="math inline">\(\Theta\)</span>在<strong>训练集</strong>上的优劣，然而，最小化训练误差并不意味着模型的泛化误差也会最小，为了降低泛化误差我们还需要关注过拟合问题，因此损失函数往往要加上<strong>正则项</strong>。统计学上称为经验风险最小化，即由于无法获得全部数据，所以只能用经验风险作为实际风险的近似。非常有意思的是：尽管大多数神经网络都是严重过参数化的，但是反而有着比较不错的泛化能力，这与传统的机器学习观点是矛盾的，泛化理论也需要更加深入的研究。</p>
<p>深度学习中的<span class="math inline">\(f\)</span>通常是多层的复合函数，由于太复杂而无法求出解析解，所以要用数值优化算法去求解。实际中主流的深度学习优化算法都利用梯度下降来求解，梯度下降是深度学习优化算法的基础，尽管目前已经很少直接使用，但它却是其他高级优化算法的基石： 假设网络的参数为<span class="math inline">\(x=(x_1,x_2,...,x_d)^T\)</span>，优化的目标函数为<span class="math inline">\(f\)</span>，那么<span class="math inline">\(f\)</span>的梯度为： <span class="math display">\[
\nabla f(\mathbf{x}) = \bigg[\frac{\partial f(\mathbf{x})}{\partial x_1}, \frac{\partial f(\mathbf{x})}{\partial x_2}, \ldots, \frac{\partial f(\mathbf{x})}{\partial x_d}\bigg]^\top
\]</span> 每个元素对应着目标函数在该方向上的变化率，因此只要沿着梯度的反方向就可以使目标函数减小得最快：<span class="math inline">\(\mathbf{x} \leftarrow \mathbf{x} - \eta \nabla f(\mathbf{x})\)</span>，<span class="math inline">\(\eta\)</span>是一个被称为学习率的超参数，用来控制每一步的大小。<span class="math inline">\(\eta\)</span>过小，收敛过程极度缓慢；<span class="math inline">\(\eta\)</span>过大，可能造成损失函数在最小点附近波动甚至发散。学习率的调整是神经网络训练过程中一个重要的调整参数，常常使人头痛不已，因此也出现了很多学习率自适应调整的算法，将在下面深入分析这些算法的优劣。</p>
<p>有了优化模型及最基础的求解方法后，我们需要对其性质和优缺点进行分析，以便于后续的改进。深度学习的优化问题大多是非凸的，因此存在很多挑战：</p>
<ol type="1">
<li>局部最优：对于凸优化问题，局部最优即是全局最优。然而对于非凸问题，当损失函数到达局部最优点时，<span class="math inline">\(J(\Theta)\)</span>的梯度为0，<span class="math inline">\(\Theta\)</span>无法继续更新，损失函数无法继续下降；</li>
<li>鞍点：该点既不是局部最小也不是全局最小，但是该点的梯度消失，无法继续更新；</li>
<li>梯度消失/爆炸：由于初始值和激活函数选择不当 (如sigmoid)，当梯度反向回传时，可能在某一层求导后梯度值很小/很大，导致训练速度极其缓慢。因此初始值的选择通常采用很小的随机数，避免收敛到比较差的区域，激活函数通常也会选择ReLU，避免梯度消失问题。</li>
</ol>
<p>局部最小和鞍点示意图如下： <img src="https://img-blog.csdnimg.cn/20210710191530881.png" alt="在这里插入图片描述" /> 尤其在高维空间中，鞍点的问题变得更加严重：假设<span class="math inline">\(\Theta\)</span>是一个k维向量，<span class="math inline">\(J(\Theta)\)</span>的海森矩阵就有k个特征值，其梯度为0的点有可能是局部最小（特征值均为正）、局部最大（特征值均为负）或者是鞍点（特征值有正有负）。高维空间中特征值有正有负的概率很大，因此鞍点出现的可能性远大于局部最优点出现的可能性，并且鞍点周围的平坦区域可能很大，需要增加噪声扰动来逃离鞍点。</p>
<p>由于上述问题的存在，通常很难找到<span class="math inline">\(J(\Theta)\)</span>的全局最优解，但实际上为了减少过拟合的风险我们并不需要训练集上的全局最优，经典的梯度下降就可以带来足够的局部最优。</p>
<p>分析完优化模型本身的问题，再来看看最基础的GD的问题：目标函数通常是训练集中所有样本的损失的平均值，故目标函数的梯度为： <span class="math display">\[
\nabla f(\mathbf{x}) = \frac{1}{n} \sum_{i = 1}^n \nabla f_i(\mathbf{x})
\]</span> 如果用Full-batch GD，那么每次迭代每个参数的梯度计算的时间复杂度为<span class="math inline">\(O(n)\)</span>，对于大规模数据，这样的更新速度显然无法令人忍受。</p>
<p>学习率的选择是一项重要的调参工作，因此学习率的自适应变化就成为了研究热点之一，一些二阶方法应运而生，我们首先来看看牛顿法该如何解决这个问题。</p>
<p>对于损失函数<span class="math inline">\(f\)</span>，利用泰勒展开式有： <span class="math display">\[
f(\mathbf{x} + \boldsymbol{\epsilon}) = f(\mathbf{x}) + \boldsymbol{\epsilon}^\top \nabla f(\mathbf{x}) + \frac{1}{2} \boldsymbol{\epsilon}^\top \nabla^2 f(\mathbf{x}) \boldsymbol{\epsilon} + \mathcal{O}(\|\boldsymbol{\epsilon}\|^3)
\]</span> 式中的<span class="math inline">\(\nabla^2 f(\mathbf{x})\)</span>即<span class="math inline">\(d*d\)</span>海森矩阵，存储了函数的二阶偏导数。为了求得<span class="math inline">\(f\)</span>的最小值，令上式对<span class="math inline">\(\epsilon\)</span>求导得0，有：<span class="math inline">\(\boldsymbol{\epsilon}=-\nabla f(\mathbf{x})H^{-1}\)</span>，即每次的参数更新为<span class="math inline">\(\mathbf{x} \leftarrow \mathbf{x} - \nabla f(\mathbf{x})H^{-1}\)</span>。二阶近似利用了损失函数的曲率信息，即如果曲率比较小，那么这步更新就会比较大，反之则更新较小。这里没有了学习率，而是通过“梯度的梯度”自动调整步幅，看起来比一阶的梯度下降要好一些。</p>
<p>然而深度学习的参数空间往往十分巨大，因此存储和计算海森矩阵的逆是不现实的，这也是牛顿法无法在DNN中使用的重要原因。为了缓解这个问题，学术界提出了一些拟牛顿法如L-BFGS等试图去降低存储消耗，但是计算代价仍然很高。</p>
<p>从以上的分析可以看到：无论是Full-batch GD还是牛顿法，都存在计算消耗大等问题，不适用于深度学习任务的大规模数据集训练，因此已经很少被直接用在深度学习模型中。为了处理这些问题，学术界提出了很多替代的优化算法，因此接下来我将调研分析当前常用的深度学习优化算法 (SGD/Adam...)的优缺点，并结合实例及前沿研究进行相关讨论。 ## Popular Algorithms 优化算法在神经网络的训练中有着举足轻重的作用，选择合适的优化算法可以使得损失函数收敛地更快，同时收敛到更好的区域。目前比较流行的算法有下面几种： ### 1 SGD 尝试用mini-batch的梯度平均值作为整体梯度的无偏估计，参数的更新非常简单，沿着梯度的反方向即是loss下降最快的方向： <span class="math display">\[
x_{t+1}=x_t-\alpha\nabla f(x_t)
\]</span> 如果是Batch GD并且学习率足够小时可以保证损失函数单调不增。实际使用时一般会采用学习率递减策略保证模型收敛。</p>
<p>实现也非常简单：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x -= lr * grads</span><br></pre></td></tr></table></figure>
<p>SGD存在几个问题：</p>
<p>首先，如果loss对于不同参数的敏感程度不同，那么收敛过程会在敏感参数方向上抖动： <img src="https://img-blog.csdnimg.cn/20210710192205509.png" /> 对于非常大的参数空间，可能会收敛到不同的区域。 其次，如果loss函数有局部最优或者鞍点，这些点上梯度为0，无法收敛到全局最优； 最后，如果采用mini-batch，那么计算出的梯度值是有噪声的，意味着收敛过程可能会是非常曲折的，也即需要更多时间。 ### 2 SGD+Momentum 为了解决SGD的问题，有学者提出了带有动量的SGD，其思想也很简单：更新参数时不仅考虑当前的梯度方向，还要考虑历史累积梯度方向，如果两者方向一致，那么这一步更新幅度就会增大；如果不一致，就会减弱沿当前梯度的下降幅度。 <span class="math display">\[
v_{t+1}=\rho v_t+\nabla f(x_t) \\
x_{t+1}=x_t-\alpha v_{t+1}
\]</span> <span class="math inline">\(\rho\)</span>可以看作是对历史梯度的衰减，一般取0.9。</p>
<p>带有动量的SGD实现： <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">v = rho * v + grads</span><br><span class="line">x -= lr * v</span><br></pre></td></tr></table></figure> 这样就解决了SGD的三个问题： 首先，由于历史梯度的存在，朝敏感方向步进的数量就会减少，会更加平滑的向最优点前进，减小了震荡，加速收敛； 其次，对于局部最优点，虽然当前梯度为0，但是依靠历史梯度可以越过该点继续下降； 最后，梯度噪声引起的震荡可以通过历史梯度互相抵消。 ### 3 Nesterov Momentum Nesterov Momentum由SGD+Momentum衍生而来，SGD+Momentum是将当前点的梯度和速度结合起来，而Nesterov Momentum则是将当前点的速度和下一个近似点的梯度结合起来，意味着我们不是在当前位置去看未来，而是多看了一步，在稍远一些的下一步看未来，可以提前调整步进大小： <img src="https://img-blog.csdnimg.cn/20210710192712932.png" alt="在这里插入图片描述" /> 所以Nesterov Momentum的更新规则为： <span class="math display">\[
v_{t+1}=\rho v_t-\alpha\nabla f(x_t+\rho v_t) \\
x_{t+1}=x_t+v_{t+1}
\]</span> 通常我们希望针对<span class="math inline">\(x_t\)</span>计算梯度，通过简单的变量替换，得到新的更新规则： <span class="math display">\[
v_{t+1}=\rho v_t-\alpha\nabla f(x_t) \\
x_{t+1}=x_t+v_{t+1}+\rho(v_{t+1}-v_t)
\]</span> Nesterov的实现： <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">v_prev = v</span><br><span class="line">v = rho * v - lr * grads</span><br><span class="line">x += (1 + rho) * v - rho * v_prev</span><br></pre></td></tr></table></figure> ### 4 AdaGrad 前面的几种方法都是设置了一个全局的学习率，AdaGrad则通过引入二阶动量使得学习率可以针对<strong>每个参数</strong>自适应地取值：对于更新频繁的参数，已经有了很多认知，不希望因为单个样本影响太大，所以学习率可以小一些；对于更新稀疏的参数，希望从偶尔出现的能更新该参数的样本中多获得一些信息，所以学习率可以设置地大一些。为了了解参数更新的频繁程度，引入二阶动量——每个维度上历史梯度值的平方和： <span class="math display">\[
grad\_squared +=\nabla^2 f(x_t) \\
x_{t+1}=x_t-\cfrac{\alpha\nabla f(x_t)}{\sqrt{grad\_squared+10^{-7}}}
\]</span> 此时的学习率实质上是<span class="math inline">\(\cfrac{\alpha}{\sqrt{grad\_squared}}\)</span>，为了避免除0，一般分母加上一个很小的平滑项。如果某个参数更新频繁，那么grad_squared就会增大，学习率也就越小。</p>
<p>AdaGrad的实现： <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">grad_sq += grads**2</span><br><span class="line">x -= lr * grads / (numpy.sqrt(grad_sq) + eps)</span><br></pre></td></tr></table></figure> AdaGrad的问题在于随着grad_squared单调递增，学习率最终会单调衰减到0，意味着很可能会提早终止训练过程。 ### 5 RMSProp/AdaDelta 为了缓解AdaGrad的学习率变化过于激进的问题，二阶动量的计算不累积全部的历史梯度，只关注过去某段时间内的梯度变化，用指数移动平均值来表示过去某时间段的二阶动量的均值： <span class="math display">\[
grad\_squared=decay\_rate*grad\_squared+(1-decay\_rate)\nabla^2 f(x_t) \\
x_{t+1}=x_t-\cfrac{\alpha\nabla f(x_t)}{\sqrt{grad\_squared+10^{-7}}}
\]</span> decay_rate是一个超参数，一般取值0.9。</p>
<p>RMSProp的实现： <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">grad_sq = decay * grad_sq + (1 - decay) * grads**2</span><br><span class="line">x -= lr * grads / (numpy.sqrt(grad_sq) + eps)</span><br></pre></td></tr></table></figure> 因此，RMSProp仍然是通过梯度的大小来调整每个参数的学习率，不过现在学习率不会单调递减。</p>
<h3 id="adam">6 Adam</h3>
<p>Adam的出现是集成了一阶动量思想和AdaGrad等的二阶动量思想，即Adaptive Momentum： <span class="math display">\[
m_{t+1}=\beta_1m_t+(1-\beta_1)\nabla f(x_t)\\
V_{t+1}=\beta_2V_t+(1-\beta_2)\nabla^2 f(x_t)\\
x_{t+1}=x_t-\cfrac{\alpha m_{t+1}}{\sqrt{V_{t+1}+10^{-7}}}
\]</span> 由于m和V初始化为0，所以开始的几次迭代会偏向取值0，为了弥补这一缺点，又引入了偏差纠正项，完整的Adam算法如下： <span class="math display">\[
m=\beta_1m+(1-\beta_1)\nabla f(x_t)\\
m_t=\cfrac{m}{1-\beta_1^t}\\
V=\beta_2V+(1-\beta_2)\nabla^2 f(x_t)\\
V_t=\cfrac{V}{1-\beta_2^t}\\
x_{t}=x_{t-1}-\cfrac{\alpha m_{t}}{\sqrt{V_{t}+10^{-7}}}
\]</span> Adam的实现： <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">m = beta_1 * m + (1 - beta_1) * grads</span><br><span class="line">m_t = m / (1 - beta_1**t)</span><br><span class="line">v = beta_2 * v + (1 - beta_2) * grads**2</span><br><span class="line">v_t = v / (1 - beta_2**t)</span><br><span class="line">x -= lr * m_t / (numpy.sqrt(v_t) + eps)</span><br></pre></td></tr></table></figure> 如果Adam再加上Nesterov的向后看一步的思想，就是Nadam算法。 ## Experiment 为了对上述算法有更加直观的认识，同时在部分程度上比较不同算法的性能，构造一维函数<span class="math inline">\(f(x)\)</span>作为损失函数，其表达式如下： <span class="math display">\[
f(x)=0.01x^2+sin(x)+\frac{1}{3}cos(3x)+\frac{1}{5}sin(5x)+\frac{1}{7}cos(7x)
\]</span> 这个损失函数含有大量的局部最小点以及悬崖，如图所示： <img src="https://img-blog.csdnimg.cn/2021071019313296.png" alt="在这里插入图片描述" /> 为了公平起见，比较时将x的初始值设为-29，每种算法的迭代次数均设置为300次，学习率均设置为0.1，迭代过程如下图所示： <img src="https://img-blog.csdnimg.cn/20210710193206336.png" alt="在这里插入图片描述" /> 最终的收敛结果如下表所示： | 算法 | 最终x | 最终损失 | | -------- | ------ | -------- | | SGD | -27.98 | 7.19 | | Momentum | -24.00 | 6.21 | | Nesterov | -24.00 | 6.21 | | AdaGrad | -27.98 | 7.19 | | RMSProp | -28.95 | 9.10 | | Adam | -26.46 | 5.59 |</p>
<p>从上图和上表可以看到：Adam算法在前期收敛很快，并且最终效果最好，是综合性能最佳的算法；带动量的SGD能够越过一些局部极小值，在没有精细调参的情况下一度达到了和Adam类似的效果；AdaGrad开始时的梯度很大，但是由于学习率过早地减小，最终效果并不出众；这些结果进一步佐证了之前对各种算法的分析。</p>
<p>如果将学习率设置为0.01，对比如下： <img src="https://img-blog.csdnimg.cn/20210710193256448.png" alt="在这里插入图片描述" /> 可以看到：精调后的Momentum、Nesterov和Adam的效果几乎不相上下，这只是初步调整了学习率参数，如果通过验证集更加精细地调整超参数的值，那么SGD+Momentum完全可以达到甚至超越Adam的表现，当然这也需要人为付出更多的努力，Adam这个烦恼则小得多。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">Created on Sun Apr 11 18:31:58 2021</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">@author: Jingtao Ren</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">a = tf.constant(<span class="number">3.0</span>)</span><br><span class="line">b = tf.constant(<span class="number">5.0</span>)</span><br><span class="line">c = tf.constant(<span class="number">7.0</span>)</span><br><span class="line">d = tf.constant(<span class="number">0.1</span>)</span><br><span class="line">x = tf.Variable(initial_value=-<span class="number">29.0</span>, name=<span class="string">&quot;x&quot;</span>, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_f</span>():</span></span><br><span class="line">    x = np.linspace(-<span class="number">30</span>, <span class="number">30</span>, <span class="number">1000</span>)</span><br><span class="line">    <span class="comment"># y = -20.0 * np.exp(b * np.abs(x)) - np.exp(np.cos(c * x)) + 20.0 + np.exp(1)</span></span><br><span class="line">    y = (<span class="number">0.1</span> * x) ** <span class="number">2</span> + np.sin(x) + np.cos(a * x) / a + np.sin(b * x) / b + np.cos(c * x) / c</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;y&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">&#x27;Loss Function&#x27;</span>)</span><br><span class="line">    plt.plot(x, y)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_train</span>(<span class="params">y</span>):</span></span><br><span class="line">    x = np.arange(<span class="number">300</span>)</span><br><span class="line">    labels = [<span class="string">&#x27;SGD&#x27;</span>, <span class="string">&#x27;Momentum&#x27;</span>, <span class="string">&#x27;Nesterov&#x27;</span>, <span class="string">&#x27;AdaGrad&#x27;</span>, <span class="string">&#x27;RMSProp&#x27;</span>, <span class="string">&#x27;Adam&#x27;</span>]</span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Iteration&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">&#x27;Algorithm Comparison&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">6</span>):    </span><br><span class="line">        plt.plot(x, y[i], label=labels[i])</span><br><span class="line">        plt.legend()</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span>():</span></span><br><span class="line">    <span class="comment"># y = a * tf.exp(b * tf.abs(x)) - tf.exp(tf.cos(c * x)) - a + tf.exp(tf.constant(1.0))</span></span><br><span class="line">    y = tf.<span class="built_in">pow</span>(d * x, <span class="number">2</span>) + tf.sin(x) + tf.cos(a * x) / a + tf.sin(b * x) / b + tf.cos(c * x) / c</span><br><span class="line">    <span class="keyword">return</span> (y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">minimize</span>(<span class="params">optimizer, iters = <span class="number">300</span></span>):</span></span><br><span class="line">    y = []</span><br><span class="line">    <span class="comment"># optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> tf.<span class="built_in">range</span>(iters):</span><br><span class="line">        optimizer.minimize(loss, [x])</span><br><span class="line">        y.append(loss())</span><br><span class="line">    tf.<span class="built_in">print</span>(<span class="string">&quot;Final x = &quot;</span>, x)</span><br><span class="line">    tf.<span class="built_in">print</span>(<span class="string">&quot;Final Loss = &quot;</span>, loss())</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># plot_f()</span></span><br><span class="line">    ops = [tf.keras.optimizers.SGD(learning_rate=<span class="number">0.1</span>), tf.keras.optimizers.SGD(learning_rate=<span class="number">0.1</span>, momentum=<span class="number">0.9</span>),</span><br><span class="line">           tf.keras.optimizers.SGD(learning_rate=<span class="number">0.1</span>, momentum=<span class="number">0.9</span>, nesterov=<span class="literal">True</span>), tf.keras.optimizers.Adagrad(learning_rate=<span class="number">0.1</span>),</span><br><span class="line">           tf.keras.optimizers.Adadelta(learning_rate=<span class="number">0.1</span>, rho=<span class="number">0.9</span>), tf.keras.optimizers.Adam(learning_rate=<span class="number">0.1</span>)]</span><br><span class="line">    y = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">6</span>):</span><br><span class="line">        x.assign(-<span class="number">29.0</span>)</span><br><span class="line">        y.append(minimize(ops[i]))</span><br><span class="line">    plot_train(y)</span><br></pre></td></tr></table></figure>
<p>当然，这个实验非常简单，损失函数形式是一维的，实际中的网络模型参数的数量可能达到百万级别，超高维情况下算法的效率、鲁棒性以及模型最终的泛化能力才是我们真正关心的。</p>
<p>最后贴2张神图总结下： <img src="https://img-blog.csdnimg.cn/20210306203509952.gif#pic_center" alt="在这里插入图片描述" /><img src="https://img-blog.csdnimg.cn/2021030620351970.gif#pic_center" alt="在这里插入图片描述" /> ## Research Adam虽然是集大成者，而且也被推荐为起始的默认优化算法，但是一些Paper揭示了Adam的一些问题。 ### 1 过拟合 Berkeley在NIPS 2017的一篇文章指出：如果一个问题有多个全局极优，即使从相同的初始值出发，不同的优化算法也会得到完全不同的结果。文章构造了一个简单的线性可分的二分类问题，证明了SGD在这种情况下测试误差为0，而AdaGrad等自适应方法会把所有的测试样例分为正类，泛化能力极差，也就是根本不能工作。</p>
<p>随后作者又用VGG+BN+Dropout的网络结构在CIFAR-10数据集上进行了实验： <img src="https://img-blog.csdnimg.cn/20210710193712619.png" alt="在这里插入图片描述" /> 可以看到：前期训练中Adam有优势，但SGD的泛化能力确实比Adam要好。</p>
<p>最后，为了彻底黑化Adam，文章又用了文本数据集和一些NLP模型做了实验： <img src="https://img-blog.csdnimg.cn/20210710193823396.png" alt="在这里插入图片描述" /> 即便有时候自适应方法的训练loss会更低，但SGD的泛化能力都无一例外地胜过了自适应的方法。自适应方法在训练初期速度很快，但是后期表现平平。</p>
<p>泛化能力差的原因在于：自适应方法倾向于关注稀疏的特征，因为这些特征对于训练样例的鉴别是很有效的，尤其在训练样例数少而特征较多的数据集中，但是这些特征其实并非关键特征，这样自适应学习率算法出现过拟合的风险就会增大，导致泛化能力不佳，最终的收敛效果不如传统的SGD。 ### 2 二阶动量波动 Google的一篇文章从数学上证明了在某些特定情况下Adam可能不收敛，因为二阶动量取的是某个时间窗口的变化，所以<span class="math inline">\(V_t\)</span>的变化可能会剧烈震荡，尤其在高维情况下，梯度的方差可能随时间波动很大，导致学习率震荡，模型无法收敛。这也是为什么一般<span class="math inline">\(\beta_2\)</span>要取0.999这么大的值，避免二阶动量有太大波动。</p>
<p>一般认为Adam默认的<span class="math inline">\(\beta_1\)</span>和<span class="math inline">\(\beta_2\)</span>不需要调整，采用默认的0.9和0.999即可。但是这两个超参如果不按这样设置，Adam可能永远不会收敛到最优值。文章从数学上证明了对任意的<span class="math inline">\(\beta_1,\beta_2\in[0,1),\beta_1&lt;\sqrt{\beta_2}\)</span>，都存在一个随机的凸优化问题使得Adam不能收敛到最优解。</p>
<p>为了避免二阶动量的剧烈震荡，文章对其进行了控制，提出了一个新算法AMSGrad确保模型收敛，<span class="math inline">\(V_t=max(V_{t-1},\beta_2V_{t-1}+(1-\beta_2)\nabla^2 f(x_t))\)</span>。</p>
<p>作者随后通过人造数据和真实数据进行了实验：</p>
<p>人造数据上的结果： <img src="https://img-blog.csdnimg.cn/20210710193934596.png" alt="在这里插入图片描述" /> 很显然在Adam没有找到最优解的这些数据上，改进后的算法都表现良好。</p>
<p>在MNIST上的效果： <img src="https://img-blog.csdnimg.cn/20210710193956571.png" alt="在这里插入图片描述" /> 这篇文章最终获得了2018年ICLR最佳论文，但是引起了很大争议。主要原因在于其构造的令Adam失效的数据在实际情况中出现的概率极低，即使出现也会在数据预处理时被筛掉，因此并没有特别广泛的实际用处。另外，文章过于强调训练集上的损失函数值，甚至有人通过复现表明文章提出的AMSGrad算法在测试数据上表现很差，与原文中的某些结论相互矛盾。 ### 3 学习率下降 arXiv上的一篇文章通过在CIFAR-10上的实验证明Adam在一些情况下虽然速度快，但收敛效果没有SGD好： <img src="https://img-blog.csdnimg.cn/20210710194231292.png" alt="在这里插入图片描述" /> 文章通过实验发现主要原因在于后期Adam的学习率过低，影响了最终效果。文章尝试通过控制学习率下界，提高了最终收敛效果。</p>
<p>既然Adam后期有问题，那么一个自然的改进就是前期训练使用Adam，用来快速减小loss；后期训练转换为SGD，用稍慢的速度寻找更佳的解甚至是最优解。但是这样也会引入新的问题：在什么时刻切换？切换为SGD后的学习率又该如何设置？</p>
<p>文章提出了SWATS(Switches from Adam to SGD)策略来解决上面2个问题，在CIFAR-10和CIFAR-100数据集上实验效果看着还不错： <img src="https://img-blog.csdnimg.cn/20210710194309531.png" alt="在这里插入图片描述" /> <img src="https://img-blog.csdnimg.cn/20210710194329636.png" alt="在这里插入图片描述" />这些文章都采用了一些比较极端的数据去探索Adam的不适情况，然而实际中遇到这些极端情况的概率并不大，因此Adam并不失为首选尝试。通过上面的讨论可以看到：SGD和Adam各有优劣，精调后的SGD一般最终会收敛到更好的效果；Adam在训练前期收敛速度快，在稀疏数据上表现更好，对超参不敏感，不需要十分精细的调参。</p>
<p>如果对优化算法不熟悉，可以先尝试SGD+Nesterov Momentum或者Adam；如果对某个优化算法很精通，那么调参就会相对容易些。如果资源足够，也可以尝试L-BFGS等二阶优化方法。另外，选择之前要充分了解数据的性质，对于比较稀疏的数据可以优先尝试学习率自适应调整的算法。 ## Reference [1] CS231n: Convolutional Neural Networks for Visual Recognition. lecture 8, Stanford University. [2] The Marginal Value of Adaptive Gradient Methods in Machine Learning. NIPS'17 [3] On the Convergence of Adam and Beyond. ICLR'18 [4] Improving Generalization Performance by Switching from Adam to SGD. arXiv [5] Optimization methods for large-scale machine learning. SIAM Review, 2018. [6] Optimization for deep learning: theory and algorithms. arXiv, 2019. [7] Understanding Black-box Predictions via Influence Functions. ICML'17. [8] Understanding Deep Learning Requires Rethinking Generalization. ICLR'17.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/18/">18</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="EIMadrigal"
      src="/images/favicon.png">
  <p class="site-author-name" itemprop="name">EIMadrigal</p>
  <div class="site-description" itemprop="description">Hello World</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">171</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">19</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/EIMadrigal" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;EIMadrigal" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:andrew.renj@gmail.com" title="E-Mail → mailto:andrew.renj@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.cnblogs.com/EIMadrigal" title="cnblogs → https:&#x2F;&#x2F;www.cnblogs.com&#x2F;EIMadrigal" rel="noopener" target="_blank"><i class="fab fa-codiepie fa-fw"></i>cnblogs</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://weibo.com/EIMadrigal" title="Weibo → https:&#x2F;&#x2F;weibo.com&#x2F;EIMadrigal" rel="noopener" target="_blank"><i class="fab fa-weibo fa-fw"></i>Weibo</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2018 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">EIMadrigal</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>


    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <span id="busuanzi_container_site_pv">Total views: <span id="busuanzi_value_site_pv"></span></span>
    <span class="post-meta-divider">|</span>
    <span id="busuanzi_container_site_uv">Total visitors: <span id="busuanzi_value_site_uv"></span></span>
    <span class="post-meta-divider">|</span>

<script>
$(document).ready(function() {

    var int = setInterval(fixCount, 50);
    var countOffset = 20000;

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset);
            clearInterval(int);
        }  
    }
       	
});
</script> 

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


</body>
</html>
