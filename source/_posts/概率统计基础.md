---
title: 概率统计基础
url: probability-and-statistics-basics
date: 2021-01-11 09:24:00
description: Basic formula of probability and statistics
categories: Math
tags: [Probability & Statistics]
---

联合概率$P(A,B)$即两个事件同时发生的概率
条件概率（后验概率）$P(A|B)P(B)=P(B|A)P(A)$
全概率公式
$$
P(A)=\sum_nP(A,B_n)=\sum_nP(A|B_n)P(B_n)
$$
贝叶斯定理：
$$
P(A|B)=\frac{P(B|A)P(A)}{P(B)}
$$
$A$的后验概率等于标准相似度乘以先验概率

离散型随机变量概率分布（分布律）：
|$X$|  $x_1$|...|$x_i$|...
|--|--|--|--|--|
|  $P$|  $p_1$|...|$p_i$|...
满足$p_i\geq0, \sum_{i=1}^{\infty}p_i=1$

0-1分布：$X\sim B(1,p)$
|$X$|  0|1|
|--|--|--
|$P$|$1-p$|$p$|
$$
P(X=k)=p^k(1-p)^{1-k},k=0,1
$$

二项分布：$X\sim B(n,p)$
$n$重伯努利试验
$$
P(X=k)=C_n^kp^k(1-p)^{n-k},k=0,1,...,n
$$

Poisson分布：$X\sim P(\lambda)$
$$
P(X=k)=\frac{\lambda^ke^{-\lambda}}{k!},k=0,1,...,n,\lambda>0
$$
$E(X)=\lambda,D(X)=\lambda$
可以证明：Poisson分布是二项分布在$\lambda=np,n\to\infty$的极限分布。

连续型随机变量概率分布函数：$F(x)=\int_{-\infty}^{x}f(t)dt$，$f(x)$称为概率密度函数。
均匀分布：$X\sim U(a,b)$
$$
f(x)=
\begin{cases}
\cfrac{1}{b-a}, &x\in(a,b)\\
0, &其它
\end{cases}
$$
指数分布：$X\sim E(\lambda)$
$$
f(x)=
\begin{cases}
\lambda e^{-\lambda x}, &x>0\\
0, &x\leq0
\end{cases}
$$
正态分布（高斯分布）：$X\sim N(\mu,\sigma^2)$
$$
f(x)=\cfrac{1}{\sigma \sqrt{2\pi}}e^{-\cfrac{(x-\mu)^2}{2\sigma^2}}
$$
$\mu$是位置参数，决定对称轴位置；$\sigma$是尺度参数，决定分布的幅度。
标准正态分布$X\sim N(0,1)$

数学期望：
离散型：$E(X)=\sum_ip_ix_i$
连续型：$E(X)=\int_{-\infty}^{+\infty}xf(x)dx$

方差：随机变量的离散程度，距离期望的距离
$D(X)=E[(X-E(X))^2]=E(X^2)-E(X)^2$
$$D(X)=\cfrac{1}{N}\sum_{i=1}^{N}(x_i-\mu)^2=\cfrac{1}{N}(\sum_{i=1}^{N}x_i^2-N\mu^2)$$
离散型：$$D(X)=\sum_{i=1}^{\infty}[x_i-E(X)]^2p_i$$
连续型：$$D(X)=\int_{-\infty}^{+\infty}[x-E(X)]^2f(x)dx$$

标准差（均方差）是方差的算术平方根

样本标准差：
$$s=\sqrt{\cfrac{1}{n-1}\sum_{i=1}^{n}(x_i-\bar x)^2}$$
对于二维随机变量，协方差用来描述$X$与$Y$之间的相互关系：
$$Cov(X,Y)=E\{[X-E(x)][Y-E(Y)]\}$$

相关系数：
$$
\rho_{XY}=\cfrac{Cov(X,Y)}{\sqrt{D(X)D(Y)}}
$$

de Movire-Laplace中心极限定理：
$n_A$为$n$重伯努利试验中$A$发生的次数，$P(A)=p$，对任意实数$x$，有：
$$
\lim\limits_{n\to+\infty}P(\cfrac{n_A-np}{\sqrt{np(1-p)}}\leq x)=\int_{-\infty}^{x}\cfrac{1}{\sqrt{2\pi}}e^{-\frac{t^2}{2}}dt=\Phi(x)
$$
$n$充分大时，$n_A\sim N(np,np(1-p))$，故：
$$
P(a<n_a\leq $$="" $n$充分大时，$\sum_{i="1}^{n}X_i\sim" \cfrac{\sqrt{n}(\bar="" \lim\limits_{n\to+\infty}p(|\cfrac{n_x}{n}-p|<\epsilon)="1" b)\approx\phi(\cfrac{b-n\mu}{\sqrt{n}\sigma})-\phi(\cfrac{a-n\mu}{\sqrt{n}\sigma})="" b)\approx\phi(\cfrac{b-np}{\sqrt{np(1-p)}})-\phi(\cfrac{a-np}{\sqrt{np(1-p)}})="" n(0,1)="" n(0,1)$，即：="" n(0,1)$，故：="" n(n\mu,n\sigma^2)$，$y_n\sim="" p(a<\sum_{i="1}^{n}X_i\leq" x-\mu)}{\sigma}\sim="" y_n="\cfrac{\sum_{i=1}^{n}X_i-n\mu}{\sqrt{n}\sigma}" 中心极限定理表明：任意的一个概率分布中生成的随机变量，其序列和统一地归约到正态分布：$y_n\sim="" 伯努利大数定律：事件$x$在每次试验中发生概率是$p$，$n$次独立重复试验中，$x$发生的次数为$n_x$，则：="" 即事件的发生频率依概率收敛于事件的概率。="" 独立同分布中心极限定理：="" 辛钦大数定律：$x_i$为独立同分布的随机变量序列，且期望$\mu$存在，则对$\forall\epsilon="" 随机变量$x_1,x_2,...,x_n,...$独立同分布，$e(x_i)="\mu,D(X_i)=\sigma^2$，前$n$个变量和的标准化变量为：">0$，有：
$$
\lim\limits_{n\to+\infty}P(|\cfrac{1}{n}\sum_{i=1}^{n}X_i-\mu|\geq\epsilon)=0
$$

正太分布熵的大小，取决于方差的大小。</n_a\leq>